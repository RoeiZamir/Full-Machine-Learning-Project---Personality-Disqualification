Final Project - EDA - Pre-Cleaning
"Throughout the notebook, blue text highlights important findings that arose during the data analysis. These findings have had an impact on decision-making, treatment methods, and interventions in the project data throughout the EDA analysis. Alternatively, they may have influenced decisions in the more advanced stages of project execution."
1. Import Packages
#pip install missingno
#pip install cloudinary
#pip install openCV-python
#pip install chart_studio
#pip install researchpy
import pandas as pd
import numpy as np

from scipy import stats
from scipy.stats import chisquare
from scipy.stats import chi2_contingency
from scipy.stats import f_oneway
from scipy.stats import kstest
from scipy.stats import ks_2samp
from scipy.stats import norm
from scipy.stats import iqr

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.datasets import load_boston
from sklearn import linear_model

import matplotlib.pyplot as plt
plt.style.use('classic')
import seaborn as sns
plt.style.use('seaborn')
get_ipython().run_line_magic('matplotlib', 'inline')

from ydata_profiling import ProfileReport
import statsmodels.api as sm
import statsmodels.formula.api as smf

import chart_studio
import re
import cv2

import missingno as msno
import warnings
warnings.filterwarnings("ignore")

import cloudinary
import cloudinary.uploader
import cloudinary.api

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#port csv
#mport pyodbc
#.read_sql_query

import researchpy as rp
import scipy.stats as stats

import pandas as pd
import numpy as np
import scipy.stats as ss
from math import log

#pd.set_option('display.max_rows', None, 'display.max_columns', None)
2. Import Data
2.1 From SQL server
#cnxn_str = ("Driver={SQL Server Native Client 11.0};"
#            "Server=LENOVO,1433;"
#            "Database=Personality;"
#            "Trusted_Connection=yes;")
#cnxn = pyodbc.connect(cnxn_str)
#cursor = cnxn.cursor()
#df_1 = pd.read_sql("SELECT TOP(100) * FROM Final_Flat_File_before_EDA", cnxn)
#data = pd.read_sql(SELECT* FROM Personality.dbo.Flat_File_before_EDA", cnxn)
#cursor = cnxn.cursor()
# first alter the table, adding a column
#cursor.execute("ALTER TABLE Final_Flat_File_before_EDA " +
#               "ADD FIRST VARCHAR(20)")
# now update that column to contain firstName + lastName
#cursor.execute("UPDATE Final_Flat_File_before_EDA " +
#               "SET FIRST = firstName + " " + lastName")
#cnxn.commit()
2.2 Import from auto refreshing CSV file
#import csv
#rows = []
#with open("C:/Users/Knowl/Desktop/Project/perfrct_with_file_clean.csv", 'r') as file:
#    csvreader = csv.reader(file)
#    header = next(csvreader)
#    for row in csvreader:
#        rows.append(row)
#print(header)
#print(rows)
### Making list of missing value types

missing_values = ["n/a", "na", "missing", "Null", "NaN", "isnull", "None", "?", "<N\A>", "9999"]
df = pd.read_csv("C:/Users/Knowl/Desktop/Project/Flat_File_Before EDA - All_Participants.csv", na_values = missing_values)
df.head()
     id  Population_Type_Dico Process_Type         Designated_Role  \
0   540                   0.0  miun_kzinim              prosecutor   
1  1128                   NaN        other     operational officer   
2   604                   0.0  miun_kzinim  administrative officer   
3   605                   0.0  miun_kzinim  administrative officer   
4   536                   0.0  miun_kzinim              prosecutor   

   Assessment_Center_Grade  Cognitive_Ability_Test  Morals_and_Values_Test  \
0                      4.9                     7.0                       4   
1                      4.3                     6.0                       1   
2                      4.4                     9.0                       1   
3                      5.1                     6.0                       2   
4                      4.9                     7.0                       5   

   Job_Interview_Grade  Heberew_Test_Grade  Filling_Instruction_Test  ...  \
0                  4.0                 9.0                 13.500000  ...   
1                  NaN                 8.0                       NaN  ...   
2                  5.0                 9.0                 17.799999  ...   
3                  5.0                 9.0                 11.000000  ...   
4                  5.0                 9.0                 12.000000  ...   

   L_dico  MA_dico  SE_dico RT_dico TRIN_dico VRIN_dico CR_dico  Q_dico  \
0     0.0      0.0      0.0     0.0       0.0       0.0     0.0     0.0   
1     NaN      NaN      NaN     NaN       NaN       NaN     NaN     NaN   
2     1.0      0.0      0.0     0.0       0.0       0.0     0.0     0.0   
3     1.0      0.0      0.0     0.0       1.0       0.0     0.0     0.0   
4     0.0      1.0      0.0     0.0       0.0       0.0     0.0     0.0   

   RS_dico  Personality_Disqualification_Dico  
0      0.0                                  0  
1      NaN                                  0  
2      0.0                                  0  
3      0.0                                  0  
4      0.0                                  0  

[5 rows x 122 columns]
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2300 entries, 0 to 2299
Columns: 122 entries, id to Personality_Disqualification_Dico
dtypes: float64(109), int64(6), object(7)
memory usage: 2.1+ MB
df.columns = df.columns.str.replace('(\([A-Z]*)\)', '')
df.columns = df.columns.str.replace('-', '_')
df.columns = df.columns.str.replace('.', '')
df.columns = df.columns.str.replace(' ', '')
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Part A - EDA Exsaminitions
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
"In this section, a wide variety of statistical analyses will be conducted to investigate the project data. The statistical analyses in this section will be used for decision-making purposes at various stages of the project, including the stage of data enrichment, features selection, and development of the project models. Simultaneously, during this stage of EDA analysis, the data will not be manipulated within the data frame. The second part of the EDA analysis is the treatment phase. during this phase, changes will be made to the data framework, among other things, based on the results found in the EDA exsaminitions phase."
3. Variables Measures Scales (Data Types) 
3.1 Accurate coding and classification of the variables into the following data types :
Notes - (1) int cant contain missing values; (2) In general, python commands run more easily (without errors) when dichotomous  categorical variables are coded numerically (rather than literally - String - Varchar) and when there are no missing values in the data frame; (3)
### checking datatype before conversions
df.dtypes
id                                     int64
Population_Type_Dico                 float64
Process_Type                          object
Designated_Role                       object
Assessment_Center_Grade              float64
                                      ...   
VRIN_dico                            float64
CR_dico                              float64
Q_dico                               float64
RS_dico                              float64
Personality_Disqualification_Dico      int64
Length: 122, dtype: object
df.head()
     id  Population_Type_Dico Process_Type         Designated_Role  \
0   540                   0.0  miun_kzinim              prosecutor   
1  1128                   NaN        other     operational officer   
2   604                   0.0  miun_kzinim  administrative officer   
3   605                   0.0  miun_kzinim  administrative officer   
4   536                   0.0  miun_kzinim              prosecutor   

   Assessment_Center_Grade  Cognitive_Ability_Test  Morals_and_Values_Test  \
0                      4.9                     7.0                       4   
1                      4.3                     6.0                       1   
2                      4.4                     9.0                       1   
3                      5.1                     6.0                       2   
4                      4.9                     7.0                       5   

   Job_Interview_Grade  Heberew_Test_Grade  Filling_Instruction_Test  ...  \
0                  4.0                 9.0                 13.500000  ...   
1                  NaN                 8.0                       NaN  ...   
2                  5.0                 9.0                 17.799999  ...   
3                  5.0                 9.0                 11.000000  ...   
4                  5.0                 9.0                 12.000000  ...   

   L_dico  MA_dico  SE_dico RT_dico TRIN_dico VRIN_dico CR_dico  Q_dico  \
0     0.0      0.0      0.0     0.0       0.0       0.0     0.0     0.0   
1     NaN      NaN      NaN     NaN       NaN       NaN     NaN     NaN   
2     1.0      0.0      0.0     0.0       0.0       0.0     0.0     0.0   
3     1.0      0.0      0.0     0.0       1.0       0.0     0.0     0.0   
4     0.0      1.0      0.0     0.0       0.0       0.0     0.0     0.0   

   RS_dico  Personality_Disqualification_Dico  
0      0.0                                  0  
1      NaN                                  0  
2      0.0                                  0  
3      0.0                                  0  
4      0.0                                  0  

[5 rows x 122 columns]
It can be seen that the automatic conversion operation significantly improved the classification of the variables to the appropriate levels. At the same time, the conversion is still not perfect, especially with regard to categorical variables encoded as string variables (more correct classification - categorical variables). Therefore, it was decided to carry out number of "manually" actions to adjust the research variables types.
3.2 Changing variables types
df2 = pd.DataFrame(df)
### change variables decimal to only one after dot 
df2.round(1)
df2.head()
     id  Population_Type_Dico Process_Type         Designated_Role  \
0   540                   0.0  miun_kzinim              prosecutor   
1  1128                   NaN        other     operational officer   
2   604                   0.0  miun_kzinim  administrative officer   
3   605                   0.0  miun_kzinim  administrative officer   
4   536                   0.0  miun_kzinim              prosecutor   

   Assessment_Center_Grade  Cognitive_Ability_Test  Morals_and_Values_Test  \
0                      4.9                     7.0                       4   
1                      4.3                     6.0                       1   
2                      4.4                     9.0                       1   
3                      5.1                     6.0                       2   
4                      4.9                     7.0                       5   

   Job_Interview_Grade  Heberew_Test_Grade  Filling_Instruction_Test  ...  \
0                  4.0                 9.0                 13.500000  ...   
1                  NaN                 8.0                       NaN  ...   
2                  5.0                 9.0                 17.799999  ...   
3                  5.0                 9.0                 11.000000  ...   
4                  5.0                 9.0                 12.000000  ...   

   L_dico  MA_dico  SE_dico RT_dico TRIN_dico VRIN_dico CR_dico  Q_dico  \
0     0.0      0.0      0.0     0.0       0.0       0.0     0.0     0.0   
1     NaN      NaN      NaN     NaN       NaN       NaN     NaN     NaN   
2     1.0      0.0      0.0     0.0       0.0       0.0     0.0     0.0   
3     1.0      0.0      0.0     0.0       1.0       0.0     0.0     0.0   
4     0.0      1.0      0.0     0.0       0.0       0.0     0.0     0.0   

   RS_dico  Personality_Disqualification_Dico  
0      0.0                                  0  
1      NaN                                  0  
2      0.0                                  0  
3      0.0                                  0  
4      0.0                                  0  

[5 rows x 122 columns]
###not used at this point
#f2 = df2.convert_dtypes()
Coding variables type as category
df2 = df2.astype({"Designated_Role":'category', "Education_Level":'category', "Educational_Institution":'category', "Rovaee_Training_Type":'category', "Exam_Language":'category'})
#df2 = df2.astype({"Designated_Role":'str', "Education_Level":'str', "Educational_Institution":'str', "Rovaee_Training_Type":'str', "Exam_Language":'str'})
Coding variables type as integer (not used)
###not used at this point
#df2=df2.astype({"M2a":'int',"ASP":'int',"R":'int',"CYN":'int',"BIM":'int',"PA":'int',"M2APS":'int',"AAS":'int',"tcent":'int',"OBS":'int',"MDS":'int',"TPA":'int',"BIZ":'int',"ANG":'int',"WSD":'int',"DEP":'int',"DIS":'int',"K":'int',"SOD":'int',"D":'int',"D1":'int',"D2":'int',"D3":'int',"D4":'int',"D5":'int',"MF":'int',"DOE":'int',"PT":'int',"ANX":'int',"F":'int',"SC":'int',"HEA":'int',"FP":'int',"HS":'int',"FAM":'int',"PD":'int',"ES":'int',"FCENT":'int',"WRK":'int',"HY":'int',"L":'int',"MA":'int',"SE":'int',"RT":'int',"TRIN":'int',"VRIN":'int',"CR":'int',"Q":'int',"RS":'int'})
Coding variables type as object
df2=df2.astype({"Population_Type_Dico":'object',"Gender_Dico":'object',"Commander_in_Army_Dico":'object',"Combat_Army_Unit_Dico":'object',"M2a_dico":'object',"ASP_dico":'object',"R_dico":'object',"CYN_dico":'object',"BIM_dico":'object',"PA_dico":'object',"M2APS_dico":'object',"AAS_dico":'object',"tcent_dico":'object',"OBS_dico":'object',"MDS_dico":'object',"TPA_dico":'object',"BIZ_dico":'object',"ANG_dico":'object',"WSD_dico":'object',"DEP_dico":'object',"DIS_dico":'object',"K_dico":'object',"SOD_dico":'object',"D_dico":'object',"D1_dico":'object',"D2_dico":'object',"D3_dico":'object',"D4_dico":'object',"D5_dico":'object',"MF_dico":'object',"DOE_dico":'object',"PT_dico":'object',"ANX_dico":'object',"F_dico":'object',"SC_dico":'object',"HEA_dico":'object',"FP_dico":'object',"HS_dico":'object',"FAM_dico":'object',"PD_dico":'object',"ES_dico":'object',"FCENT_dico":'object',"WRK_dico":'object',"HY_dico":'object',"L_dico":'object',"MA_dico":'object',"SE_dico":'object',"RT_dico":'object',"TRIN_dico":'object',"VRIN_dico":'object',"CR_dico":'object',"Q_dico":'object',"RS_dico":'object'})
#df2=df2.astype({"Personality_Disqualification_Dico":'object'})
Coding variables type as bool (not used)
###not used at this point
#df2=df2.astype({"Population_Type_Dico":'bool',"Gender_Dico":'bool',"Commander_in_Army_Dico":'bool',"Combat_Army_Unit_Dico":'bool',"M2a_dico":'bool',"ASP_dico":'bool',"R_dico":'bool',"CYN_dico":'bool',"BIM_dico":'bool',"PA_dico":'bool',"M2APS_dico":'bool',"AAS_dico":'bool',"tcent_dico":'bool',"OBS_dico":'bool',"MDS_dico":'bool',"TPA_dico":'bool',"BIZ_dico":'bool',"ANG_dico":'bool',"WSD_dico":'bool',"DEP_dico":'bool',"DIS_dico":'bool',"K_dico":'bool',"SOD_dico":'bool',"D_dico":'bool',"D1_dico":'bool',"D2_dico":'bool',"D3_dico":'bool',"D4_dico":'bool',"D5_dico":'bool',"MF_dico":'bool',"DOE_dico":'bool',"PT_dico":'bool',"ANX_dico":'bool',"F_dico":'bool',"SC_dico":'bool',"HEA_dico":'bool',"FP_dico":'bool',"HS_dico":'bool',"FAM_dico":'bool',"PD_dico":'bool',"ES_dico":'bool',"FCENT_dico":'bool',"WRK_dico":'bool',"HY_dico":'bool',"L_dico":'bool',"MA_dico":'bool',"SE_dico":'bool',"RT_dico":'bool',"TRIN_dico":'bool',"VRIN_dico":'bool',"CR_dico":'bool',"Q_dico":'bool',"RS_dico":'bool',"Personality_Disqualification_Dico":'bool'})
Coding variables type as dates (not used)
#df2.Examination_Date=df2.Examination_Date.astype('datetime64');
3.3 Variable types after applying the fixes
df2.dtypes
id                                      int64
Population_Type_Dico                   object
Process_Type                           object
Designated_Role                      category
Assessment_Center_Grade               float64
                                       ...   
VRIN_dico                              object
CR_dico                                object
Q_dico                                 object
RS_dico                                object
Personality_Disqualification_Dico       int64
Length: 122, dtype: object
The purpose of examining missing values at this early stage is to compare the data reached in the Python dataframe with the data in the source file (to ensure complete compatibility between the data source and the data destination)."
print(df2.isnull().sum())
id                                     0
Population_Type_Dico                 147
Process_Type                           0
Designated_Role                      148
Assessment_Center_Grade              988
                                    ... 
VRIN_dico                             29
CR_dico                               29
Q_dico                                29
RS_dico                               29
Personality_Disqualification_Dico      0
Length: 122, dtype: int64
A complete match was found between the data received from the source file (CSV) and the current Python dataframe. With a quick glance, even at this stage, we can see that there are quite a few missing values within the project variables. In any case, as mentioned, the EDA analysis will continue to discuss the issue of missing values and how to handle them .
--- Chapter 3 Summary ---
•	All the data in the columns of the dataframe were adjusted to the relevant variable types. Variables on the Boolean scale were intentionally defined as Object type for technical considerations regarding data processing.
•	Even at this early stage, it can be observed that a significant number of variables have a high amount of missing values (in-depth tests on this issue will be conducted in more advanced EDA stages)."
4. Data Frame structure
4.1 Dataframe Project shape, columns and categories Names
df2.shape
(2300, 122)
The datafrme contains 122 variables and 2300 observations. There is a full match between the data source file (CSV) and the data reached to the current Python dataframe.
df2.columns
Index(['id', 'Population_Type_Dico', 'Process_Type', 'Designated_Role',
       'Assessment_Center_Grade', 'Cognitive_Ability_Test',
       'Morals_and_Values_Test', 'Job_Interview_Grade', 'Heberew_Test_Grade',
       'Filling_Instruction_Test',
       ...
       'L_dico', 'MA_dico', 'SE_dico', 'RT_dico', 'TRIN_dico', 'VRIN_dico',
       'CR_dico', 'Q_dico', 'RS_dico', 'Personality_Disqualification_Dico'],
      dtype='object', length=122)
for col in ['Educational_Institution', 'Education_Level', 'Designated_Role', 'Rovaee_Training_Type', 'Exam_Language'] :
    
    print(f'-----{col}------')
    print(df2[col].unique())
    print('-----------------')
-----Educational_Institution------
[NaN, 'other', 'college', 'university']
Categories (3, object): ['college', 'other', 'university']
-----------------
-----Education_Level------
[NaN, 'complete_bagrot', 'without_bagrot', 'Academic education', 'diploma']
Categories (4, object): ['Academic education', 'complete_bagrot', 'diploma', 'without_bagrot']
-----------------
-----Designated_Role------
['prosecutor', 'operational officer', 'administrative officer', NaN, 'operational core units', ..., 'Traffic cops', 'administrative nagad', 'special unit', 'intelligence', 'Control center volunteers']
Length: 12
Categories (11, object): ['Border Guard fighters', 'Control center volunteers', 'Police dispatchers', 'Traffic cops', ..., 'operational core units', 'operational officer', 'prosecutor', 'special unit']
-----------------
-----Rovaee_Training_Type------
[NaN, 'rovaee 3', 'rovaee 8', 'rovaee 5', 'rovaee 2', ..., 'rovaee 12', 'rovaee 1', 'rovaee 4', 'rovaee 13', 'rovaee 9']
Length: 14
Categories (13, object): ['rovaee 0', 'rovaee 1', 'rovaee 10', 'rovaee 12', ..., 'rovaee 6', 'rovaee 7', 'rovaee 8', 'rovaee 9']
-----------------
-----Exam_Language------
[NaN, 'h', 'r', 'e', 'a', 'R']
Categories (5, object): ['R', 'a', 'e', 'h', 'r']
-----------------
4.2 Duplicate values in dataframe
df2.head()
     id Population_Type_Dico Process_Type         Designated_Role  \
0   540                  0.0  miun_kzinim              prosecutor   
1  1128                  NaN        other     operational officer   
2   604                  0.0  miun_kzinim  administrative officer   
3   605                  0.0  miun_kzinim  administrative officer   
4   536                  0.0  miun_kzinim              prosecutor   

   Assessment_Center_Grade  Cognitive_Ability_Test  Morals_and_Values_Test  \
0                      4.9                     7.0                       4   
1                      4.3                     6.0                       1   
2                      4.4                     9.0                       1   
3                      5.1                     6.0                       2   
4                      4.9                     7.0                       5   

   Job_Interview_Grade  Heberew_Test_Grade  Filling_Instruction_Test  ...  \
0                  4.0                 9.0                 13.500000  ...   
1                  NaN                 8.0                       NaN  ...   
2                  5.0                 9.0                 17.799999  ...   
3                  5.0                 9.0                 11.000000  ...   
4                  5.0                 9.0                 12.000000  ...   

   L_dico  MA_dico  SE_dico RT_dico TRIN_dico VRIN_dico CR_dico  Q_dico  \
0     0.0      0.0      0.0     0.0       0.0       0.0     0.0     0.0   
1     NaN      NaN      NaN     NaN       NaN       NaN     NaN     NaN   
2     1.0      0.0      0.0     0.0       0.0       0.0     0.0     0.0   
3     1.0      0.0      0.0     0.0       1.0       0.0     0.0     0.0   
4     0.0      1.0      0.0     0.0       0.0       0.0     0.0     0.0   

   RS_dico Personality_Disqualification_Dico  
0      0.0                                 0  
1      NaN                                 0  
2      0.0                                 0  
3      0.0                                 0  
4      0.0                                 0  

[5 rows x 122 columns]
print(df.duplicated().sum())
0
No duplications were found in the data frame
 4.3 Drop of Irrelevant variables : 
- Examination_Date Variable: The participants in the project are divided into training, validation, and test datasets based on the Examination_Date variable. However, the time variable will not be used for model development.
- Process Type: Following a clarification obtained from officials in the Department of Behavioral Sciences at the Israel Police, it was discovered that this factor is tautological with the outcome variable. Many of the candidates who were rejected in the personality test do not have a coded value in this component (due to technical reasons).
df = df.drop(columns=['Examination_Date', 'Process_Type'])
df2 = df2.drop(columns=['Examination_Date', 'Process_Type'])
df2.head()
     id Population_Type_Dico         Designated_Role  Assessment_Center_Grade  \
0   540                  0.0              prosecutor                      4.9   
1  1128                  NaN     operational officer                      4.3   
2   604                  0.0  administrative officer                      4.4   
3   605                  0.0  administrative officer                      5.1   
4   536                  0.0              prosecutor                      4.9   

   Cognitive_Ability_Test  Morals_and_Values_Test  Job_Interview_Grade  \
0                     7.0                       4                  4.0   
1                     6.0                       1                  NaN   
2                     9.0                       1                  5.0   
3                     6.0                       2                  5.0   
4                     7.0                       5                  5.0   

   Heberew_Test_Grade  Filling_Instruction_Test  Emotional_Inteligence_Test  \
0                 9.0                 13.500000                        23.0   
1                 8.0                       NaN                        15.0   
2                 9.0                 17.799999                        20.0   
3                 9.0                 11.000000                        20.0   
4                 9.0                 12.000000                        24.0   

   ...  L_dico  MA_dico SE_dico RT_dico TRIN_dico  VRIN_dico  CR_dico Q_dico  \
0  ...     0.0      0.0     0.0     0.0       0.0        0.0      0.0    0.0   
1  ...     NaN      NaN     NaN     NaN       NaN        NaN      NaN    NaN   
2  ...     1.0      0.0     0.0     0.0       0.0        0.0      0.0    0.0   
3  ...     1.0      0.0     0.0     0.0       1.0        0.0      0.0    0.0   
4  ...     0.0      1.0     0.0     0.0       0.0        0.0      0.0    0.0   

  RS_dico Personality_Disqualification_Dico  
0     0.0                                 0  
1     NaN                                 0  
2     0.0                                 0  
3     0.0                                 0  
4     0.0                                 0  

[5 rows x 120 columns]
--- Chapter 4 Summary ---
•	Adequacy was found between the number of observations, variables, and category names in the current dataframe and the data in the source file (CSV).
•	No duplications were found in the dataframe (as per the id variable).
•	Variables that are not intended to be used in the future for model development were excluded from the dataframe.
Social Sciences VS. Exact Sciences Measurement Level Perspectives
It is important to note that there are differences, mainly between researches from disciplines in the field of exact sciences and in the field of social sciences, regarding the measurement scale of variables in order-profit scales. social sciences researches holds that subjective variables (such as, Feelings, attitudes, beliefs, emotions measured with questionnaires, Grades  scores in tests  job selection tests evaluation grades  temperature  credit score  SAT score, etc)  are variables in "interval scale". In such variables the differences between ratios remain stable, and therefore, parametric tests can be applied to variables on this scale. Contrary to the above explanation, researchers, mainly from the fields of exact sciences, believe that such numerical variables (Without absolute zero point) are found only in an ordinal scale, and Therefore, parametric tests cannot be applied to variables in these scales.
At this point, with the required modesty, it is important to emphasized that the project writer holds social sciences perspective, according to which the numerical variables, such as, candidates scores in job selection tests (cognitive tests  assessment center scores  MMPI personality test scales, etc), found in a interval measurement scale and can be included in parametric statistical analysis. In this context, it is true that there is a debate regarding the compliance  non-compliance of variables on an interval scale with the assumptions and conditions necessary for parametric tests execution, but on the other hand, referring to these variables under "ordinal scale" entails significant limitations which no less threaten the predictive validity of prediction models, such as - reducing the range of statistical tests types can be performed to develop models, use non-parametric tests which, in some,characterized by low statistical power compared to parametric tests, increasing the chance of multicollinearity problems, over-fitting and generalization problems due to ineffective treatment of the independent variables and more.
It is important to note that there are differences, primarily between research from disciplines in the field of exact sciences and research in the field of social sciences, regarding the measurement scale of variables related to ordinal and interval. Social sciences research holds that subjective variables, such as feelings, attitudes, beliefs, and emotions measured with questionnaires, as well as grades, scores in tests, job selection test evaluation grades, temperature, credit score, SAT score, etc., are considered variables on an 'interval scale'. In such variables, the differences between ratios remain stable, allowing for the application of parametric tests. Contrary to the above explanation, researchers, primarily from the fields of exact sciences, believe that such numerical variables without an absolute zero point can only be found in an ordinal scale. Therefore, they argue that parametric tests cannot be applied to variables on these scales.
At this point, it is important to emphasize with humility that the project writer adopts a social sciences perspective. According to this perspective, numerical variables such as candidates' scores in job selection tests (cognitive tests, assessment center scores, MMPI personality test scales, etc.) are found on an interval measurement scale and can be included in parametric statistical analysis. While there is a debate regarding the suitability of variables on an interval scale for meeting the assumptions and conditions required for executing parametric tests, categorizing these variables as belonging to an 'ordinal scale' imposes limitations. These limitations threaten the predictive validity of prediction models no less, as they reduce the range of statistical tests that can be performed for model development, necessitate the use of non-parametric tests which, in some cases, exhibit lower statistical power compared to parametric tests, and increase the risk of problems such as multicollinearity, overfitting, and generalization due to the ineffective treatment of independent variables (Among other things, due to avoiding the application of crucial statistical procedures on variables within an ordinal scale).
Williams, M. N. (2021). Levels of measurement and statistical analyses. Meta-Psychology, 5, 1-14. https : / /doi.org/10.15626/MP.2019.1916‏
5. Variables Groups - Numerical  Dummy  Categorical
For a convenient and intuitive presentation of the analyses conducted as part of the EFA, the project variables were divided into multiple data sets. This division of variables into separate data frames also facilitated an efficient, faster, and simpler analysis of the data.
#list(set(df2.dtypes.tolist()))
Numeric variables group 
df_num = df2.select_dtypes(include = ['float64', 'int64'])
df_num_no_target = df2.select_dtypes(include = ['float64', 'int64']).drop(columns=['Personality_Disqualification_Dico'])

#df_num.dtypes
#df_num_no_target.info()
#df_num = df2.select_dtypes(include = ['float64', 'int64']).join(df2['Personality_Disqualification_Dico'])
Category variables group 
df_categorical = df2.select_dtypes(include = ['category']).join(df['Personality_Disqualification_Dico'])
df_categorical_no_target = df2.select_dtypes(include = ['category'])

#df_categorical.info()
##df_categorical_no_target.info()
#df_categorical.dtypes
Dummy variables group 
df_dummy = df[[col for col in df if np.isin(df[col].dropna().unique(), [0, 1]).all()]]
for col in df:
    if col in df_dummy.columns:
        df_dummy[col] = df_dummy[col].astype('object')
        
#df_dummy.info() 
#df_dummy.dtypes
### This command changes the values of the dichotomous variable from True\False to numeric numbers - 0,1

#for col in df_dummy:
#    df_dummy[col] = df_dummy[col].map({True: '1',False :'0' })
###command that changes data types methods and missing values coding (<n/a> \ NaN) in accordance with pandas or with numpy
#df_dummy = df_dummy.convert_dtypes()

#df_dummy.head(100)
#df_dummy.dtypes
Only outcome variables group 
df_outcome = pd.DataFrame(df2, columns=["Personality_Disqualification_Dico"])
#df_outcome
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Descriptive Statistics
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
6.1 Basic Descriptive Statistics
Descriptive statistical analysis serves as an introduction to becoming acquainted with the data frame variables. In this section, basic descriptive statistical data for the project variables will be presented using frequency tables.
In general, the objectives of the analyses based on frequency tables in this section are::
•	Initial quality assurance (QA) examinations: Identifying values that appear abnormal (extreme values) or that fall outside the range of the measurement scale for variables on the ordinal/interval scale (such as typing mistakes or errors in data coding).
•	Examining dispersion indices and measures of central tendency::
	For numerical variables: Identifying factors with a negligible/low level of variation, examining variable means, and obtaining a general understanding of their distribution.
	For categorical/dichotomous variables: Analyzing the frequency distribution of variables across different categories, forming an impression of the dispersion of observations within the data frame among the variable classes, and identifying departments with low or negligible representation of participants—later considered for merging with other departments (pertinent to categorical variables only).
	Conducting an initial check for the presence of missing values in all variables within the dataset, and the possibility of identifying which department contributes the most to missing values.
###change variables decimal to only one after period  

df_num.round(1)
df_num.head(3)
     id  Assessment_Center_Grade  Cognitive_Ability_Test  \
0   540                      4.9                     7.0   
1  1128                      4.3                     6.0   
2   604                      4.4                     9.0   

   Morals_and_Values_Test  Job_Interview_Grade  Heberew_Test_Grade  \
0                       4                  4.0                 9.0   
1                       1                  NaN                 8.0   
2                       1                  5.0                 9.0   

   Filling_Instruction_Test  Emotional_Inteligence_Test  \
0                 13.500000                        23.0   
1                       NaN                        15.0   
2                 17.799999                        20.0   

   Recruit_Quality_Measure_Score  Honesty_Test  ...     L    MA    SE    RT  \
0                            NaN             5  ...  56.0  47.0  51.0  35.0   
1                            NaN             8  ...   NaN   NaN   NaN   NaN   
2                            NaN             6  ...  65.0  43.0  35.0  43.0   

   TRIN  VRIN    CR    Q    RS  Personality_Disqualification_Dico  
0  64.0  46.0  53.0  1.0  48.0                                  0  
1   NaN   NaN   NaN  NaN   NaN                                  0  
2  64.0  42.0  34.0  0.0  35.0                                  0  

[3 rows x 62 columns]
6.1 Numeric Variables
df_num.describe().round(2)
            id  Assessment_Center_Grade  Cognitive_Ability_Test  \
count  2300.00                  1312.00                 2040.00   
mean   1150.50                     3.59                    5.50   
std     664.10                     1.05                    1.42   
min       1.00                     1.10                    1.00   
25%     575.75                     2.80                    5.00   
50%    1150.50                     3.60                    5.00   
75%    1725.25                     4.40                    6.00   
max    2300.00                     6.20                    9.00   

       Morals_and_Values_Test  Job_Interview_Grade  Heberew_Test_Grade  \
count                 2300.00              2060.00             2069.00   
mean                     2.42                 4.06                7.34   
std                      2.12                 0.58                0.92   
min                      1.00                 2.00                5.00   
25%                      1.00                 4.00                7.00   
50%                      2.00                 4.00                7.00   
75%                      3.00                 4.00                8.00   
max                     17.00                 6.00                9.00   

       Filling_Instruction_Test  Emotional_Inteligence_Test  \
count                   1596.00                     1269.00   
mean                      11.03                       16.41   
std                        3.60                        2.83   
min                        1.00                       10.00   
25%                        8.50                       14.00   
50%                       11.00                       17.00   
75%                       13.80                       18.00   
max                       20.00                       24.00   

       Recruit_Quality_Measure_Score  Honesty_Test  ...        L       MA  \
count                         353.00       2300.00  ...  2271.00  2271.00   
mean                          111.66          8.67  ...    69.96    55.36   
std                             9.27          1.81  ...    12.45     8.78   
min                           100.00          1.00  ...    33.00    36.00   
25%                           103.00          9.00  ...    61.00    49.00   
50%                           109.00          9.00  ...    70.00    53.00   
75%                           118.00          9.00  ...    78.00    62.00   
max                           145.00         21.00  ...   100.00    94.00   

            SE       RT     TRIN     VRIN       CR        Q       RS  \
count  2271.00  2271.00  2271.00  2271.00  2271.00  2271.00  2271.00   
mean     44.18    42.71    61.65    47.76    46.98     1.53    45.26   
std       7.15     7.02     7.87     9.12     7.58     2.51     9.09   
min      35.00    35.00    50.00    30.00    30.00     0.00    31.00   
25%      40.00    39.00    57.00    42.00    41.00     0.00    38.00   
50%      44.00    43.00    64.00    46.00    46.00     0.00    45.00   
75%      48.00    47.00    65.00    54.00    50.00     2.00    51.00   
max     106.00    84.00   107.00    96.00    86.00    10.00    93.00   

       Personality_Disqualification_Dico  
count                            2300.00  
mean                                0.09  
std                                 0.29  
min                                 0.00  
25%                                 0.00  
50%                                 0.00  
75%                                 0.00  
max                                 1.00  

[8 rows x 62 columns]
6.2 Descriptive Statistics Findings:
Unusual Values:
•	For variables with an interval measurement scale, no values exceed the minimum/maximum values within their measurement range. For example, assessment center scores range from 1 to 7, resulting in a maximum score of 6.2.
•	For variables with a ratio measurement scale, it is evident that the data includes extreme values. For instance, the maximum age among candidates is 65 years old, and the maximum number of years of study is 26 years. These extreme values clearly do not represent the main tendency among the population of candidates for the Israel Police.
- Imortant note : While examining the data table, it was discovered that the "Q" scale is coded on a range of 1 to 10, while the personality scales' range spans from 1 to 120. This inconsistency appears to stem from a coding issue in the police databases. Further investigation revealed that the "Q" personality scale does not significantly contribute to predicting the dependent variable, especially considering the numerous other personality scales present in the dataframe. Consequently, since the "Q" scale is an incorrect variable, it will be omitted from the data frame (Refer to the comprehensive removal of problematic variables at the end of the current chapter, where variables causing issues and technical difficulties in statistical analysis were excluded).
Variables' Variance:
•	In general, an examination of the variables' standard deviations reveals considerable variance, considering the units of measurement for the various variables. This applies to all numerical variables; no variables with zero or negligible dispersion were found. As previously mentioned, it's apparent that there are several variables with a high number of missing values. A more comprehensive discussion about the extent of missing data and strategies for handling it will be presented in the more advanced stages of the EDA analysis.
6.3 Dummy Variables
df_dummy.describe()
        Population_Type_Dico  Gender_Dico  Commander_in_Army_Dico  \
count                 2153.0         2300                  1028.0   
unique                   2.0            2                     2.0   
top                      0.0            1                     0.0   
freq                  2081.0         1453                   776.0   

        Combat_Army_Unit_Dico  M2a_dico  ASP_dico  R_dico  CYN_dico  BIM_dico  \
count                   718.0    2271.0    2271.0  2271.0    2271.0    2271.0   
unique                    2.0       2.0       2.0     2.0       2.0       2.0   
top                       0.0       0.0       0.0     0.0       0.0       0.0   
freq                    379.0    2234.0    2211.0  1219.0    2154.0    2245.0   

        PA_dico  ...  L_dico  MA_dico  SE_dico  RT_dico  TRIN_dico  VRIN_dico  \
count    2271.0  ...  2271.0   2271.0   2271.0   2271.0     2271.0     2271.0   
unique      2.0  ...     2.0      2.0      2.0      2.0        2.0        2.0   
top         0.0  ...     1.0      0.0      0.0      0.0        0.0        0.0   
freq     2195.0  ...  1562.0   1875.0   2247.0   2240.0     1555.0     2130.0   

        CR_dico  Q_dico  RS_dico  Personality_Disqualification_Dico  
count    2271.0  2271.0   2271.0                               2300  
unique      2.0     1.0      2.0                                  2  
top         0.0     0.0      0.0                                  0  
freq     2228.0  2271.0   2195.0                               2089  

[4 rows x 54 columns]
df_dummy.mode()
  Population_Type_Dico Gender_Dico Commander_in_Army_Dico  \
0                  0.0           1                    0.0   

  Combat_Army_Unit_Dico M2a_dico ASP_dico R_dico CYN_dico BIM_dico PA_dico  \
0                   0.0      0.0      0.0    0.0      0.0      0.0     0.0   

   ... L_dico MA_dico SE_dico RT_dico TRIN_dico VRIN_dico CR_dico Q_dico  \
0  ...    1.0     0.0     0.0     0.0       0.0       0.0     0.0    0.0   

  RS_dico Personality_Disqualification_Dico  
0     0.0                                 0  

[1 rows x 54 columns]
# Function to display frequency of categories for dichotomous variables
def display_dichotomous_frequency(dataframe):
    for column in dataframe.columns:
        unique_values = dataframe[column].dropna().unique()
        if len(unique_values) == 2:
            frequencies = dataframe[column].value_counts()
            print(f"Variable: {column}")
            for value, count in frequencies.items():
                print(f"{value}: {count}")
            print("-" * 20)

# Display frequency of categories for dichotomous variables
display_dichotomous_frequency(df_dummy)
Variable: Population_Type_Dico
0.0: 2081
1.0: 72
--------------------
Variable: Gender_Dico
1: 1453
0: 847
--------------------
Variable: Commander_in_Army_Dico
0.0: 776
1.0: 252
--------------------
Variable: Combat_Army_Unit_Dico
0.0: 379
1.0: 339
--------------------
Variable: M2a_dico
0.0: 2234
1.0: 37
--------------------
Variable: ASP_dico
0.0: 2211
1.0: 60
--------------------
Variable: R_dico
0.0: 1219
1.0: 1052
--------------------
Variable: CYN_dico
0.0: 2154
1.0: 117
--------------------
Variable: BIM_dico
0.0: 2245
1.0: 26
--------------------
Variable: PA_dico
0.0: 2195
1.0: 76
--------------------
Variable: M2APS_dico
0.0: 2270
1.0: 1
--------------------
Variable: AAS_dico
0.0: 2263
1.0: 8
--------------------
Variable: tcent_dico
0.0: 2269
1.0: 2
--------------------
Variable: OBS_dico
0.0: 2225
1.0: 46
--------------------
Variable: MDS_dico
0.0: 2146
1.0: 125
--------------------
Variable: TPA_dico
0.0: 2245
1.0: 26
--------------------
Variable: BIZ_dico
0.0: 2164
1.0: 107
--------------------
Variable: ANG_dico
0.0: 2248
1.0: 23
--------------------
Variable: DEP_dico
0.0: 2254
1.0: 17
--------------------
Variable: K_dico
0.0: 1322
1.0: 949
--------------------
Variable: SOD_dico
0.0: 2243
1.0: 28
--------------------
Variable: D_dico
0.0: 2185
1.0: 86
--------------------
Variable: D1_dico
0.0: 2237
1.0: 34
--------------------
Variable: D2_dico
0.0: 2091
1.0: 180
--------------------
Variable: D3_dico
0.0: 1923
1.0: 348
--------------------
Variable: D4_dico
0.0: 2248
1.0: 23
--------------------
Variable: D5_dico
0.0: 2248
1.0: 23
--------------------
Variable: MF_dico
0.0: 2008
1.0: 263
--------------------
Variable: DOE_dico
0.0: 1560
1.0: 711
--------------------
Variable: PT_dico
0.0: 2203
1.0: 68
--------------------
Variable: ANX_dico
0.0: 2246
1.0: 25
--------------------
Variable: F_dico
0.0: 2171
1.0: 100
--------------------
Variable: SC_dico
0.0: 2070
1.0: 201
--------------------
Variable: HEA_dico
0.0: 2199
1.0: 72
--------------------
Variable: FP_dico
0.0: 1949
1.0: 322
--------------------
Variable: HS_dico
0.0: 1828
1.0: 443
--------------------
Variable: FAM_dico
0.0: 2237
1.0: 34
--------------------
Variable: PD_dico
0.0: 2168
1.0: 103
--------------------
Variable: ES_dico
0.0: 2114
1.0: 157
--------------------
Variable: FCENT_dico
1.0: 1781
0.0: 490
--------------------
Variable: WRK_dico
0.0: 2244
1.0: 27
--------------------
Variable: HY_dico
0.0: 2075
1.0: 196
--------------------
Variable: L_dico
1.0: 1562
0.0: 709
--------------------
Variable: MA_dico
0.0: 1875
1.0: 396
--------------------
Variable: SE_dico
0.0: 2247
1.0: 24
--------------------
Variable: RT_dico
0.0: 2240
1.0: 31
--------------------
Variable: TRIN_dico
0.0: 1555
1.0: 716
--------------------
Variable: VRIN_dico
0.0: 2130
1.0: 141
--------------------
Variable: CR_dico
0.0: 2228
1.0: 43
--------------------
Variable: RS_dico
0.0: 2195
1.0: 76
--------------------
Variable: Personality_Disqualification_Dico
0: 2089
1: 211
--------------------
Descriptive statistics of the dichotomous research factors revealed three factors with an unrepresented category (i.e., a category with 0% participants in the category). Naturally, these variables are not valuable for model development. Therefore, these variables will be omitted at the end of this chapter.
The three dichotomous variables without variance are: M2APS_dico; WSD_dico; DIS_dico.
In addition to the above, two dichotomous factors were found to be characterized by a high number of missing values: Combat_Army_Unit_Dico; Commander_in_Army_Dico. A more comprehensive discussion about the missing issue will be provided in the more advanced stages of the EDA analysis.
6.4 Categorical variables
df_categorical_no_target.describe()
               Designated_Role Exam_Language  Education_Level  \
count                     2152          1638             1343   
unique                      11             5                4   
top     operational core units             h  complete_bagrot   
freq                      1214          1617              508   

       Educational_Institution Rovaee_Training_Type  
count                     1336                 1424  
unique                       3                   13  
top                      other             rovaee 2  
freq                       999                  420  
The data in the table above (unique column) indicates that with respect to the categorical variables, there are no variables with unrepresented categories (0%). It was discovered that among the categorical variables, there are 4 factors with a high number of missing values: Education_Level; Educational_Institution; Rovaee_Training_Type; Exam_Language.
In the subsequent stages of the EDA analysis, we will explore the most appropriate methods for handling the missing values in these variables. This might involve options such as completing missing values using a prediction model, filling in with an average or median, or treating the missing values as a distinct category, among others. Alternatively, we will consider the possibility of omitting variables characterized by a high number of missing values.
Categorical variables - frequency distribution tables
# Function to calculate and display relative frequency and percentage
def display_categorical_frequency(df_categorical_no_target, column):
    valid_data = df_categorical_no_target[column].dropna()
    frequencies = valid_data.value_counts()
    total_valid_observations = len(valid_data)
    
    print(f"Variable: {column}")
    for value, count in frequencies.items():
        relative_frequency = count / total_valid_observations
        percentage = (count / total_valid_observations) * 100
        print(f"{value}: Relative Frequency = {relative_frequency:.2f}, Percentage = {percentage:.2f}%")
    print("-" * 20)
# Display relative frequency and percentage for categorical variables
for column in df_categorical_no_target.columns:
    if df_categorical_no_target[column].dtype == 'category':
        display_categorical_frequency(df_categorical_no_target, column)
Variable: Designated_Role
operational core units: Relative Frequency = 0.56, Percentage = 56.41%
Border Guard fighters: Relative Frequency = 0.21, Percentage = 20.91%
administrative nagad: Relative Frequency = 0.10, Percentage = 10.08%
prosecutor: Relative Frequency = 0.04, Percentage = 3.62%
administrative officer: Relative Frequency = 0.03, Percentage = 2.65%
special unit: Relative Frequency = 0.02, Percentage = 1.81%
Police dispatchers: Relative Frequency = 0.02, Percentage = 1.58%
Control center volunteers: Relative Frequency = 0.01, Percentage = 0.93%
Traffic cops: Relative Frequency = 0.01, Percentage = 0.79%
intelligence: Relative Frequency = 0.01, Percentage = 0.79%
operational officer: Relative Frequency = 0.00, Percentage = 0.42%
--------------------
Variable: Exam_Language
h: Relative Frequency = 0.99, Percentage = 98.72%
r: Relative Frequency = 0.01, Percentage = 0.79%
a: Relative Frequency = 0.00, Percentage = 0.24%
e: Relative Frequency = 0.00, Percentage = 0.18%
R: Relative Frequency = 0.00, Percentage = 0.06%
--------------------
Variable: Education_Level
complete_bagrot: Relative Frequency = 0.38, Percentage = 37.83%
without_bagrot: Relative Frequency = 0.27, Percentage = 27.40%
Academic education: Relative Frequency = 0.27, Percentage = 27.25%
diploma: Relative Frequency = 0.08, Percentage = 7.52%
--------------------
Variable: Educational_Institution
other: Relative Frequency = 0.75, Percentage = 74.78%
college: Relative Frequency = 0.22, Percentage = 22.31%
university: Relative Frequency = 0.03, Percentage = 2.92%
--------------------
Variable: Rovaee_Training_Type
rovaee 2: Relative Frequency = 0.29, Percentage = 29.49%
rovaee 0: Relative Frequency = 0.22, Percentage = 22.33%
rovaee 7: Relative Frequency = 0.14, Percentage = 13.97%
rovaee 5: Relative Frequency = 0.12, Percentage = 12.08%
rovaee 3: Relative Frequency = 0.09, Percentage = 8.99%
rovaee 8: Relative Frequency = 0.07, Percentage = 6.53%
rovaee 6: Relative Frequency = 0.03, Percentage = 2.88%
rovaee 12: Relative Frequency = 0.01, Percentage = 1.26%
rovaee 4: Relative Frequency = 0.01, Percentage = 0.91%
rovaee 1: Relative Frequency = 0.01, Percentage = 0.77%
rovaee 10: Relative Frequency = 0.01, Percentage = 0.63%
rovaee 13: Relative Frequency = 0.00, Percentage = 0.07%
rovaee 9: Relative Frequency = 0.00, Percentage = 0.07%
--------------------
Designated_Role - frequency table
df3 = pd.DataFrame(df).groupby(['Designated_Role']).agg({'Designated_Role':'count' })
df3 = df3.rename(columns={'Designated_Role': 'freq'})

df3['constant'] = 1

df3['sum_all'] = df3.groupby('constant')['freq'].transform('sum')
df3.freq = df3.freq.astype('float')
df3.sum_all = df3.sum_all.astype('float')
df3.constant = df3.constant.astype('float')

df3['valid precent'] = (df3.freq / df3.sum_all) * 100
df3.drop(columns=['constant', 'sum_all'])

                             freq  valid precent
Designated_Role                                 
Border Guard fighters       450.0      20.910781
Control center volunteers    20.0       0.929368
Police dispatchers           34.0       1.579926
Traffic cops                 17.0       0.789963
administrative nagad        217.0      10.083643
administrative officer       57.0       2.648699
intelligence                 17.0       0.789963
operational core units     1214.0      56.412639
operational officer           9.0       0.418216
prosecutor                   78.0       3.624535
special unit                 39.0       1.812268
The table above presents the variable 'Designated_Role,' which comprises five categories with a low number of participants: Control center volunteers; police dispatchers; traffic cops; intelligence; operational officer.
Based on the forthcoming findings from subsequent parts of the EDA analysis, it is recommended to explore the feasibility of consolidating departments with relatively low participant numbers and participant categories that involve candidates designated for roles of a similar nature. One approach to consider for merging categories within the variable is to assess the prevalence of personality disorders in each department. This could involve merging departments with low, medium, or high rates of personality disorders.
Exam_Language - frequency table
df3 = pd.DataFrame(df).groupby(['Exam_Language']).agg({'Exam_Language':'count' })
df3 = df3.rename(columns={'Exam_Language': 'freq'})

df3['constant'] = 1

df3['sum_all'] = df3.groupby('constant')['freq'].transform('sum')
df3.freq = df3.freq.astype('float')
df3.sum_all = df3.sum_all.astype('float')
df3.constant = df3.constant.astype('float')

df3['valid precent'] = (df3.freq / df3.sum_all) * 100
df3.drop(columns=['constant', 'sum_all'])
                 freq  valid precent
Exam_Language                       
R                 1.0       0.061050
a                 4.0       0.244200
e                 3.0       0.183150
h              1617.0      98.717949
r                13.0       0.793651
The table indicates that an overwhelming majority of the candidates (98.7%) took the selection tests in the Hebrew language. A negligible proportion of the candidates took tests in Russian, Hebrew, or Amharic. It's important to note that even after merging all categories of candidates who took screening tests in languages other than Hebrew, the combined category will still consist of a low number of only 21 participants.
 Given the limited size of the sample, attempting to establish a connection between the language of the exam and personality disqualification would not yield valid conclusions. To avoid the potential for over-adjustment, it was decided to exclude the exam language factor at this stage of the data analysis (This exclusion will be implemented at the end of this chapter).
Education_Level - frequency table
df3 = pd.DataFrame(df).groupby(['Education_Level']).agg({'Education_Level':'count' })
df3 = df3.rename(columns={'Education_Level': 'freq'})

df3['constant'] = 1

df3['sum_all'] = df3.groupby('constant')['freq'].transform('sum')
df3.freq = df3.freq.astype('float')
df3.sum_all = df3.sum_all.astype('float')
df3.constant = df3.constant.astype('float')

df3['valid precent'] = (df3.freq / df3.sum_all) * 100
df3.drop(columns=['constant', 'sum_all'])
                     freq  valid precent
Education_Level                         
Academic education  366.0      27.252420
complete_bagrot     508.0      37.825763
diploma             101.0       7.520477
without_bagrot      368.0      27.401340
There is a relatively low proportion of participants in the 'diploma holder' category. It is advisable to contemplate merging this category either with the 'matriculation' category or the 'academic rejection' category, depending on the outcomes of the analyses to be conducted in the upcoming EDA sections.
Educational_Institution - frequency table
df3 = pd.DataFrame(df).groupby(['Educational_Institution']).agg({'Educational_Institution':'count' })
df3 = df3.rename(columns={'Educational_Institution': 'freq'})

df3['constant'] = 1

df3['sum_all'] = df3.groupby('constant')['freq'].transform('sum')
df3.freq = df3.freq.astype('float')
df3.sum_all = df3.sum_all.astype('float')
df3.constant = df3.constant.astype('float')

df3['valid precent'] = (df3.freq / df3.sum_all) * 100
df3.drop(columns=['constant', 'sum_all'])
                          freq  valid precent
Educational_Institution                      
college                  298.0      22.305389
other                    999.0      74.775449
university                39.0       2.919162
The table provides insight into the fact that a very small percentage (3%) of the candidates possess a university education. 74% of the candidates fall under the 'other' category, which signifies candidates lacking any form of academic education. The cohort of participants with a university education is limited in size, making it challenging to draw valid conclusions.
 Considering the role of the type of educational institution in predicting personality disqualification, along with forthcoming tests that will yield additional EDA findings, a determination will be made on whether to retain or exclude this factor from the data frame. This decision will factor in the small sample size of participants with university education.
Rovaee_Training_Type - frequency table
df3 = pd.DataFrame(df).groupby(['Rovaee_Training_Type']).agg({'Rovaee_Training_Type':'count' })
df3 = df3.rename(columns={'Rovaee_Training_Type': 'freq'})

df3['constant'] = 1

df3['sum_all'] = df3.groupby('constant')['freq'].transform('sum')
df3.freq = df3.freq.astype('float')
df3.sum_all = df3.sum_all.astype('float')
df3.constant = df3.constant.astype('float')

df3['valid precent'] = (df3.freq / df3.sum_all) * 100
df3.drop(columns=['constant', 'sum_all'])
                       freq  valid precent
Rovaee_Training_Type                      
rovaee 0              318.0      22.331461
rovaee 1               11.0       0.772472
rovaee 10               9.0       0.632022
rovaee 12              18.0       1.264045
rovaee 13               1.0       0.070225
rovaee 2              420.0      29.494382
rovaee 3              128.0       8.988764
rovaee 4               13.0       0.912921
rovaee 5              172.0      12.078652
rovaee 6               41.0       2.879213
rovaee 7              199.0      13.974719
rovaee 8               93.0       6.530899
rovaee 9                1.0       0.070225
Likewise, concerning the variable 'Rovaee_Training_Type,' it has come to light that certain categories display minimal to zero representation of candidates. As previously indicated, aligned with the forthcoming battery of tests slated for the EDA analysis, we will explore the feasibility of introducing weighting considerations amongst the departments that encompass candidates who have undergone combat military training (rovaee 5-13), those who have undergone training as combat support (rovaee 3-4), or individuals in administrative office roles (rovaee 0-2). This evaluation will be predicated on several factors, including the personality disqualification rate within each department.
--- Chapter 6 Summary ---
•	Numerical variables: Indications of extreme values were discovered in a significant portion of the variables (this matter will be thoroughly examined in the more advanced phases of the EDA analysis).
•	Categorical variables: Across all categorical variables, we encountered classes with minimal to low representation of N participants. During the data enrichment and engineering phase, it's imperative to explore the feasibility of consolidating departments with scant participant representation into other departments.
•	Dichotomous variables: In the present stage, we have identified several variables that need to be excluded from the data frame. These include dummy variables with no participants (0%) in a certain category (M2APS_dico; WSD_dico; DIS_dico), as well as an exceptional personality scale 'Q' (an uncommon scale that falls outside the measurement range of the other personality scales).
In light of the above findings, we will now proceed to omit variables for which the decision has been made to exclude them from model development (the rationales for their exclusion are elaborated upon throughout the statistical analyses). Beyond the consideration that variables exhibiting 0% variance or erroneous coding hold no relevance for model development, retaining them within the data frame introduces technical and unnecessary complexities when conducting certain advanced statistical analyses in the context of the EFA stages. As such, the decision has been made to exclude these variables during the "EDA examination phase" rather than postponing their removal until the "EDA treatment phase".
df_dummy = df_dummy.drop(columns=['M2APS_dico', 'WSD_dico', 'DIS_dico', 'Q_dico']);
df2 = df2.drop(columns=['M2APS_dico', 'WSD_dico', 'DIS_dico', 'Q_dico', 'Q', 'Exam_Language']);
df = df.drop(columns=['M2APS_dico', 'WSD_dico', 'DIS_dico', 'Q_dico', 'Q', 'Exam_Language']) ;
df_num = df_num.drop(columns=['Q']);                                
df_num_no_target = df_num_no_target.drop(columns=['Q']);      
df_categorical = df_categorical.drop(columns=['Exam_Language'])
df_categorical_no_target = df_categorical_no_target.drop(columns=['Exam_Language'])
7. Continuous Variables Distribution
# import sweetviz as sv
# #analyzing the dataset
# df_report = sv.analyze(df)
# #display the report
# df_report.show_html('df.html')
describe = df_dummy.iloc[:,:5].describe()
def highlight(cell_value):
    highlight = 'background-color: cyan;'
    default = ''
    if cell_value == describe.loc['mean']['lang_US']:
        return highlight
    else:
        return default  
#describe.style.applymap(highlight)
7.1 Auto EDA Wuth Sweetviz
#!pip install sweetviz
import sweetviz as sv
#statistics_report_before_cleaning = sv.analyze(df)
#statistics_report_before_cleaning.show_html('df.html')
7.2 Numeric variables - Graphs descriptions using histograms
df_num_no_target.iloc[:,1:30].hist(figsize=(25, 25), bins=50, xlabelsize=12, ylabelsize=12);
 
df_num_no_target.iloc[:,:29:-1].hist(figsize=(25, 25), bins=50, xlabelsize=12, ylabelsize=12);
 
Potential Normal Distribution: The charts above display 21 variables with a potential normal distribution, including: Cognitive Ability Test; Emotional Intelligence Test; HEA; R; PA; Tcent; CR; HY; L; MA; HS; PD; ES; FCENT; PT; MF; D; D2; D3; Hebrew Test Grade; Filling Instruction Test.
Positive Asymmetric Distribution: There are 33 charts characterized by a positive asymmetric distribution, including: Morals and Values Test; CYN; Recruit Quality Measure Score; Years of Education; Age; ASP; BIM; M2a; M2ASP; AAS; OBS; MDS; TPA; BIZ; ANG; WSD; DEP; DIS; RS; TRIN; VRIN; WRK; SE; RT; FAM; FP; ANX; F SC; D4; D5; D1; SOD.
Negative Asymmetric Distribution: Two charts exhibit a negative asymmetric distribution, including: K; DOE.
Importance of Independent Variables' Distribution: The nature of variable distributions holds significant implications for the types of analyses performed in model development, including the existence or absence of assumptions serving as the basis for calculating statistical tests (parametric/non-parametric). For instance, variables not following a normal distribution will undergo Spearman testing, while normally distributed variables will employ the Pearson test. Furthermore, distribution nature influences treatment methodologies for extreme values (to be elaborated on in advanced EDA sections).
Additionally, variable distribution nature imparts knowledge about variable variance, as well as the presence and extent of extreme values. A cursory observation reveals substantial value dispersion in most numerical predictors within the project, without any apparent signs of extreme value issues. Only one variable, 'number of years of study,' demonstrates relatively low variation.
As a general principle regarding variable distributions, certain variables (primarily those concerning personality scales) may potentially exhibit extreme values, such as high/low standard grade scores or IQR tests. It is worth noting that most project variables, including personality scales, adhere to a defined range of values. In advanced EDA segments, an exploration will ascertain whether extreme values exist within the aforementioned testing methods. Moreover, it will address the question of whether, within the specific context of the project's objectives, excluding extreme values is appropriate or not. This assessment will determine variables in which extreme values can remain within the data frame, and those where such retention is not recommended.
7.3 Numeric Vars - Skewness measure
Skewness Measure - General Description:
•	Skewness measure provides a crucial indication regarding the nature of variable distribution. A score of 0 in the skewness measure indicates a perfectly normal distribution, characterized by a bell-shaped curve wherein central tendencies (average, median, mode, mid-range) align at the center, and the frequency of values decreases towards the distribution edges.
•	Conversely, a higher skewness measure score corresponds to a greater level of asymmetry in the distribution, particularly a positive (right) skew. Conversely, a lower skewness measure score (below zero) signifies a higher level of negative (left) skewness.
According to Kalima (Klima, 2007):
•	If skewness is close to -1 or 1, the distribution is highly skewed.
•	If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.
•	If skewness is between -0.5 and 0.5, the distribution is approximately symmetric  normal.
def highlight(cell_value):
    highlight = 'background-color: pink;'
    default = ''
    if cell_value >= -0.4 and cell_value <= 0.5:
        return highlight
    else:
        return default
    print("\nModified Stlying DataFrame:")
pd.DataFrame(df_num.skew(),columns=['skewness']).sort_values(by='skewness', ascending=False).style.applymap(highlight)
<pandas.io.formats.style.Styler at 0x20c0359b850>
From the table above, according to the observed results using the skewness meusure, it appears that 16 project variables are normally distributed (skewness between -0.5 to 0.5).
The relevant variables are : CR; HS; MF; D3; HY; D; Honesty Test; Job Interview Grade; L; Emotional Inteligence Test; Assessment Center Grade; Cognitive Ability Test; Heberew Test Grade; Filling Instruction Test; R; D2
As mentioned, when selecting statistical tests for data analysis, consideration must be given to whether the research predictors are normally distributed or not (requiring the use of nonparametric tests). For instance, while performing tasks like multiple linear regression, Pearson's correlation test, logistic regression, and one-way/two-way ANOVA analysis, the distribution of the data becomes crucial. It is important to note that the appropriate statistical test for examining relationships between variables or differences between groups depends on the level of measurement of the variable(s) against which the predictor is being tested. For variables that do not follow a normal distribution, to mitigate the possibility of bias in results, the guiding principle is to resort to non-parametric significance tests. Examples of such tests include the chi-square test, Mann-Whitney U test, Spearman's rank correlation test, and others.
7.4 Kolmogorov-Smirnov Test
Similar to the skewness measure, the Kolmogorov-Smirnov test is designed to assess the nature of variable distributions. However, unlike the skewness index, this test places the concept of normal distribution under a statistical significance examination. In the context of the Kolmogorov-Smirnov test, contrary to many other statistical inference tests, the null hypothesis posits that the variable is not normally distributed in the population. In contrast, the research hypothesis (H1) proposes that the variable follows a normal distribution within the population.
Consequently, a significant outcome in the Kolmogorov-Smirnov test indicates that the variable being evaluated does not adhere to a normal distribution in the population. Conversely, a nonsignificant result suggests the opposite: the variable follows a normal distribution in the population.
from scipy.stats import kstest, norm
# Loop through each column in the DataFrame
for column in df_num.columns:
    # Filter out NaN values if present in the column
    data = df_num[column].dropna()

    # Perform the Kolmogorov-Smirnov test against a normal distribution
    _, p_value = kstest(data, norm.cdf, args=(data.mean(), data.std()))

    # Set the significance level
    significance_level = 0.05

    # Compare p-value with significance level and print the result
    if p_value < significance_level:
        print(f"{column}: Not normally distributed (p-value: {p_value:.4f})")
    else:
        print(f"{column}: Normally distributed (p-value: {p_value:.4f})")
id: Not normally distributed (p-value: 0.0000)
Assessment_Center_Grade: Not normally distributed (p-value: 0.0000)
Cognitive_Ability_Test: Not normally distributed (p-value: 0.0000)
Morals_and_Values_Test: Not normally distributed (p-value: 0.0000)
Job_Interview_Grade: Not normally distributed (p-value: 0.0000)
Heberew_Test_Grade: Not normally distributed (p-value: 0.0000)
Filling_Instruction_Test: Not normally distributed (p-value: 0.0282)
Emotional_Inteligence_Test: Not normally distributed (p-value: 0.0000)
Recruit_Quality_Measure_Score: Not normally distributed (p-value: 0.0000)
Honesty_Test: Not normally distributed (p-value: 0.0000)
Years_of_Education: Not normally distributed (p-value: 0.0000)
Age: Not normally distributed (p-value: 0.0000)
M2a: Not normally distributed (p-value: 0.0000)
ASP: Not normally distributed (p-value: 0.0000)
R: Not normally distributed (p-value: 0.0000)
CYN: Not normally distributed (p-value: 0.0000)
BIM: Not normally distributed (p-value: 0.0000)
PA: Not normally distributed (p-value: 0.0000)
M2APS: Not normally distributed (p-value: 0.0000)
AAS: Not normally distributed (p-value: 0.0000)
tcent: Not normally distributed (p-value: 0.0000)
OBS: Not normally distributed (p-value: 0.0000)
MDS: Not normally distributed (p-value: 0.0000)
TPA: Not normally distributed (p-value: 0.0000)
BIZ: Not normally distributed (p-value: 0.0000)
ANG: Not normally distributed (p-value: 0.0000)
WSD: Not normally distributed (p-value: 0.0000)
DEP: Not normally distributed (p-value: 0.0000)
DIS: Not normally distributed (p-value: 0.0000)
K: Not normally distributed (p-value: 0.0000)
SOD: Not normally distributed (p-value: 0.0000)
D: Not normally distributed (p-value: 0.0000)
D1: Not normally distributed (p-value: 0.0000)
D2: Not normally distributed (p-value: 0.0000)
D3: Not normally distributed (p-value: 0.0000)
D4: Not normally distributed (p-value: 0.0000)
D5: Not normally distributed (p-value: 0.0000)
MF: Not normally distributed (p-value: 0.0000)
DOE: Not normally distributed (p-value: 0.0000)
PT: Not normally distributed (p-value: 0.0000)
ANX: Not normally distributed (p-value: 0.0000)
F: Not normally distributed (p-value: 0.0000)
SC: Not normally distributed (p-value: 0.0000)
HEA: Not normally distributed (p-value: 0.0000)
FP: Not normally distributed (p-value: 0.0000)
HS: Not normally distributed (p-value: 0.0000)
FAM: Not normally distributed (p-value: 0.0000)
PD: Not normally distributed (p-value: 0.0000)
ES: Not normally distributed (p-value: 0.0000)
FCENT: Not normally distributed (p-value: 0.0000)
WRK: Not normally distributed (p-value: 0.0000)
HY: Not normally distributed (p-value: 0.0000)
L: Not normally distributed (p-value: 0.0000)
MA: Not normally distributed (p-value: 0.0000)
SE: Not normally distributed (p-value: 0.0000)
RT: Not normally distributed (p-value: 0.0000)
TRIN: Not normally distributed (p-value: 0.0000)
VRIN: Not normally distributed (p-value: 0.0000)
CR: Not normally distributed (p-value: 0.0000)
RS: Not normally distributed (p-value: 0.0000)
Personality_Disqualification_Dico: Not normally distributed (p-value: 0.0000)
Unlike the findings obtained from the skewness measure test, the table above shows that the Kolmogorov-Smirnov test found significant results for all of the project factors. Therefore, while the skewness measure examination identified 16 independent variables as normally distributed, the Kolmogorov-Smirnov test presents a different outcome. According to this test, no project variables found to be normally distributed.
The discrepancy between the findings obtained using the skewness measure compared to the Kolmogorov-Smirnov test may arise from the fact that the latter test employs a distinct calculation method and is more stringent compared to the skewness measure. Another explanation could be that the Kolmogorov-Smirnov test is based on an assessment of statistical significance. In this context, it's known that as statistical significance tests utilize larger participant samples, the test's statistical power proportionally increases. Consequently, the likelihood of detecting a significant effect grows, even for minor or seemingly insignificant differences. Hence, when dealing with substantial participant samples, the emphasis often shifts towards assessing effect sizes and the strength of connections between variables, with less focus on statistical significance.
Considering statistical power, a sample comprising 2,300 participants possesses relatively high statistical power. Consequently, this high power results in the Kolmogorov-Smirnov test easily detecting significant effects (being sensitive to small differences), even in relation to variables that are approximately normally distributed.
In order to illustrate the aforementioned assertions, we conducted the Kolmogorov-Smirnov test once more, this time with a subset of only 500 participants that were randomly sampled from the larger dataset. The outcomes of this analysis, performed on this relatively smaller sample size, are presented in the table below:
# Randomly sample rows from the DataFrame
sample_size = 500
random_indices = np.random.choice(df_num.index, size=sample_size, replace=False)
sampled_df = df_num.loc[random_indices]

# Loop through each column in the sampled DataFrame
for column in df_num.columns:
    # Filter out NaN values if present in the column
    data = sampled_df[column].dropna()

    # Perform the Kolmogorov-Smirnov test against a normal distribution
    _, p_value = kstest(data, 'norm', args=(data.mean(), data.std()))

    # Set the significance level
    significance_level = 0.05

    # Compare p-value with significance level and print the result
    if p_value < significance_level:
        print(f"{column}: Not normally distributed (p-value: {p_value:.4f})")
    else:
        print(f"{column}: Normally distributed (p-value: {p_value:.4f})")
id: Not normally distributed (p-value: 0.0215)
Assessment_Center_Grade: Normally distributed (p-value: 0.1210)
Cognitive_Ability_Test: Not normally distributed (p-value: 0.0000)
Morals_and_Values_Test: Not normally distributed (p-value: 0.0000)
Job_Interview_Grade: Not normally distributed (p-value: 0.0000)
Heberew_Test_Grade: Not normally distributed (p-value: 0.0000)
Filling_Instruction_Test: Normally distributed (p-value: 0.5978)
Emotional_Inteligence_Test: Not normally distributed (p-value: 0.0274)
Recruit_Quality_Measure_Score: Not normally distributed (p-value: 0.0111)
Honesty_Test: Not normally distributed (p-value: 0.0000)
Years_of_Education: Not normally distributed (p-value: 0.0000)
Age: Not normally distributed (p-value: 0.0000)
M2a: Not normally distributed (p-value: 0.0000)
ASP: Not normally distributed (p-value: 0.0000)
R: Not normally distributed (p-value: 0.0242)
CYN: Not normally distributed (p-value: 0.0001)
BIM: Not normally distributed (p-value: 0.0000)
PA: Not normally distributed (p-value: 0.0000)
M2APS: Not normally distributed (p-value: 0.0000)
AAS: Not normally distributed (p-value: 0.0000)
tcent: Not normally distributed (p-value: 0.0000)
OBS: Not normally distributed (p-value: 0.0000)
MDS: Not normally distributed (p-value: 0.0000)
TPA: Not normally distributed (p-value: 0.0000)
BIZ: Not normally distributed (p-value: 0.0000)
ANG: Not normally distributed (p-value: 0.0000)
WSD: Not normally distributed (p-value: 0.0000)
DEP: Not normally distributed (p-value: 0.0000)
DIS: Not normally distributed (p-value: 0.0000)
K: Not normally distributed (p-value: 0.0002)
SOD: Not normally distributed (p-value: 0.0000)
D: Not normally distributed (p-value: 0.0024)
D1: Not normally distributed (p-value: 0.0000)
D2: Not normally distributed (p-value: 0.0000)
D3: Not normally distributed (p-value: 0.0000)
D4: Not normally distributed (p-value: 0.0000)
D5: Not normally distributed (p-value: 0.0000)
MF: Not normally distributed (p-value: 0.0000)
DOE: Not normally distributed (p-value: 0.0000)
PT: Not normally distributed (p-value: 0.0021)
ANX: Not normally distributed (p-value: 0.0000)
F: Not normally distributed (p-value: 0.0000)
SC: Not normally distributed (p-value: 0.0000)
HEA: Not normally distributed (p-value: 0.0000)
FP: Not normally distributed (p-value: 0.0000)
HS: Not normally distributed (p-value: 0.0004)
FAM: Not normally distributed (p-value: 0.0000)
PD: Not normally distributed (p-value: 0.0257)
ES: Not normally distributed (p-value: 0.0000)
FCENT: Not normally distributed (p-value: 0.0000)
WRK: Not normally distributed (p-value: 0.0000)
HY: Not normally distributed (p-value: 0.0207)
L: Not normally distributed (p-value: 0.0006)
MA: Not normally distributed (p-value: 0.0000)
SE: Not normally distributed (p-value: 0.0000)
RT: Not normally distributed (p-value: 0.0000)
TRIN: Not normally distributed (p-value: 0.0000)
VRIN: Not normally distributed (p-value: 0.0000)
CR: Not normally distributed (p-value: 0.0002)
RS: Not normally distributed (p-value: 0.0000)
Personality_Disqualification_Dico: Not normally distributed (p-value: 0.0000)
Indeed, as evident from the data, with a reduced sample size of only 500 participants, the count of not significant variables in the Kolmogorov-Smirnov test has risen from one to three variables (Assessment_Center_Grade; Filling_Instruction_Test; Recruit_Quality_Measure_Score, p>0.05). This outcome implies that when dealing with a smaller sample, the Smirnov Kolmogorov test identified three variables that adhere to a normal distribution pattern.
Now, let's perform the test on a sample of only 250 participants (further reducing the statistical power of the predictive test):

# Randomly sample rows from the DataFrame
sample_size = 250
random_indices = np.random.choice(df_num.index, size=sample_size, replace=False)
sampled_df = df_num.loc[random_indices]

# Loop through each column in the sampled DataFrame
for column in df_num.columns:
    # Filter out NaN values if present in the column
    data = sampled_df[column].dropna()

    # Perform the Kolmogorov-Smirnov test against a normal distribution
    _, p_value = kstest(data, 'norm', args=(data.mean(), data.std()))

    # Set the significance level
    significance_level = 0.05

    # Compare p-value with significance level and print the result
    if p_value < significance_level:
        print(f"{column}: Not normally distributed (p-value: {p_value:.4f})")
    else:
        print(f"{column}: Normally distributed (p-value: {p_value:.4f})")
id: Normally distributed (p-value: 0.1306)
Assessment_Center_Grade: Normally distributed (p-value: 0.2503)
Cognitive_Ability_Test: Not normally distributed (p-value: 0.0001)
Morals_and_Values_Test: Not normally distributed (p-value: 0.0000)
Job_Interview_Grade: Not normally distributed (p-value: 0.0000)
Heberew_Test_Grade: Not normally distributed (p-value: 0.0000)
Filling_Instruction_Test: Normally distributed (p-value: 0.9630)
Emotional_Inteligence_Test: Normally distributed (p-value: 0.0892)
Recruit_Quality_Measure_Score: Normally distributed (p-value: 0.4217)
Honesty_Test: Not normally distributed (p-value: 0.0000)
Years_of_Education: Not normally distributed (p-value: 0.0000)
Age: Not normally distributed (p-value: 0.0000)
M2a: Not normally distributed (p-value: 0.0000)
ASP: Not normally distributed (p-value: 0.0060)
R: Normally distributed (p-value: 0.0504)
CYN: Normally distributed (p-value: 0.1077)
BIM: Not normally distributed (p-value: 0.0000)
PA: Not normally distributed (p-value: 0.0059)
M2APS: Not normally distributed (p-value: 0.0000)
AAS: Not normally distributed (p-value: 0.0000)
tcent: Not normally distributed (p-value: 0.0166)
OBS: Not normally distributed (p-value: 0.0000)
MDS: Not normally distributed (p-value: 0.0001)
TPA: Not normally distributed (p-value: 0.0010)
BIZ: Not normally distributed (p-value: 0.0000)
ANG: Not normally distributed (p-value: 0.0004)
WSD: Not normally distributed (p-value: 0.0000)
DEP: Not normally distributed (p-value: 0.0000)
DIS: Not normally distributed (p-value: 0.0000)
K: Normally distributed (p-value: 0.2692)
SOD: Not normally distributed (p-value: 0.0000)
D: Not normally distributed (p-value: 0.0438)
D1: Not normally distributed (p-value: 0.0002)
D2: Not normally distributed (p-value: 0.0001)
D3: Not normally distributed (p-value: 0.0000)
D4: Not normally distributed (p-value: 0.0000)
D5: Not normally distributed (p-value: 0.0000)
MF: Not normally distributed (p-value: 0.0097)
DOE: Not normally distributed (p-value: 0.0000)
PT: Not normally distributed (p-value: 0.0090)
ANX: Not normally distributed (p-value: 0.0001)
F: Not normally distributed (p-value: 0.0000)
SC: Not normally distributed (p-value: 0.0000)
HEA: Not normally distributed (p-value: 0.0016)
FP: Not normally distributed (p-value: 0.0000)
HS: Normally distributed (p-value: 0.0657)
FAM: Not normally distributed (p-value: 0.0009)
PD: Not normally distributed (p-value: 0.0015)
ES: Not normally distributed (p-value: 0.0112)
FCENT: Not normally distributed (p-value: 0.0164)
WRK: Not normally distributed (p-value: 0.0000)
HY: Normally distributed (p-value: 0.0621)
L: Normally distributed (p-value: 0.1512)
MA: Not normally distributed (p-value: 0.0009)
SE: Not normally distributed (p-value: 0.0001)
RT: Not normally distributed (p-value: 0.0000)
TRIN: Not normally distributed (p-value: 0.0000)
VRIN: Not normally distributed (p-value: 0.0000)
CR: Normally distributed (p-value: 0.0705)
RS: Not normally distributed (p-value: 0.0001)
Personality_Disqualification_Dico: Not normally distributed (p-value: 0.0000)
In line with the explanations provided in the preceding paragraph, it becomes apparent that when considering the relatively limited sample of participants, the Smirnov Kolmogorov test has identified 9 variables without statistical significance, suggesting that these variables adhere to a normal distribution pattern within the population. The relatively elevated count of significant factors in the Smirnov Kolmogorov test does not stem from "selection" bias, which pertains to variations in the target factor due to distinctions in sample characteristics. Instead, it predominantly arises due to the application of the statistical test on a sample containing just 250 participants, consequently leading to a reduction in the statistical power of the inference test.
Given the test outcomes and the comprehensive explanations elucidated above, it seems that the Skewness index may presents a more reliable and pertinent result concerning the project variables distribution (normal  not normal) within the population.
--- Chapter 7 Summary ---
•	An examination of the distribution of the numerical variables under study has revealed a favorable dispersion of variable values. A substantial proportion of variables exhibit an approximately normal distribution pattern. The exploration of these distributions further indicates a satisfactory scattering of the numerical variables across their ranges.
•	Utilizing the skewness measure, it was deduced that 11 of the study's predictors follow a normal distribution. In contrast, the outcomes derived from the Smirnov Kolmogorov test indicate that only 3 predictors adhere to a normal distribution within the population. It's worth noting that the Smirnov-Kolmogorov test becomes more stringent with an increase in the number of observations (N) included in the analysis. As the number of participants grows, the statistical power of the inference test increases, enhancing the likelihood of detecting significant effects, even when these effects possess minor strength.
8. Frequencies tables of dummy variables
#a rows number    #b columns number    #c plot counter initialize

a = 20   
b = 3   
c = 1   

fig = plt.figure(figsize=(30,65))
plt.subplots_adjust(hspace = 1)
sns.set(font_scale = 1.5)
for i in df_dummy:
    if i != 'Personality_Disqualification_Dico':
        
        plt.subplot(a, b, c)
        plt.title('{}'.format(i))
        plt.xlabel(i)
        sns.countplot(df_dummy[i], palette=['Gray','black'])
        c = c + 1
        
fig.tight_layout()
plt.show()
 
The charts above reveal a substantial portion of dummy variables exhibiting a sub-categorization within a single category. 
This phenomenon is particularly notable in relation to the MMPI-2 test personality scale variables. The significance and relevance of such variables warrant closer examination during the more advanced stages of the Exploratory Data Analysis (EDA).For instance, an investigation into their unique contribution towards predicting the target variable becomes impotrtant consideration in decisions regarding the retention or exclusion of these variables from the data frame.
In light of the above discussion, it's essential to highlight that even with a limited number of cases (small N), abnormally high or elevated scores in personality scales hold significant predictive value for personality disqualification. Consequently, it remains challenging at this juncture to definitively decide whether retaining or omitting personality scales with underrepresented categories is the correct approach. Further investigation and analysis are warranted to make an informed determination regarding the inclusion or exclusion of such variables.
Despite the initial impressions drawn from the visual examination of the charts, it is crucial to emphasize that there are no departments completely devoid of data in the dichotomous project variables at this juncture. Any variables containing categories with zero percent representation were previously removed in an earlier stage of the analysis. While there are personality scales that exhibit very low representation, the subsequent stages of data enrichment and engineering will involve techniques aimed at mitigating any potential bias stemming from the small number of participants (e.g., utilizing an index variable that factors in the cumulative count of personality scale elevations). These practices are intended to ensure the robustness of the analysis despite the limited N participants in these cases.
8.1 Categorical variables destributions in graphs (Bar Charts)
c = 1

fig = plt.figure(figsize=(30,45))
for i in df_categorical_no_target:
    if i != 'Personality_Disqualification_Dico':
        plt.subplot(6, 1, c)
        plt.title('{}'.format(i))
        plt.xlabel(i)
        sns.set(font_scale = 1.5)
        sns.countplot(df_categorical_no_target[i])
        c = c + 1

plt.show()
 
From the charts above, we can glean the following insights:
Designated_Role variable: A significant majority of candidates are concentrated within the operational core units and border guard fighters categories. Notably, a few departments display a lower count of participants, underscoring the need to explore potential adjustments in weight between these smaller departments and their more populous counterparts.
Process_Type variable: The "miun nagadim" category commands a majority of candidates, whereas the "miun kzinim" categories exhibit relatively fewer participants.
Rovaee_Training_Type: The provided table highlights that a mere 3% of candidates possess a university education. Additionally, a substantial 74% of participants with spouses are classified within the "other" category, representing individuals devoid of academic education. The presence of candidates with a university education is notably limited, introducing challenges in drawing robust conclusions.
As previously alluded to, the decision regarding the retention or omission of the type of educational institution as a factor in the data frame hinges on its predictive significance for personality disqualification. This determination will also consider supplementary tests conducted in subsequent EDA stages, while also acknowledging the relatively modest sample size of participants with a university education.
--- Chapter 8 Summary ---
- We have identified a significant portion of the dummy variables that exhibit sub-classifications within one of the categories, particularly evident in the MMPI-2 test personality scale variables. The need and contribution of such variables will be further investigated in the subsequent stages of the EDA analysis.
- As evidenced in earlier sections, we have identified a noteworthy number of categories within the categorical variables that feature a low count of participants. It is imperative to explore the feasibility of merging these smaller departments with other, more substantial departments.
9. Description of the target variable
9.1 Below, a histogram of the variable "Personality_Disqualification_Dico" will be displayed:
sns.set_theme(style="whitegrid")

g = sns.catplot(data=df_num, x="Personality_Disqualification_Dico",\
    kind="count", palette="pastel", alpha=.6, height=4.5)


g.despine(left=True)
g.set_axis_labels("", "Personality_Disqualification_Dico (g)")
<seaborn.axisgrid.FacetGrid at 0x20c0b332f40>
 
depen_table = pd.DataFrame(df).groupby(['Personality_Disqualification_Dico']).agg({'Personality_Disqualification_Dico':'count' })
depen_table = depen_table.rename(columns={'Personality_Disqualification_Dico': 'freq'})

depen_table['constant'] = 1

depen_table['sum_all'] = depen_table.groupby('constant')['freq'].transform('sum')
depen_table.freq = depen_table.freq.astype('float')
depen_table.sum_all = depen_table.sum_all.astype('float')
depen_table.constant = depen_table.constant.astype('float')

depen_table['valid precent'] = (depen_table.freq / depen_table.sum_all) * 100
depen_table.drop(columns=['constant', 'sum_all'])
                                     freq  valid precent
Personality_Disqualification_Dico                       
0                                  2089.0      90.826087
1                                   211.0       9.173913
Findings:
 The target variable categories in the current project are imbalanced, with the "Rare" category accounting for less than 15% of the data. Specifically, only 9.2% of participants in the data frame were personality disqualified (coded as 1), while the remaining 90.8% were candidates who were not personality disqualified (coded as 0).
 To mitigate issues arising from sub-classification, records of participants who were personality disqualified (coded as 1) will be duplicated using four resampling methods exclusively within the training dataset. These methods include under-sampling, oversampling, a combination of over and under sampling, and the Synthetic Minority Over-sampling Technique (SMOTE). An additional method based on the random forest classifier from the scikit-learn library in Python will also be employed. This method automatically balances the class distribution. After applying these resampling methods, we will select the method that yields the highest accuracy results.
To address the issue of underclassification in model development, we will explore various strategies, including:
Threshold Adjustment: We will assess the impact of changing the threshold point used by predictive models to classify the classes of the target variable. Modifying this threshold can lead to different classification outcomes, potentially improving the balance between the two categories.
Model Parameters: We will investigate the utilization of model parameters that impose penalties in cases of sub-classification of the rare target factor categories. This penalty mechanism can help the model assign higher costs to misclassifications of the minority class, encouraging more balanced predictions.
Duplicate / omit observations : A random sub-sample of participants will be utilized if duplicating records of the rare category fails to address the sub-classification issues. Such issues might manifest as prediction models significantly favoring one category over the other, particularly the rare category of personality disqualification. If such problems arise even after duplicating rare category observations, we will also explore random omission of records from the common target factor category.
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
10. Predictive Statistics - Correlations Between Independents
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
10. Numeric Variables Internal Correlations
10.1.1 Correlations Matrix
The measurement scale of the numerical variables in the project is generally considered to be interval, which allows for the application of parametric tests based on the disciplinary perspective of the social sciences. Despite this, when investigating the relationship between the project predictors and the outcome factor (personality disqualification), we opted to utilize the Spearman test.
It's important to clarify that the decision to employ the non-parametric Spearman test was not solely influenced by the measurement scale of the variables (whether they are interval or ordinal),but rather by the distribution characteristics of the variables. In light of the skewness measure, particularly the results obtained from the Smirnov Kolomorov test, it becomes evident that most of the continuous research variables deviate from a normal distribution. Consequently, the selection of the Spearman test is justified, as it is appropriate when the normality assumptions are not met and when variables exhibit non-normal distributions.
df_num_corr = pd.DataFrame(df_num)
corr = df_num.corr(method = 'spearman')
corr
                                         id  Assessment_Center_Grade  \
id                                 1.000000                -0.002351   
Assessment_Center_Grade           -0.002351                 1.000000   
Cognitive_Ability_Test             0.008071                 0.244644   
Morals_and_Values_Test             0.002597                -0.224451   
Job_Interview_Grade                0.003951                 0.344213   
...                                     ...                      ...   
TRIN                               0.020258                 0.066066   
VRIN                               0.009624                -0.200421   
CR                                 0.013654                 0.021571   
RS                                -0.011297                -0.099990   
Personality_Disqualification_Dico -0.070461                 0.090582   

                                   Cognitive_Ability_Test  \
id                                               0.008071   
Assessment_Center_Grade                          0.244644   
Cognitive_Ability_Test                           1.000000   
Morals_and_Values_Test                          -0.159557   
Job_Interview_Grade                              0.147793   
...                                                   ...   
TRIN                                            -0.003523   
VRIN                                            -0.207374   
CR                                               0.034011   
RS                                              -0.167033   
Personality_Disqualification_Dico               -0.090770   

                                   Morals_and_Values_Test  \
id                                               0.002597   
Assessment_Center_Grade                         -0.224451   
Cognitive_Ability_Test                          -0.159557   
Morals_and_Values_Test                           1.000000   
Job_Interview_Grade                             -0.149521   
...                                                   ...   
TRIN                                            -0.075911   
VRIN                                             0.300640   
CR                                               0.149548   
RS                                               0.152414   
Personality_Disqualification_Dico                0.149527   

                                   Job_Interview_Grade  Heberew_Test_Grade  \
id                                            0.003951           -0.083692   
Assessment_Center_Grade                       0.344213            0.405271   
Cognitive_Ability_Test                        0.147793            0.421445   
Morals_and_Values_Test                       -0.149521           -0.241243   
Job_Interview_Grade                           1.000000            0.210137   
...                                                ...                 ...   
TRIN                                          0.045647            0.061105   
VRIN                                         -0.135507           -0.298563   
CR                                           -0.049276            0.027538   
RS                                           -0.063507           -0.167793   
Personality_Disqualification_Dico            -0.197623           -0.108073   

                                   Filling_Instruction_Test  \
id                                                -0.013057   
Assessment_Center_Grade                            0.282423   
Cognitive_Ability_Test                             0.791452   
Morals_and_Values_Test                            -0.173727   
Job_Interview_Grade                                0.187009   
...                                                     ...   
TRIN                                               0.022662   
VRIN                                              -0.211726   
CR                                                 0.007689   
RS                                                -0.161905   
Personality_Disqualification_Dico                 -0.096404   

                                   Emotional_Inteligence_Test  \
id                                                  -0.554531   
Assessment_Center_Grade                              0.241169   
Cognitive_Ability_Test                               0.347647   
Morals_and_Values_Test                              -0.158706   
Job_Interview_Grade                                  0.104094   
...                                                       ...   
TRIN                                                -0.005057   
VRIN                                                -0.281706   
CR                                                   0.016574   
RS                                                  -0.106074   
Personality_Disqualification_Dico                   -0.120278   

                                   Recruit_Quality_Measure_Score  \
id                                                     -0.998322   
Assessment_Center_Grade                                 0.380366   
Cognitive_Ability_Test                                  0.214146   
Morals_and_Values_Test                                 -0.060961   
Job_Interview_Grade                                     0.128126   
...                                                          ...   
TRIN                                                   -0.028587   
VRIN                                                   -0.078322   
CR                                                     -0.047589   
RS                                                     -0.046540   
Personality_Disqualification_Dico                      -0.048045   

                                   Honesty_Test  ...        HY         L  \
id                                    -0.064169  ... -0.018222  0.048332   
Assessment_Center_Grade               -0.092052  ...  0.090045 -0.169947   
Cognitive_Ability_Test                -0.103923  ... -0.072502 -0.139547   
Morals_and_Values_Test                 0.039017  ... -0.153655 -0.125750   
Job_Interview_Grade                   -0.090921  ...  0.078857 -0.078573   
...                                         ...  ...       ...       ...   
TRIN                                  -0.038765  ...  0.230551  0.123095   
VRIN                                   0.033310  ... -0.090485 -0.138636   
CR                                    -0.030656  ... -0.370408 -0.339901   
RS                                     0.019143  ... -0.027593 -0.229468   
Personality_Disqualification_Dico      0.017369  ...  0.009719 -0.057639   

                                         MA        SE        RT      TRIN  \
id                                 0.020687 -0.055985 -0.002891  0.020258   
Assessment_Center_Grade           -0.068950 -0.066157 -0.232083  0.066066   
Cognitive_Ability_Test             0.000925 -0.065942 -0.206577 -0.003523   
Morals_and_Values_Test             0.169571  0.226179  0.333854 -0.075911   
Job_Interview_Grade               -0.030884 -0.114803 -0.160134  0.045647   
...                                     ...       ...       ...       ...   
TRIN                              -0.106586 -0.139707 -0.084878  1.000000   
VRIN                               0.143960  0.340840  0.397131  0.162375   
CR                                 0.417882  0.234935  0.161156 -0.205215   
RS                                -0.028947  0.280560  0.316051 -0.037487   
Personality_Disqualification_Dico  0.068525  0.119030  0.152529  0.003709   

                                       VRIN        CR        RS  \
id                                 0.009624  0.013654 -0.011297   
Assessment_Center_Grade           -0.200421  0.021571 -0.099990   
Cognitive_Ability_Test            -0.207374  0.034011 -0.167033   
Morals_and_Values_Test             0.300640  0.149548  0.152414   
Job_Interview_Grade               -0.135507 -0.049276 -0.063507   
...                                     ...       ...       ...   
TRIN                               0.162375 -0.205215 -0.037487   
VRIN                               1.000000  0.110785  0.281968   
CR                                 0.110785  1.000000  0.054015   
RS                                 0.281968  0.054015  1.000000   
Personality_Disqualification_Dico  0.123865  0.062122  0.087100   

                                   Personality_Disqualification_Dico  
id                                                         -0.070461  
Assessment_Center_Grade                                     0.090582  
Cognitive_Ability_Test                                     -0.090770  
Morals_and_Values_Test                                      0.149527  
Job_Interview_Grade                                        -0.197623  
...                                                              ...  
TRIN                                                        0.003709  
VRIN                                                        0.123865  
CR                                                          0.062122  
RS                                                          0.087100  
Personality_Disqualification_Dico                           1.000000  

[61 rows x 61 columns]
corr = df_num.corr(method = 'spearman')
corr.style.background_gradient(cmap='coolwarm').set_precision(2)
<pandas.io.formats.style.Styler at 0x20c5149cbe0>
In the diagram above, the shading of the boxes corresponds to the strength of the linear relationship between the variables. A lighter and bluer shade indicates a weaker linear relationship, while a darker and redder shade indicates a stronger linear relationship. It's important to note that correlations with a value of 1 are not meaningful as they indicate a relationship between identical variables.
Several independent variables exhibit a notably strong correlation with each other. This topic will be further explored in the subsequent section that addresses multicollinearity.
10.1.2 Multicollinearity examination
The table below displays the Spearman correlations among all the numerical variables in the project. Our aim is to identify variables with multicollinear relationships, specifically when the positive or negative correlation surpasses a threshold of +/-0.8.
Multicollinearity, in essence, signifies a scenario in which two or more variables included in a model explain similar variances in the dependent variable. This arises when the variables either represent the same content or are extremely similar. Such a situation can introduce bias and compromise the predictive accuracy of models, particularly those founded on the assumption of a linear connection between predictors and the target variable. This also impacts the assessment of their distinctive contributions towards predicting the target variable. This phenomenon is evident in multivariate linear regression, where incorporating variables with multicollinear relationships can lead to distorted estimations.
var_correlations = pd.DataFrame(columns=['var1','var2','Spearman correlation','p-value'])
all_num_df = df.drop(columns=df_categorical.columns)
for i in df_num.columns:
        for j in df_num.columns:
            b = "{}/{}".format(i,j)
            c = "{}/{}".format(j,i)
            if i != j and i != 'id' :
                if (c not in var_correlations.index):
                    mask = ~pd.isna(df_num[i]) & ~pd.isna(df_num[j]) 
                    a = stats.spearmanr(df_num[i][mask], df_num[j][mask])
                    var_correlations.loc[b] = [i,j,abs(a[0]),a[1]]
        
var_correlations_filtered = var_correlations.loc[(var_correlations['Spearman correlation'] > 0.7) & (var_correlations['p-value'] < 0.05)]
var_correlations_filtered = var_correlations_filtered.sort_values(by=['Spearman correlation'], ascending=False)
var_correlations_filtered = var_correlations_filtered.iloc[1:]
var_correlations_filtered
                                                                   var1  \
tcent/FCENT                                                       tcent   
K/FCENT                                                               K   
tcent/K                                                           tcent   
ASP/CYN                                                             ASP   
BIM/SC                                                              BIM   
M2a/OBS                                                             M2a   
Cognitive_Ability_Test/Filling_Instruction_Test  Cognitive_Ability_Test   
CYN/K                                                               CYN   
WSD/D1                                                              WSD   
M2a/FCENT                                                           M2a   
M2a/tcent                                                           M2a   
TPA/FCENT                                                           TPA   
tcent/TPA                                                         tcent   
M2a/WRK                                                             M2a   
M2a/K                                                               M2a   
R/FCENT                                                               R   
R/tcent                                                               R   
CYN/FCENT                                                           CYN   
CYN/tcent                                                           CYN   

                                                                     var2  \
tcent/FCENT                                                         FCENT   
K/FCENT                                                             FCENT   
tcent/K                                                                 K   
ASP/CYN                                                               CYN   
BIM/SC                                                                 SC   
M2a/OBS                                                               OBS   
Cognitive_Ability_Test/Filling_Instruction_Test  Filling_Instruction_Test   
CYN/K                                                                   K   
WSD/D1                                                                 D1   
M2a/FCENT                                                           FCENT   
M2a/tcent                                                           tcent   
TPA/FCENT                                                           FCENT   
tcent/TPA                                                             TPA   
M2a/WRK                                                               WRK   
M2a/K                                                                   K   
R/FCENT                                                             FCENT   
R/tcent                                                             tcent   
CYN/FCENT                                                           FCENT   
CYN/tcent                                                           tcent   

                                                 Spearman correlation  p-value  
tcent/FCENT                                                  0.994675      0.0  
K/FCENT                                                      0.803523      0.0  
tcent/K                                                      0.801835      0.0  
ASP/CYN                                                      0.801572      0.0  
BIM/SC                                                       0.799659      0.0  
M2a/OBS                                                      0.796712      0.0  
Cognitive_Ability_Test/Filling_Instruction_Test              0.791452      0.0  
CYN/K                                                        0.777048      0.0  
WSD/D1                                                       0.758732      0.0  
M2a/FCENT                                                    0.743031      0.0  
M2a/tcent                                                    0.740722      0.0  
TPA/FCENT                                                    0.735001      0.0  
tcent/TPA                                                    0.734536      0.0  
M2a/WRK                                                      0.732426      0.0  
M2a/K                                                        0.727303      0.0  
R/FCENT                                                      0.724821      0.0  
R/tcent                                                      0.720545      0.0  
CYN/FCENT                                                    0.709016      0.0  
CYN/tcent                                                    0.708164      0.0  
The table presented above highlights a total of 4 instances of multicollinear relationships (Spearman corr=>0.8). In addition, 15 more variables is on the verge of multicollinearity (0.7 to 0.7999). Variables that exhibit multicollinearity contribute to explaining the same variance in the dependent variable. Additionally, notably, a significant portion of these multicollinear relationships are observed among the personality scales within the MMPI-2 test.
These findings bear substantial significance for the later stages of this project. They will be particularly influential during phases such as data enrichment, data engineering, feature selection, and more. As we progress, we will integrate these findings into the model training process. This integration may involve exploratory factor analysis, regression through stepwise methods, creating indices or composite measures for weighting personality scales, and various other approaches.
The results of these statistical tests will guide our decisions regarding whether to assign weight to multicollinear variables, and if so, which methodology to adopt. Alternatively, based on the outcomes of the tests, we will assess whether omitting multicollinear variables entirely is a more suitable solution, leaving only one representative variable in the dataset. In such cases, we will make informed choices about which variable to retain and which to exclude.
11. category  Dummy Variables Internal correlations
11.1 Dummy variables internal crrelations
Chi-Square Test
def cramers_corrected_stat(df):
    chi2_df = pd.DataFrame(columns=['var1','var2','p-value','Chi2 Statistic'])
    for i in df:
        if ((i in df.columns) and i != 'Personality_Disqualification_Dico'):
            for j in df:
                if (i in df.columns) and j != 'Personality_Disqualification_Dico' and i < j:
                    confusion_matrix = pd.crosstab(df[i], df[j])
                    chi_res = chi2_contingency(confusion_matrix)
                    b = "{}/{}".format(i,j)
                    if chi_res[1] < 0.05:
                        chi2_df.loc[b] = [i,j,chi_res[1],chi_res[0]]
                    
    return chi2_df.sort_values(by='p-value')

dummy_internal_corr = cramers_corrected_stat(df_dummy)
#dummy_internal_corr.head(50)
dummy_internal_corr = cramers_corrected_stat(df_dummy)
dummy_internal_corr.head(50).round(3)
                            var1      var2  p-value  Chi2 Statistic
M2a_dico/WRK_dico       M2a_dico  WRK_dico      0.0         849.631
ANX_dico/M2a_dico       ANX_dico  M2a_dico      0.0         826.082
RT_dico/WRK_dico         RT_dico  WRK_dico      0.0         817.039
M2a_dico/OBS_dico       M2a_dico  OBS_dico      0.0         780.955
D1_dico/D4_dico          D1_dico   D4_dico      0.0         684.119
OBS_dico/WRK_dico       OBS_dico  WRK_dico      0.0         678.481
M2a_dico/RT_dico        M2a_dico   RT_dico      0.0         660.781
DEP_dico/FAM_dico       DEP_dico  FAM_dico      0.0         602.637
D1_dico/D5_dico          D1_dico   D5_dico      0.0         596.819
DEP_dico/M2a_dico       DEP_dico  M2a_dico      0.0         552.486
FAM_dico/M2a_dico       FAM_dico  M2a_dico      0.0         535.006
OBS_dico/RT_dico        OBS_dico   RT_dico      0.0         526.386
BIM_dico/FAM_dico       BIM_dico  FAM_dico      0.0         525.311
ANX_dico/OBS_dico       ANX_dico  OBS_dico      0.0         521.323
D5_dico/DEP_dico         D5_dico  DEP_dico      0.0         514.382
D4_dico/DEP_dico         D4_dico  DEP_dico      0.0         514.382
DEP_dico/SE_dico        DEP_dico   SE_dico      0.0         492.377
M2a_dico/TPA_dico       M2a_dico  TPA_dico      0.0         481.016
HS_dico/HY_dico          HS_dico   HY_dico      0.0         456.248
BIZ_dico/SC_dico        BIZ_dico   SC_dico      0.0         452.821
SE_dico/WRK_dico         SE_dico  WRK_dico      0.0         450.841
HEA_dico/SC_dico        HEA_dico   SC_dico      0.0         446.765
DEP_dico/WRK_dico       DEP_dico  WRK_dico      0.0         436.145
Gender_Dico/MF_dico  Gender_Dico   MF_dico      0.0         419.770
DOE_dico/K_dico         DOE_dico    K_dico      0.0         412.545
FCENT_dico/K_dico     FCENT_dico    K_dico      0.0         403.702
ANG_dico/M2a_dico       ANG_dico  M2a_dico      0.0         402.926
FCENT_dico/R_dico     FCENT_dico    R_dico      0.0         395.857
BIM_dico/BIZ_dico       BIM_dico  BIZ_dico      0.0         392.247
RT_dico/SE_dico          RT_dico   SE_dico      0.0         390.409
D1_dico/PT_dico          D1_dico   PT_dico      0.0         390.165
BIZ_dico/F_dico         BIZ_dico    F_dico      0.0         387.636
M2a_dico/SE_dico        M2a_dico   SE_dico      0.0         385.272
BIM_dico/F_dico         BIM_dico    F_dico      0.0         382.957
BIZ_dico/FAM_dico       BIZ_dico  FAM_dico      0.0         379.831
DEP_dico/RT_dico        DEP_dico   RT_dico      0.0         378.099
FAM_dico/RT_dico        FAM_dico   RT_dico      0.0         376.858
BIZ_dico/M2a_dico       BIZ_dico  M2a_dico      0.0         375.069
ANX_dico/RT_dico        ANX_dico   RT_dico      0.0         374.044
F_dico/M2a_dico           F_dico  M2a_dico      0.0         371.911
FAM_dico/WRK_dico       FAM_dico  WRK_dico      0.0         371.867
D1_dico/D_dico           D1_dico    D_dico      0.0         368.756
BIM_dico/DEP_dico       BIM_dico  DEP_dico      0.0         361.224
D4_dico/SE_dico          D4_dico   SE_dico      0.0         359.954
ANX_dico/WRK_dico       ANX_dico  WRK_dico      0.0         358.386
D1_dico/DEP_dico         D1_dico  DEP_dico      0.0         343.529
D5_dico/M2a_dico         D5_dico  M2a_dico      0.0         339.206
ANX_dico/SE_dico        ANX_dico   SE_dico      0.0         329.941
ANX_dico/F_dico         ANX_dico    F_dico      0.0         325.265
BIZ_dico/RT_dico        BIZ_dico   RT_dico      0.0         322.454
The provided table demonstrates evident relationships among numerous dummy variables (p < 0.05). In the subsequent section, we will proceed to gauge the magnitude of these relationships among the independent variables using the Cramer's V measure.
Internal correlations strength - Cramer's V measure
import pandas as pd
import numpy as np
import scipy.stats as ss

# create the contingency table matrix
def contingency_table_matrix(df):
    columns = df.columns
    matrix = np.zeros((len(columns), len(columns)))
    for i, col1 in enumerate(columns):
        for j, col2 in enumerate(columns):
            contingency_table = pd.crosstab(df[col1], df[col2])
            chi2, _, _, _ = ss.chi2_contingency(contingency_table)
            min_categories = min(contingency_table.shape[0], contingency_table.shape[1])
            n = contingency_table.sum().sum()
            V = np.sqrt(chi2 / (n * (min_categories - 1)))
            matrix[i, j] = V
    return pd.DataFrame(matrix, columns=columns, index=columns)


# generate the Cramer's V coefficient matrix
dummy_cramers = contingency_table_matrix(df_dummy)
#dummy_cramers.head()
sorted_dummy_cramers = dummy_cramers.unstack().sort_values(ascending=False);
sorted_dummy_cramers_filtered = pd.DataFrame(sorted_dummy_cramers);
sorted_dummy_rename = sorted_dummy_cramers_filtered.rename(columns={0: "Cramers V"}, errors="raise");
dummy_cramers_final = sorted_dummy_rename[sorted_dummy_rename["Cramers V"] < 1];

dummy_cramers_final['Variables'] = dummy_cramers_final.index;

pd_dummy_cramers_V = pd.DataFrame(dummy_cramers_final, columns=["Variables","Cramers V"]);                     
pd_dummy_cramers_V.head(100)
                                          Variables  Cramers V
R_dico      R_dico                 (R_dico, R_dico)   0.999115
K_dico      K_dico                 (K_dico, K_dico)   0.999095
Gender_Dico Gender_Dico  (Gender_Dico, Gender_Dico)   0.999066
TRIN_dico   TRIN_dico        (TRIN_dico, TRIN_dico)   0.998980
DOE_dico    DOE_dico           (DOE_dico, DOE_dico)   0.998976
...                                             ...        ...
WRK_dico    DEP_dico           (WRK_dico, DEP_dico)   0.438235
MF_dico     Gender_Dico      (MF_dico, Gender_Dico)   0.429929
Gender_Dico MF_dico          (Gender_Dico, MF_dico)   0.429929
DOE_dico    K_dico               (DOE_dico, K_dico)   0.426214
K_dico      DOE_dico             (K_dico, DOE_dico)   0.426214

[100 rows x 2 columns]
Upon calculation of the Cramer's V index, a considerable number of dummy variables exhibit robust relationships, some of which demonstrate the maximum level of effect magnitude.  Notably, these strong relationships primarily pertain to the personality scales within the MMPI-2 test. It is apparent that a significant portion of these scales capture similar underlying content.
The implications of these findings will be duly incorporated into the phases of data enrichment, data engineering, and feature selection. In these stages, a diverse range of techniques will be employed to address the weighting of personality scales, aiming to minimize any potential interference within the prediction models.
11.2 Categorial variables internal crrelations
Chi-Square Test
def cramers_corrected_stat(df):
    chi2_df = pd.DataFrame(columns=['var1','var2','p-value','Chi2 Statistic'])
    for i in df:
        if ((i in df.columns) and i != 'Personality_Disqualification_Dico'):
            for j in df:
                if (i in df.columns) and j != 'Personality_Disqualification_Dico' and i < j:
                    confusion_matrix = pd.crosstab(df[i], df[j])
                    chi_res = chi2_contingency(confusion_matrix)
                    b = "{}/{}".format(i,j)
                    if chi_res[1] < 0.05:
                        chi2_df.loc[b] = [i,j,chi_res[1],chi_res[0]]
                    
    return chi2_df.sort_values(by='p-value')

df_categorical_corr = cramers_corrected_stat(df_categorical)
df_categorical_corr.head(50).round(3)
                                                                 var1  \
Education_Level/Educational_Institution               Education_Level   
Designated_Role/Education_Level                       Designated_Role   
Designated_Role/Educational_Institution               Designated_Role   
Designated_Role/Rovaee_Training_Type                  Designated_Role   
Education_Level/Rovaee_Training_Type                  Education_Level   
Educational_Institution/Rovaee_Training_Type  Educational_Institution   

                                                                 var2  \
Education_Level/Educational_Institution       Educational_Institution   
Designated_Role/Education_Level                       Education_Level   
Designated_Role/Educational_Institution       Educational_Institution   
Designated_Role/Rovaee_Training_Type             Rovaee_Training_Type   
Education_Level/Rovaee_Training_Type             Rovaee_Training_Type   
Educational_Institution/Rovaee_Training_Type     Rovaee_Training_Type   

                                              p-value  Chi2 Statistic  
Education_Level/Educational_Institution           0.0         711.543  
Designated_Role/Education_Level                   0.0         306.674  
Designated_Role/Educational_Institution           0.0         252.777  
Designated_Role/Rovaee_Training_Type              0.0         396.546  
Education_Level/Rovaee_Training_Type              0.0         146.075  
Educational_Institution/Rovaee_Training_Type      0.0          99.396  
Internal Correlations Strength - Cramer's-V Measure
import pandas as pd
import numpy as np
import scipy.stats as ss

# create the contingency table matrix
def contingency_table_matrix(df):
    columns = df.columns
    matrix = np.zeros((len(columns), len(columns)))
    for i, col1 in enumerate(columns):
        for j, col2 in enumerate(columns):
            contingency_table = pd.crosstab(df[col1], df[col2])
            chi2, _, _, _ = ss.chi2_contingency(contingency_table)
            min_categories = min(contingency_table.shape[0], contingency_table.shape[1])
            n = contingency_table.sum().sum()
            V = np.sqrt(chi2 / (n * (min_categories - 1)))
            matrix[i, j] = V
    return pd.DataFrame(matrix, columns=columns, index=columns)


# generate the Cramer's V coefficient matrix
categorical_cramers = contingency_table_matrix(df_categorical)
#dummy_cramers.head()
 Not surprisingly, a robust correlation was identified between the type of educational institution and the level of education. Additionally, a moderately strong association emerged between the educational institution and the type of rifle.  Given these results, it is advisable to explore the feasibility of applying weighting techniques or considering the retention of either the educational institution variable or the education level variable within the dataset. This step aims to ensure that the interrelatedness of these variables does not lead to redundant information or potential multicollinearity issues in subsequent analyses.
sorted_categorical_cramers = categorical_cramers.unstack().sort_values(ascending=False);
sorted_categorical_cramers_filtered = pd.DataFrame(sorted_categorical_cramers);
sorted_categorical_rename = sorted_categorical_cramers_filtered.rename(columns={0: "Cramers V"}, errors="raise");
categorical_cramers_final = sorted_categorical_rename[sorted_categorical_rename["Cramers V"] < 1];

categorical_cramers_final['Variables'] = categorical_cramers_final.index;

pd_categorical_cramers_V = pd.DataFrame(categorical_cramers_final, columns=["Variables","Cramers V"]);                     
pd_categorical_cramers_V.head(30)
                                                                                                             Variables  \
Personality_Disqualification_Dico Personality_Disqualification_Dico  (Personality_Disqualification_Dico, Personalit...   
Education_Level                   Educational_Institution                   (Education_Level, Educational_Institution)   
Educational_Institution           Education_Level                           (Educational_Institution, Education_Level)   
Designated_Role                   Educational_Institution                   (Designated_Role, Educational_Institution)   
Educational_Institution           Designated_Role                           (Educational_Institution, Designated_Role)   
Designated_Role                   Education_Level                                   (Designated_Role, Education_Level)   
Education_Level                   Designated_Role                                   (Education_Level, Designated_Role)   
Personality_Disqualification_Dico Designated_Role                    (Personality_Disqualification_Dico, Designated...   
Designated_Role                   Personality_Disqualification_Dico  (Designated_Role, Personality_Disqualification...   
Educational_Institution           Rovaee_Training_Type                 (Educational_Institution, Rovaee_Training_Type)   
Rovaee_Training_Type              Educational_Institution              (Rovaee_Training_Type, Educational_Institution)   
                                  Education_Level                              (Rovaee_Training_Type, Education_Level)   
Education_Level                   Rovaee_Training_Type                         (Education_Level, Rovaee_Training_Type)   
Rovaee_Training_Type              Designated_Role                              (Rovaee_Training_Type, Designated_Role)   
Designated_Role                   Rovaee_Training_Type                         (Designated_Role, Rovaee_Training_Type)   
Rovaee_Training_Type              Personality_Disqualification_Dico  (Rovaee_Training_Type, Personality_Disqualific...   
Personality_Disqualification_Dico Rovaee_Training_Type               (Personality_Disqualification_Dico, Rovaee_Tra...   
Education_Level                   Personality_Disqualification_Dico  (Education_Level, Personality_Disqualification...   
Personality_Disqualification_Dico Education_Level                    (Personality_Disqualification_Dico, Education_...   
                                  Educational_Institution            (Personality_Disqualification_Dico, Educationa...   
Educational_Institution           Personality_Disqualification_Dico  (Educational_Institution, Personality_Disquali...   

                                                                     Cramers V  
Personality_Disqualification_Dico Personality_Disqualification_Dico   0.997391  
Education_Level                   Educational_Institution             0.516619  
Educational_Institution           Education_Level                     0.516619  
Designated_Role                   Educational_Institution             0.316715  
Educational_Institution           Designated_Role                     0.316715  
Designated_Role                   Education_Level                     0.284159  
Education_Level                   Designated_Role                     0.284159  
Personality_Disqualification_Dico Designated_Role                     0.282386  
Designated_Role                   Personality_Disqualification_Dico   0.282386  
Educational_Institution           Rovaee_Training_Type                0.192870  
Rovaee_Training_Type              Educational_Institution             0.192870  
                                  Education_Level                     0.190410  
Education_Level                   Rovaee_Training_Type                0.190410  
Rovaee_Training_Type              Designated_Role                     0.171834  
Designated_Role                   Rovaee_Training_Type                0.171834  
Rovaee_Training_Type              Personality_Disqualification_Dico   0.166670  
Personality_Disqualification_Dico Rovaee_Training_Type                0.166670  
Education_Level                   Personality_Disqualification_Dico   0.098238  
Personality_Disqualification_Dico Education_Level                     0.098238  
                                  Educational_Institution             0.058968  
Educational_Institution           Personality_Disqualification_Dico   0.058968  
--- Chapter 8 Summary ---
•	The results obtained from the preceding chapters underscore the presence of multicollinear relationships among numerous variables in the dataset, with a notable emphasis on those linked to the MMPI-2 personality test scales. These variables exhibit strong interconnections that could potentially introduce bias in certain models' outcomes. To mitigate this concern, it becomes imperative to explore diverse strategies to address the issue of multicollinearity—either by assigning weights to variables or by removing them altogether from the dataset.
•	The approach chosen to handle multicollinear variables will be guided by subsequent statistical analyses, which may encompass techniques such as exploratory factor analysis, averaging, index creation, development of an integrated index based on Spearman weights or logistic regression coefficients, establishment of interaction variables, and similar methods. It is important to acknowledge that the chosen solution methodology will also be contingent on the level of measurement of variables involved in the weighting process (e.g., factor analysis may not be well-suited for variables on an ordinal scale).
Main multicollinear variables: 
•	Most personality scales (Dummy & Numerical)
•	Educational_Institution / Education_Level
•	Cognitive_Ability_Test / Filling_Instruction_Test
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
12. Correlations with the outcome variable
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
12.1 Numeric variables correlations with the outcome variable 
12.1.1 Sperman and Kendall rank correlation tests
Indeed, it's important to reiterate that the Pearson correlation test isn't the appropriate choice for assessing the relationships between the independent research variables and the target factor in this project. This stems from two key factors: the non-normal distribution of the majority of predictors and the dichotomous nature of the target variable.
Firstly, due to the non-normal distribution of most predictors, using the Pearson correlation test, which assumes normality, can lead to erroneous results. Secondly, the dichotomous nature of the target variable precludes the usage of the Pearson correlation, which is applicable only when both the independent variable and the dependent variable adhere to specific scale types, such as interval or higher scales from the social sciences perspective, or quantile scales from the perspective of the exact sciences.
Given these constraints, alternative methods like the Spearman correlation test and Kendall rank correlation, are more suitable for analyzing the relationships between variables with non-normal distributions.
Organizing the data before checking the relationships
#######set targer and feasures

target = "Personality_Disqualification_Dico"

features = df2.columns.tolist()

####### dont include list

if "id" in features:
    features.remove("id")
    
if "Personality_Disqualification_Dico" in features:
    features.remove("Personality_Disqualification_Dico")

Variables types optimization
# Features by dtype

features
features_by_dtype = {}

for feature in features:
    
    feature_dtype = str(df.dtypes[feature])
    
    try:
        features_by_dtype[feature_dtype]
    except KeyError:
        features_by_dtype[feature_dtype] = []
        
    
    features_by_dtype[feature_dtype].append(feature)

dtypes = features_by_dtype.keys()
# Categorical Features

categorical_features = features_by_dtype["object"]
categorical_features = categorical_features + ["MSSubClass"]

#categorical_features
# Binary Features

#binary_features = [c for c in categorical_features if len(df[c].unique()) == 2]

#binary_features
# Numerical Features

float_features = features_by_dtype["float64"]
int_features = features_by_dtype["int64"]
numerical_features = float_features + int_features
remove_list = ["GarageYrBlt", "YearBuilt", "YearRemodAdd", "MoSold", "YrSold", "MSSubClass"]
numerical_features = [n for n in numerical_features if n not in remove_list]

#numerical_features
Take into account "0" values in variables as "unic"

# Has Zero Features

has_zero_features = []

for n in numerical_features:
    if 0 in df[n].unique():
        has_zero_features.append(n)
        
#has_zero_features

# Bounded Features

bounded_features = ["OverallQual", "OverallCond"]

# Temporal Features

temporal_features = remove_list.copy()
temporal_features.remove("MSSubClass")

temporal_features
['GarageYrBlt', 'YearBuilt', 'YearRemodAdd', 'MoSold', 'YrSold']
# Summary

#features, target
#categorical_features, numerical_features, temporal_features
#binary_features, has_zero_features, bounded_features
pass
spearman = df2[numerical_features + [target]].corr("spearman")
kendall = df2[numerical_features + [target]].corr("kendall")
pearson = df2[numerical_features + [target]].corr("pearson")

p = pearson[target].rename("Pearson")
k = kendall[target].rename("Kendall")
s = spearman[target].rename("Spearman")

from pandas import DataFrame

corr = DataFrame(data = p)
corr = corr.assign(Spearman = s)
corr = corr.assign(Kendall = k)

from IPython.display import display
Certainly, Spearman and Kendall correlations can provide valuable insights into the relationships between the independent variables in the project and the dichotomous target factor. As mentioned earlier, here, we focus on the non-parametric correlations (Spearman and Kendall) as they are more appropriate for non-normally distributed data and dichotomous outcomes. The Pearson correlation is included for the sake of comparison, though it may not be directly relevant in this context.
Analyzing the differences in correlation strength across these measures can reveal nuances in the relationships between variables, accounting for both linear and monotonic associations. This comprehensive view can help in identifying patterns and understanding the factors that may influence the target factor more accurately. It's important to consider both the direction and magnitude of these correlations, as well as their statistical significance, to draw meaningful conclusions about the relationships in the dataset.
display(corr.round(2).sort_values("Pearson", ascending=False))
                                   Pearson  Spearman  Kendall
Personality_Disqualification_Dico     1.00      1.00     1.00
FAM                                   0.22      0.18     0.15
BIM                                   0.21      0.18     0.15
DEP                                   0.21      0.17     0.15
WSD                                   0.21      0.17     0.15
Morals_and_Values_Test                0.20      0.15     0.14
F                                     0.20      0.16     0.14
SC                                    0.20      0.17     0.14
DIS                                   0.20      0.16     0.14
MDS                                   0.20      0.17     0.15
RT                                    0.19      0.15     0.13
M2a                                   0.19      0.15     0.13
ANX                                   0.18      0.14     0.12
WRK                                   0.18      0.14     0.12
PD                                    0.18      0.15     0.13
HEA                                   0.17      0.15     0.13
D4                                    0.16      0.11     0.10
OBS                                   0.16      0.12     0.10
BIZ                                   0.15      0.11     0.10
ASP                                   0.15      0.12     0.10
SE                                    0.15      0.12     0.10
D1                                    0.15      0.11     0.09
VRIN                                  0.15      0.12     0.11
D5                                    0.15      0.13     0.11
PT                                    0.14      0.10     0.08
ANG                                   0.13      0.10     0.08
tcent                                 0.13      0.10     0.08
TPA                                   0.11      0.09     0.08
CYN                                   0.11      0.11     0.09
HS                                    0.11      0.09     0.07
FP                                    0.11      0.09     0.07
RS                                    0.10      0.09     0.07
AAS                                   0.10      0.10     0.08
PA                                    0.10      0.08     0.07
Assessment_Center_Grade               0.09      0.09     0.07
D                                     0.09      0.09     0.07
D3                                    0.08      0.07     0.06
SOD                                   0.08      0.06     0.05
MA                                    0.08      0.07     0.06
CR                                    0.07      0.06     0.05
M2APS                                 0.03      0.00     0.00
TRIN                                  0.03      0.00     0.00
HY                                    0.03      0.01     0.01
Honesty_Test                          0.00      0.02     0.02
D2                                   -0.01     -0.01    -0.01
Age                                  -0.04     -0.06    -0.05
Recruit_Quality_Measure_Score        -0.05     -0.05    -0.04
MF                                   -0.06     -0.07    -0.06
L                                    -0.06     -0.06    -0.05
R                                    -0.08     -0.07    -0.06
DOE                                  -0.08     -0.06    -0.06
Filling_Instruction_Test             -0.10     -0.10    -0.08
Cognitive_Ability_Test               -0.10     -0.09    -0.08
Years_of_Education                   -0.10     -0.11    -0.11
Heberew_Test_Grade                   -0.11     -0.11    -0.10
Emotional_Inteligence_Test           -0.12     -0.12    -0.10
K                                    -0.13     -0.10    -0.09
FCENT                                -0.13     -0.10    -0.08
ES                                   -0.14     -0.12    -0.10
Job_Interview_Grade                  -0.20     -0.20    -0.19
12.1.2 Strength of connections relative level between the project predictors and the target variable (shown according to the Spearman correlation)
Job Selection Processes Predictors :
df_selection_process = pd.DataFrame(df, columns=['Morals_and_Values_Test', 'Assessment_Center_Grade', 'Honesty_Test', 'Recruit_Quality_Measure_Score', 'Cognitive_Ability_Test', 'Filling_Instruction_Test', 'Heberew_Test_Grade', 'Years_of_Education', 'Emotional_Inteligence_Test', 'Job_Interview_Grade', 'Personality_Disqualification_Dico'])
plt.figure(figsize= (5,10))
sns.heatmap(df_selection_process.corr()[['Personality_Disqualification_Dico']].sort_values(by='Personality_Disqualification_Dico', ascending=False), vmin=-1, vmax=1, annot=True, cmap='coolwarm')
<AxesSubplot:>
 
MMPI-2 Personality Scsles
df_num_MMPI = df_num
df_num_MMPI = df_num_MMPI.drop(columns=['Morals_and_Values_Test', 'Assessment_Center_Grade', 'Honesty_Test', 'Recruit_Quality_Measure_Score', 'Cognitive_Ability_Test', 'Filling_Instruction_Test', 'Heberew_Test_Grade', 'Years_of_Education', 'Emotional_Inteligence_Test', 'Job_Interview_Grade'])
plt.figure(figsize= (5,10))
sns.heatmap(df_num_MMPI.corr()[['Personality_Disqualification_Dico']].sort_values(by='Personality_Disqualification_Dico', ascending=False), vmin=-1, vmax=1, annot=True, cmap='coolwarm')
<AxesSubplot:>
 
The analysis of correlations between the numerical predictors and the dichotomous target factor has revealed several noteworthy insights: 
Statistically Significant Relationships: A significant number of personal scales, selection process factors, and socio-demographic variables exhibit statistically significant linear relationships with the target factor (p < 0.001 to p < 0.05).
Effect Size of Correlations: While many relationships are statistically significant, they tend to have relatively low effect sizes (Spearman correlations around +/-0.05 to +/-0.2). This indicates that the strength of the correlations is not very high, suggesting that the relationships between these variables and the target factor are relatively weak.
Strong Positive Correlations: Predictors with weak to moderate positive correlations with the outcome variable (corr > 0.15) include variables such as FAM, WSD, BIM, Morals_and_Values_Test, DEP, PD, SC, MDS, F, DIS, RT, M2a, and WRK. These predictors demonstrate a significant positive relationship with personality disqualification.
Strong Negative Correlations: Predictors with weak to moderate negative correlations with the outcome variable (corr < -0.15) include Interview_Grade and Emotional_Intelligence_Test. These variables show a significant negative association with personality disqualification.
Number of Significant Correlations: In total, 53 variables have been found to have statistically significant correlations with the outcome variable within the range of p < 0.001 to p < 0.05.
Importance of Effect Size: Despite the statistical significance of these correlations, it's important to emphasize that most of the independent variables show only negligible to weak correlations with the target variable. In large samples, statistical significance can be achieved even with small effect sizes, making effect size a crucial consideration.
Focus on Effect Sizes: The emphasis on effect sizes rather than just statistical significance aligns with the goals of developing robust predictive models. It indicates that while some relationships might be statistically significant, they may not hold substantial predictive power or practical significance.
Ongoing Research and Data Enrichment: The absence of strong correlations is not the end of the analysis but rather the beginning of a comprehensive process. Further exploration, data engineering, and creative solutions will be needed to develop valid predictive models. This process is crucial for enhancing the early identification of candidates prone to personality disqualification, ultimately contributing to the recruitment process.
This analysis highlights the importance of understanding the practical implications of correlations and effect sizes, especially in the context of developing predictive models that can drive actionable insights in real-world applications.
12.2 Correlations between the dummy and The categor variables with the outcome variable
12.2.1 Dummy Variables - Chi-Squre test
def cramers_corrected_stat(df):
    chi2_df = pd.DataFrame(columns=['var1','var2','p-value','Chi2 Statistic'])
    for i in df:
        if ((i in df.columns) and i != 'Personality_Disqualification_Dico'):
            for j in df:
                    confusion_matrix = pd.crosstab(df[i], df[j])
                    chi_res = chi2_contingency(confusion_matrix)
                    b = "{}/{}".format(i,j)
                    if chi_res[1] < 0.05:
                        chi2_df.loc[b] = [i,j,chi_res[1],chi_res[0]]
                    
    return chi2_df.sort_values(by='p-value')

chi2_df = cramers_corrected_stat(df_dummy)
chi2_df_filtered_1 = pd.DataFrame.from_records(chi2_df);
chi2_df_filtered_2 = chi2_df_filtered_1[chi2_df_filtered_1.var2=='Personality_Disqualification_Dico'];

chi2_df_filtered_3 = chi2_df_filtered_2.rename(columns={"var1": "Dummy variable", "var2" :"Outcome variable"}, errors="raise");
chi2_df_filtered_3.round(2)
     Dummy variable                   Outcome variable  p-value  \
659         PT_dico  Personality_Disqualification_Dico     0.00   
686         PD_dico  Personality_Disqualification_Dico     0.00   
773        FAM_dico  Personality_Disqualification_Dico     0.00   
798        HEA_dico  Personality_Disqualification_Dico     0.00   
801        ANX_dico  Personality_Disqualification_Dico     0.00   
900        MDS_dico  Personality_Disqualification_Dico     0.00   
937         D1_dico  Personality_Disqualification_Dico     0.00   
944        ASP_dico  Personality_Disqualification_Dico     0.00   
983          F_dico  Personality_Disqualification_Dico     0.00   
996         SC_dico  Personality_Disqualification_Dico     0.00   
997        M2a_dico  Personality_Disqualification_Dico     0.00   
1026       BIM_dico  Personality_Disqualification_Dico     0.00   
1033      VRIN_dico  Personality_Disqualification_Dico     0.00   
1038        D4_dico  Personality_Disqualification_Dico     0.00   
1049       WRK_dico  Personality_Disqualification_Dico     0.00   
1062    Gender_Dico  Personality_Disqualification_Dico     0.00   
1067       DEP_dico  Personality_Disqualification_Dico     0.00   
1078       OBS_dico  Personality_Disqualification_Dico     0.00   
1083        HS_dico  Personality_Disqualification_Dico     0.00   
1090       BIZ_dico  Personality_Disqualification_Dico     0.00   
1129        PA_dico  Personality_Disqualification_Dico     0.00   
1134        RT_dico  Personality_Disqualification_Dico     0.00   
1145       ANG_dico  Personality_Disqualification_Dico     0.00   
1166        SE_dico  Personality_Disqualification_Dico     0.00   
1275     FCENT_dico  Personality_Disqualification_Dico     0.00   
1278         D_dico  Personality_Disqualification_Dico     0.00   
1305        HY_dico  Personality_Disqualification_Dico     0.00   
1330        D3_dico  Personality_Disqualification_Dico     0.00   
1359       SOD_dico  Personality_Disqualification_Dico     0.00   
1360        RS_dico  Personality_Disqualification_Dico     0.00   
1381        D2_dico  Personality_Disqualification_Dico     0.00   
1424         K_dico  Personality_Disqualification_Dico     0.00   
1505        MA_dico  Personality_Disqualification_Dico     0.00   
1542        FP_dico  Personality_Disqualification_Dico     0.00   
1547       TPA_dico  Personality_Disqualification_Dico     0.01   
1572         R_dico  Personality_Disqualification_Dico     0.01   
1609       CYN_dico  Personality_Disqualification_Dico     0.01   
1624        D5_dico  Personality_Disqualification_Dico     0.02   
1661         L_dico  Personality_Disqualification_Dico     0.02   
1672        MF_dico  Personality_Disqualification_Dico     0.02   
1703       AAS_dico  Personality_Disqualification_Dico     0.03   
1718        ES_dico  Personality_Disqualification_Dico     0.04   

      Chi2 Statistic  
659            80.71  
686            75.00  
773            63.06  
798            60.22  
801            59.95  
900            48.12  
937            45.57  
944            45.25  
983            41.15  
996            39.91  
997            39.89  
1026           38.10  
1033           37.34  
1038           36.45  
1049           35.96  
1062           34.47  
1067           33.68  
1078           33.18  
1083           32.68  
1090           31.91  
1129           29.17  
1134           28.83  
1145           28.25  
1166           26.41  
1275           19.26  
1278           19.00  
1305           17.58  
1330           16.38  
1359           14.93  
1360           14.39  
1381           13.59  
1424           12.04  
1505            8.95  
1542            7.92  
1547            7.70  
1572            6.99  
1609            6.22  
1624            5.89  
1661            5.21  
1672            5.04  
1703            4.59  
1718            4.08  
A statistically significant relationship has been observed between all of the dichotomous research variables and the outcome factor (p < 0.05 - 0.001). However, considering the large sample size of participants, the significance of statistical tests might be constrained. In such cases, significant differences can arise even when the effect sizes are low or even negligible. Therefore, to delve deeper into the strength of the relationships between predictors and the target factor, we will utilize the Cramers V index in the following section. This index provides a clearer understanding of the relationships by considering the practical significance beyond just statistical significance.
12.2.2 Dummy Variables - Cramers V (correlation strength)
cramers_results_dummy_final = dummy_cramers.Personality_Disqualification_Dico
print(cramers_results_dummy_final)
Population_Type_Dico                 0.016123
Gender_Dico                          0.122423
Commander_in_Army_Dico               0.028600
Combat_Army_Unit_Dico                0.047164
M2a_dico                             0.132540
ASP_dico                             0.141155
R_dico                               0.055489
CYN_dico                             0.052350
BIM_dico                             0.129521
PA_dico                              0.113339
AAS_dico                             0.044973
tcent_dico                           0.016065
OBS_dico                             0.120873
MDS_dico                             0.145560
TPA_dico                             0.058233
BIZ_dico                             0.118534
ANG_dico                             0.111542
DEP_dico                             0.121781
K_dico                               0.072800
SOD_dico                             0.081076
D_dico                               0.091460
D1_dico                              0.141652
D2_dico                              0.077349
D3_dico                              0.084919
D4_dico                              0.126691
D5_dico                              0.050946
MF_dico                              0.047095
DOE_dico                             0.011642
PT_dico                              0.188516
ANX_dico                             0.162481
F_dico                               0.134616
SC_dico                              0.132571
HEA_dico                             0.162840
FP_dico                              0.059061
HS_dico                              0.119966
FAM_dico                             0.166632
PD_dico                              0.181727
ES_dico                              0.042374
FCENT_dico                           0.092087
WRK_dico                             0.125828
HY_dico                              0.087986
L_dico                               0.047876
MA_dico                              0.062791
SE_dico                              0.107840
RT_dico                              0.112676
TRIN_dico                            0.000000
VRIN_dico                            0.128223
CR_dico                              0.039005
RS_dico                              0.079604
Personality_Disqualification_Dico    0.997391
Name: Personality_Disqualification_Dico, dtype: float64
Indeed, although statistical significance was detected between the dichotomous research variables and the outcome factor (p < 0.05 - 0.001), the Cramers V index revealed negligible to low effect sizes for all predictors. This outcome emphasizes that each individual factor's predictive contribution to the target variable is limited. In the forthcoming stages of data enrichment and data engineering, we will explore diverse approaches such as varying methods of weighting and creating distinct combinations among the variables. Through these efforts, we aim to create a holistic model that synergistically amplifies the predictive power beyond the cumulative influence of individual variables.
---------------------------------------------------------------------------------------------------------------
12.2.3 Chi-Squre test - examine the correlation between categor predictors and the outcome variable
####### Designated Role #######
crosstab, test_results, expected = rp.crosstab(df_categorical["Designated_Role"], df_categorical["Personality_Disqualification_Dico"],
test= "chi-square", expected_freqs= True, prop= "cell")
print(crosstab)
############################
test_results
                                  Personality_Disqualification_Dico        \
Personality_Disqualification_Dico                                 0     1   
Designated_Role                                                             
Border Guard fighters                                         15.71  5.20   
Control center volunteers                                      0.88  0.05   
Police dispatchers                                             1.44  0.14   
Traffic cops                                                   0.79  0.00   
administrative nagad                                           9.29  0.79   
administrative officer                                         2.65  0.00   
intelligence                                                   0.79  0.00   
operational core units                                        53.39  3.02   
operational officer                                            0.42  0.00   
prosecutor                                                     3.53  0.09   
special unit                                                   1.81  0.00   
All                                                           90.71  9.29   

                                           
Personality_Disqualification_Dico     All  
Designated_Role                            
Border Guard fighters               20.91  
Control center volunteers            0.93  
Police dispatchers                   1.58  
Traffic cops                         0.79  
administrative nagad                10.08  
administrative officer               2.65  
intelligence                         0.79  
operational core units              56.41  
operational officer                  0.42  
prosecutor                           3.62  
special unit                         1.81  
All                                100.00  
                 Chi-square test   results
0  Pearson Chi-square ( 10.0) =   171.6048
1                     p-value =     0.0000
2                  Cramer's V =     0.2824
---------------------------------------------------------------------------------------------------------------
####### Education Level #######
crosstab, test_results, expected = rp.crosstab(df_categorical["Education_Level"], df_categorical["Personality_Disqualification_Dico"],
test= "chi-square", expected_freqs= True, prop= "cell")
print(crosstab)
############################
test_results
                                  Personality_Disqualification_Dico        \
Personality_Disqualification_Dico                                 0     1   
Education_Level                                                             
Academic education                                            26.14  1.12   
complete_bagrot                                               35.15  2.68   
diploma                                                        6.70  0.82   
without_bagrot                                                24.50  2.90   
All                                                           92.48  7.52   

                                           
Personality_Disqualification_Dico     All  
Education_Level                            
Academic education                  27.25  
complete_bagrot                     37.83  
diploma                              7.52  
without_bagrot                      27.40  
All                                100.00  
                Chi-square test  results
0  Pearson Chi-square ( 3.0) =   12.9610
1                    p-value =    0.0047
2                 Cramer's V =    0.0982
test_results
                Chi-square test  results
0  Pearson Chi-square ( 3.0) =   12.9610
1                    p-value =    0.0047
2                 Cramer's V =    0.0982
---------------------------------------------------------------------------------------------------------------
####### Educational Institution #######
crosstab, test_results, expected = rp.crosstab(df_categorical["Educational_Institution"], df_categorical["Personality_Disqualification_Dico"],
test= "chi-square", expected_freqs= True, prop= "cell")
print(crosstab)
############################
test_results
                                  Personality_Disqualification_Dico        \
Personality_Disqualification_Dico                                 0     1   
Educational_Institution                                                     
college                                                       20.96  1.35   
other                                                         68.71  6.06   
university                                                     2.92  0.00   
All                                                           92.59  7.41   

                                           
Personality_Disqualification_Dico     All  
Educational_Institution                    
college                             22.31  
other                               74.78  
university                           2.92  
All                                100.00  
                Chi-square test  results
0  Pearson Chi-square ( 2.0) =    4.6456
1                    p-value =    0.0980
2                 Cramer's V =    0.0590
---------------------------------------------------------------------------------------------------------------
####### Rovaee Training Type #######
crosstab, test_results, expected = rp.crosstab(df_categorical["Rovaee_Training_Type"], df_categorical["Personality_Disqualification_Dico"],
test= "chi-square", expected_freqs= True, prop= "cell")
print(crosstab)
############################
test_results
                                  Personality_Disqualification_Dico        \
Personality_Disqualification_Dico                                 0     1   
Rovaee_Training_Type                                                        
rovaee 0                                                      21.07  1.26   
rovaee 1                                                       0.77  0.00   
rovaee 10                                                      0.63  0.00   
rovaee 12                                                      1.19  0.07   
rovaee 13                                                      0.07  0.00   
rovaee 2                                                      27.95  1.54   
rovaee 3                                                       8.78  0.21   
rovaee 4                                                       0.77  0.14   
rovaee 5                                                      10.04  2.04   
rovaee 6                                                       2.46  0.42   
rovaee 7                                                      12.43  1.54   
rovaee 8                                                       5.90  0.63   
rovaee 9                                                       0.07  0.00   
All                                                           92.13  7.87   

                                           
Personality_Disqualification_Dico     All  
Rovaee_Training_Type                       
rovaee 0                            22.33  
rovaee 1                             0.77  
rovaee 10                            0.63  
rovaee 12                            1.26  
rovaee 13                            0.07  
rovaee 2                            29.49  
rovaee 3                             8.99  
rovaee 4                             0.91  
rovaee 5                            12.08  
rovaee 6                             2.88  
rovaee 7                            13.97  
rovaee 8                             6.53  
rovaee 9                             0.07  
All                                100.00  
                 Chi-square test  results
0  Pearson Chi-square ( 12.0) =   39.5572
1                     p-value =    0.0001
2                  Cramer's V =    0.1667
---------------------------------------------------------------------------------------------------------------
A statistically significant relationship was observed between Designated_Role, Education_Level, Rovaee_Training_Type, and Personality_Disqualification (p < 0.05 - 0.001). However, the association between Educational_Institution with personality disqualification did not demonstrate statistical significance (p > 0.05).
To assess the strength of these relationships more comprehensively, we used Cramers V index. This index revealed weak to modearte effect size regarding the Designated_Role (Cramers V = 0.28, p<0.001) in predicting the personality disqualification. On the other hand, in the other variables the size of the effect was found to be negligible to low only. As a general rule, in relation to the variable, there is no point in interpreting the size of the effect, since no significant relationship was found between this factor and the target variable.
---------------------------------------------------------------------------------------------------------------
12.2.5 One Way ANOVA - Categor Variables
The ANOVA test is commonly used as a parametric test suitable for situations where the target variable is measured on an interval or ratio scale. This test is typically employed when comparing the means of three or more groups in relation to the target factor's average. However, the ANOVA test can also be adapted for comparing two groups, which is done through a two-sample t-test for independent or dependent samples.
In order to apply a one-way ANOVA to investigate differences among the categories of dichotomous variables (with two classes) and categorical variables (with three or more classes), we ensured that the independent factors in the study were encoded using dummy coding (values of 1 and 0). This approach allows the ANOVA test to compare the discrepancies in the rates of personality disqualification across different variable categories. For example, this could entail exploring differences in the personality disqualification rates between male and female candidates. The proportion of candidates who fail the personality test during the police recruitment process varies on a ratio scale with an absolute zero point. As such, if 40% of male candidates fail the personality test while only 20% of female candidates fail, this suggests that twice as many male candidates fail compared to their female counterparts. This enables the utilization of a parametric test, even when the underlying nature of the dependent variable is binary.
To begin, let's present the descriptive statistical data pertaining to the distinctions in the mean personality disqualification rates based on the classes of the independent variables within the project.
Personality disqualification rates (%) according to categorical variables classes :
fig = plt.figure(figsize=(35,20))
c = 1
for i in df_categorical:
    if i != 'Personality_Disqualification_Dico':
        plot_order = df_categorical.groupby(i)['Personality_Disqualification_Dico'].mean().sort_values(ascending=False).index.values
        plt.subplot(3, 2, c)
        plt.title('{}'.format(i))
        plt.title('Box Plot: {}'.format(col), fontsize=15)
        plt.xlabel('{}'.format(col), fontsize=15)

        plt.xlabel(i)
        sns.barplot(x=i, y='Personality_Disqualification_Dico', data=df_categorical, order=plot_order)
        c = c + 1

plt.show()
 
Before delving into the results of the ANOVA test, it's crucial to acknowledge that the findings displayed in the chart provide an initial impression of apparent linear relationships between the categorical variable classes and the target factor. However, it's important to recognize that these relationships are not linear; the illusion of linearity arises due to the way the columns in the table are presented, with the tallest column displayed first followed by the rest in ascending order.
•	Level of Education: The results indicate that the rate of personality disqualification is somewhat higher among candidates without a high school diploma or with only a diploma, in comparison to candidates possessing a full high school diploma and/or a bachelor's degree.
•	Type of Rifle Training: The findings highlight that candidates who underwent "5" and "6" rookie training exhibit a higher likelihood of disqualification compared to candidates who completed other types of training. Minor differences also suggest a slightly elevated chance of disqualification for candidates with rookie training 4, 7, or 8 in comparison to candidates with basic rookie training 1, 2, or 3.
•	Type of Educational Institution: It's interesting to observe that no instances of personality disqualification were identified among candidates with a university education. All cases of personality disqualification were found among candidates who studied at colleges or those without any academic education.
•	Type of Unit: A clear distinction emerges, indicating that the likelihood of personality disqualification is notably higher among Border Guard candidates compared to candidates aiming for administrative or operational positions within the Blue Police.
An important caveat to note is that the above findings necessitate careful consideration. As emphasized at various stages of the exploratory data analysis, some classes within the independent variables are based on small, or even negligible, participant counts. It's essential to recognize that this segment of the exploratory data analysis employs general tests to establish a preliminary impression. Subsequent sections focusing on data enrichment, data engineering, and feature selection will delve deeper and conduct more comprehensive investigations into the relationships between the independent research variables and the target factor, while also addressing potential biases introduced by the research setup or methodology.
Next, the outcomes of the ANOVA test will be presented, which aim to assess disparities in the rate of personality disqualification during the police recruitment process across various categories of the categorical predictors.
df_categorical_anova = pd.DataFrame(df_categorical) 
df_categorical_anova_new = df_categorical_anova['Personality_Disqualification_Dico_new'] = df_categorical.Personality_Disqualification_Dico * 100 + 0.0001  
df_categorical_anova.head()
          Designated_Role Education_Level Educational_Institution  \
0              prosecutor             NaN                     NaN   
1     operational officer             NaN                     NaN   
2  administrative officer             NaN                     NaN   
3  administrative officer             NaN                     NaN   
4              prosecutor             NaN                     NaN   

  Rovaee_Training_Type  Personality_Disqualification_Dico  \
0                  NaN                                  0   
1                  NaN                                  0   
2                  NaN                                  0   
3                  NaN                                  0   
4             rovaee 3                                  0   

   Personality_Disqualification_Dico_new  
0                                 0.0001  
1                                 0.0001  
2                                 0.0001  
3                                 0.0001  
4                                 0.0001  
def anova(df):
    temp = df_categorical_anova.copy()
    anova_df = pd.DataFrame(columns=['p-value','F'])
    for i in temp:
        if i != 'Personality_Disqualification_Dico_new':
            temp['log(Personality_Disqualification_Dico_new)']=np.log10(temp['Personality_Disqualification_Dico_new'])
            CategoryGroupLists=temp.groupby(i)['log(Personality_Disqualification_Dico_new)'].apply(list)
            AnovaResults = f_oneway(*CategoryGroupLists)
            anova=AnovaResults[0]
            pval=AnovaResults[1]
            b="{}".format(i)
            anova_df.loc[b] = [pval, anova]
            anova_df = anova_df.loc[anova_df['p-value'] < 0.05]
            anova_df = anova_df.sort_values(by=['F'], ascending=False)           
    return anova_df
categorical_anova = anova(df_categorical)
categorical_anova.round(2)
                                   p-value      F
Personality_Disqualification_Dico      0.0    inf
Designated_Role                        0.0  18.55
Education_Level                        0.0   4.35
Rovaee_Training_Type                   0.0   3.36
The outcomes of the one-way ANOVA test indicate that there exists a statistically significant disparity among the categories of the categorical variables concerning three out of the four variables: Designated_Role, Rovaee_Training_Type, and Education_Level. Notably, the Educational_Institution variable was not subjected to analysis due to one of its departments being entirely unrepresented within the personality disqualification category of the target factor. In other words, there were no instances of individuals with a university degree being disqualified in the personality test.
Since a comprehensive examination of the relationships between the project's predictors and the target factor will be conducted in more advanced stages, we will refrain from engaging in further follow-up tests to pinpoint the exact source of the distinctions between the groups (e.g., TEKY / Bonferoni tests).
It's also pertinent to mention that during a preliminary examination, it was determined that the relationship between the type of educational institution and personality disqualification is not statistically significant (F(2,1333) = 2.32, p > 0.05).
12.2.5 One Way ANOVA - Dummy Variables
In this segment, we will investigate variations in the proportion of candidates who were disqualified in the personality test across the different classes of dichotomous variables within the project.
In circumstances where the independent variable comprises three or more categories, an appropriate method for assessing variance is the one-way analysis of variance (ANOVA). However, this method is not applicable for comparing two independent or dependent samples. Conversely, when the independent variable takes on only two categories (dichotomous or binary order), the two-sample T-test can be employed to evaluate the mean differences between groups, and it can also function as a surrogate for conducting one-way ANOVA. In other words, the results from the ANOVA test and the two-sample T-test for dependent or independent samples are completely congruent, and their scores can be perfectly matched by extracting the square root or by squaring.
fig = plt.figure(figsize=(20,75))
c = 1
for i in df_dummy:
    if i != 'Personality_Disqualification_Dico':
        plot_order = df_dummy.groupby(i)['Personality_Disqualification_Dico'].mean().sort_values(ascending=False).index.values
        plt.subplot(20, 3, c)
        plt.title('{}'.format(i))
        plt.title('Box Plot: {}'.format(col), fontsize=10)
        plt.xlabel('{}'.format(col), fontsize=12)

        plt.xlabel(i)
        sns.barplot(x=i, y='Personality_Disqualification_Dico', data=df_dummy, order=plot_order)
        c = c + 1

plt.show()
 
The charts illustrate that candidates who exhibited elevations on the personality scales had a higher likelihood of being disqualified in the personality test, compared to candidates who did not exhibit such elevations.
While the individual differences in the rate of personality disqualification appear relatively modest, it is reasonable to infer that the cumulative impact of numerous elevations could have a significant effect.
The forthcoming section will provide the outcomes of a one-way analysis of variance (ANOVA) test, which aims to analyze variations in the frequency of personality disorders based on the dichotomous variable categories.
df_dummy_anova = pd.DataFrame(df_dummy) 
df_dummy_anova_new = df_dummy_anova['Personality_Disqualification_Dico_new'] = df_dummy.Personality_Disqualification_Dico * 100 + 0.0001 
df_dummy_anova = df_dummy_anova.convert_dtypes()
def anova(df_dummy_anova):
    temp = df_dummy_anova.copy()
    anova_df_dummy = pd.DataFrame(columns=['p-value','F'])
    for i in temp:
        if i != 'Personality_Disqualification_Dico_new':
            temp['log(Personality_Disqualification_Dico_new)']=np.log10(temp['Personality_Disqualification_Dico_new'])
            CategoryGroupLists=temp.groupby(i)['log(Personality_Disqualification_Dico_new)'].apply(list)
            AnovaResults = f_oneway(*CategoryGroupLists)
            anova=AnovaResults[0]
            pval=AnovaResults[1]
            b="{}".format(i)
            anova_df_dummy.loc[b] = [pval, anova]
            anova_df_dummy = anova_df_dummy.loc[anova_df_dummy['p-value'] < 0.05]
            anova_df_dummy = anova_df_dummy.sort_values(by=['F'], ascending=False)           
    return anova_df_dummy.head(20)
dummy_anova = anova(df_dummy_anova)
dummy_anova.round()
                                   p-value     F
Personality_Disqualification_Dico      0.0   inf
PT_dico                                0.0  88.0
PD_dico                                0.0  81.0
FAM_dico                               0.0  70.0
ANX_dico                               0.0  67.0
HEA_dico                               0.0  65.0
MDS_dico                               0.0  51.0
D1_dico                                0.0  51.0
ASP_dico                               0.0  49.0
M2a_dico                               0.0  44.0
F_dico                                 0.0  44.0
BIM_dico                               0.0  43.0
SC_dico                                0.0  42.0
D4_dico                                0.0  42.0
WRK_dico                               0.0  41.0
VRIN_dico                              0.0  40.0
DEP_dico                               0.0  39.0
OBS_dico                               0.0  37.0
Gender_Dico                            0.0  36.0
BIZ_dico                               0.0  34.0
12.3 Numeric and Dummies Variables correlations with the outcome variable
The tests and analyses described in the following sections offer a unique perspective compared to the statistical inference tests utilized thus far. These upcoming methods allow us to evaluate the individual contributions of the predictors within the models, all while controlling for or neutralizing the potential influence of other independent variables present in the model. This approach enables the isolation of the distinct relative importance of each predictor in forecasting the target outcome. Additionally, the models to be presented facilitate the inclusion of various types of variables, be they continuous, categorical, or dichotomous. The coding of these variables can be standardized to a binary representation (0, 1), allowing for a cohesive treatment within the analysis. Furthermore, these models offer the capability to assess the collective correlation or the percentage of variance that can be attributed to the prediction of the target outcome, considering all the predictors integrated into the model.
It's important to reiterate that the upcoming sections will provide a general overview of the significant predictors and their correlation strength with the target variable. This initial analysis is being conducted with a large number of predictors included in the prediction models. However, these models have not yet undergone the necessary steps to address issues such as multicollinearity and other potential sources of bias. Due to the inclusion of numerous predictors and their potential multicollinearity, the findings in these models may suffer from various methodological and statistical challenges. These include concerns such as - alpha inflation, the potential for inflated regression coefficients, limited generalizability to the larger population, the risk of overfitting, and more. therefore, it's crucial to acknowledge that the purpose of next analyses is to provide an initial and preliminary understanding.
**To address these issues and refine the prediction models, in more advanced stages of the project (including data enrichment, data engineering, and feature selection) we will implement established statistical tests and procedures to effectively address the issue of multicollinearity among variables. This will involve several strategies to mitigate the impact of multicollinearity and ensure the reliability of the predictive models. Some of these strategies include:
•	Factor Analysis: We will conduct exploratory factor analysis to identify latent factors that explain the variance shared among correlated variables. This will help us consolidate related variables and create composite measures that can capture underlying dimensions while reducing multicollinearity.
•	Tolerance and Variance Inflation Factor (VIF): We will calculate tolerance and VIF for each predictor to assess their multicollinearity relationships. Variables with high VIF values will be further examined to determine whether they need to be combined, transformed, or removed from the model.
•	Creating Indexes/Composite Measures: By combining correlated variables into a single index or composite measure, we can reduce multicollinearity while retaining important information. These indexes should reflect the underlying constructs that the correlated variables represent.
•	Stepwise Regression: Utilizing stepwise regression techniques, we can systematically select variables for inclusion in the model while accounting for multicollinearity. This approach helps identify a subset of predictors that collectively contribute to the model's predictive power.
•	Regularization Techniques: Techniques such as ridge regression and LASSO (L1 regularization) can help control multicollinearity by adding a penalty term to the regression coefficients. This encourages the model to prioritize important predictors while reducing the impact of less relevant ones.
•	Feature Selection: We will use various methods like recursive feature elimination, feature importance scores from tree-based models, and mutual information to identify and retain the most influential predictors while minimizing multicollinearity-related issues.
These strategies, along with careful consideration of the theoretical and practical significance of predictors, will enable us to refine the models and create more reliable and interpretable predictions. The goal is to build models that effectively capture the relationships between predictors and the target variable while addressing the complexities introduced by multicollinearity.
12.3.1 Logistic Reggresion Test
### first get all data into new dataframe 
df_logistic = pd.DataFrame(df2);

### keep in the data new frame only int \ float \ dummy (reprased by object type and coding as 1,0). no categories in \
### this analysis (categor vars can get in only after dummy coding) 

df_logistic = df_logistic.select_dtypes(include = ['float64', 'int64']);
#df_logistic.dtypes

### Drop or replace rows missing values - the analysis cant contaion missing
### In the current case we replaced missing with mean value of the variables, following to many missing values
### in dummy vars - missing values gets the mean proportion of the categor 1 representaion.
### in a significant number of variables (if we drop rows with missing almost no observations will left in the analysis) 

df_logistic=df_logistic.fillna(df_logistic.mean());
#df_logistic=df_logistic.dropna(axis=0)

### Drop dummy variable with 0% represation in one of the categorits
#df_logistic = df_logistic.drop(columns=['M2APS_dico', 'WSD_dico', 'DIS_dico', 'Q_dico'])

### check no missing in the dataframe
#df_logistic.isnull().sum()

### check all the variables in float or int types - the analysis cant contaion variables with types
#df_logistic.head()
#df_logistic.dtypes

df_logistic.dtypes
id                                     int64
Assessment_Center_Grade              float64
Cognitive_Ability_Test               float64
Morals_and_Values_Test                 int64
Job_Interview_Grade                  float64
                                      ...   
TRIN                                 float64
VRIN                                 float64
CR                                   float64
RS                                   float64
Personality_Disqualification_Dico      int64
Length: 61, dtype: object
### set train and test data in diff objects. this anasyis must contaion at list two predictors in x (tarin data) 

x = df_logistic[df_logistic.columns[~df_logistic.columns.isin(['Personality_Disqualification_Dico'])]]
y = df_logistic['Personality_Disqualification_Dico']
x = sm.add_constant(x);
### Choose the Relevant Prediction Model 

model = sm.Logit(y, x)
from numpy.linalg import inv, det
### set diff paramters in attempt to improve the quality of the model

result = model.fit(method='newton', C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='warn', n_jobs=None, penalty='l2', random_state=0, solver='liblinear', tol=0.0001, verbose=0, warm_start=False);
Optimization terminated successfully.
         Current function value: 0.231031
         Iterations 7
paramters : (C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='warn', n_jobs=None, penalty='l2', random_state=0, solver='liblinear', tol=0.0001, verbose=0, warm_start=False)
### see the result params \ prediction of the model

#result.params
#result.predict(x)
#result.pred_table()

### two reports with quality measures of the models

result.summary()
#result.summary2()
<class 'statsmodels.iolib.summary.Summary'>
"""
                                   Logit Regression Results                                  
=============================================================================================
Dep. Variable:     Personality_Disqualification_Dico   No. Observations:                 2300
Model:                                         Logit   Df Residuals:                     2239
Method:                                          MLE   Df Model:                           60
Date:                               Wed, 30 Aug 2023   Pseudo R-squ.:                  0.2463
Time:                                       00:46:01   Log-Likelihood:                -531.37
converged:                                      True   LL-Null:                       -705.05
Covariance Type:                           nonrobust   LLR p-value:                 4.547e-42
=================================================================================================
                                    coef    std err          z      P>|z|      [0.025      0.975]
-------------------------------------------------------------------------------------------------
const                            -8.9794     18.167     -0.494      0.621     -44.586      26.627
id                               -0.0006      0.000     -4.402      0.000      -0.001      -0.000
Assessment_Center_Grade           0.7174      0.123      5.839      0.000       0.477       0.958
Cognitive_Ability_Test           -0.0910      0.093     -0.981      0.326      -0.273       0.091
Morals_and_Values_Test            0.1360      0.035      3.930      0.000       0.068       0.204
Job_Interview_Grade              -1.2937      0.165     -7.854      0.000      -1.617      -0.971
Heberew_Test_Grade               -0.0376      0.129     -0.293      0.770      -0.290       0.214
Filling_Instruction_Test          0.0438      0.043      1.028      0.304      -0.040       0.127
Emotional_Inteligence_Test       -0.1581      0.047     -3.356      0.001      -0.250      -0.066
Recruit_Quality_Measure_Score    -0.0192      0.034     -0.571      0.568      -0.085       0.047
Honesty_Test                     -0.0124      0.047     -0.262      0.794      -0.105       0.081
Years_of_Education               -0.1929      0.073     -2.643      0.008      -0.336      -0.050
Age                               0.0041      0.016      0.259      0.796      -0.027       0.035
M2a                              -0.0165      0.037     -0.440      0.660      -0.090       0.057
ASP                               0.0323      0.020      1.602      0.109      -0.007       0.072
R                                -0.0128      0.015     -0.837      0.402      -0.043       0.017
CYN                              -0.0358      0.020     -1.763      0.078      -0.076       0.004
BIM                               0.0639      0.032      1.994      0.046       0.001       0.127
PA                                0.0041      0.013      0.315      0.753      -0.021       0.030
M2APS                            -0.0208      0.019     -1.122      0.262      -0.057       0.016
AAS                              -0.0229      0.017     -1.326      0.185      -0.057       0.011
tcent                             0.1435      0.175      0.822      0.411      -0.199       0.486
OBS                               0.0135      0.020      0.680      0.496      -0.025       0.052
MDS                               0.0122      0.015      0.811      0.418      -0.017       0.042
TPA                              -0.0184      0.019     -0.992      0.321      -0.055       0.018
BIZ                              -0.0230      0.016     -1.472      0.141      -0.054       0.008
ANG                               0.0035      0.021      0.167      0.868      -0.037       0.044
WSD                               0.1868      0.053      3.545      0.000       0.084       0.290
DEP                               0.0369      0.026      1.402      0.161      -0.015       0.089
DIS                               0.0433      0.038      1.147      0.252      -0.031       0.117
K                                -0.0080      0.034     -0.235      0.814      -0.075       0.059
SOD                              -0.0147      0.020     -0.750      0.453      -0.053       0.024
D                                 0.0390      0.030      1.311      0.190      -0.019       0.097
D1                               -0.0851      0.033     -2.586      0.010      -0.150      -0.021
D2                               -0.0053      0.016     -0.329      0.742      -0.037       0.026
D3                               -0.0355      0.014     -2.579      0.010      -0.062      -0.009
D4                               -0.0335      0.023     -1.482      0.138      -0.078       0.011
D5                               -0.0665      0.026     -2.587      0.010      -0.117      -0.016
MF                               -0.0158      0.010     -1.578      0.115      -0.035       0.004
DOE                               0.0020      0.019      0.105      0.916      -0.035       0.039
PT                               -0.0005      0.021     -0.022      0.982      -0.042       0.041
ANX                              -0.0011      0.022     -0.048      0.962      -0.045       0.043
F                                -0.0201      0.014     -1.438      0.151      -0.047       0.007
SC                               -0.0045      0.018     -0.253      0.801      -0.040       0.031
HEA                               0.0125      0.022      0.579      0.562      -0.030       0.055
FP                                0.0124      0.010      1.210      0.226      -0.008       0.033
HS                                0.0436      0.025      1.750      0.080      -0.005       0.093
FAM                               0.0102      0.018      0.581      0.561      -0.024       0.045
PD                                0.0600      0.017      3.441      0.001       0.026       0.094
ES                                0.0361      0.017      2.082      0.037       0.002       0.070
FCENT                             0.0942      0.175      0.539      0.590      -0.248       0.437
WRK                               0.0209      0.025      0.846      0.398      -0.027       0.069
HY                               -0.0195      0.016     -1.222      0.222      -0.051       0.012
L                                 0.0014      0.012      0.118      0.906      -0.022       0.025
MA                               -0.0026      0.012     -0.212      0.832      -0.027       0.022
SE                               -0.0041      0.016     -0.249      0.804      -0.036       0.028
RT                                0.0039      0.018      0.213      0.832      -0.032       0.040
TRIN                              0.0027      0.011      0.238      0.812      -0.020       0.025
VRIN                             -0.0115      0.012     -0.995      0.320      -0.034       0.011
CR                               -0.0173      0.016     -1.061      0.288      -0.049       0.015
RS                                0.0006      0.012      0.050      0.960      -0.022       0.023
=================================================================================================
"""
12.3.2 claasification matrix (not relevant at this stage)
#model = LogisticRegression(solver='liblinear', random_state=0)
#model.fit(x, y);

#model = LogisticRegression(solver='liblinear', random_state=0).fit(x, y)

###print(model.classes_)
###print(model.intercept_)
###print(model.coef_.round(3))
###print(model.predict(x))

#cm = confusion_matrix(y, model.predict(x))

#fig, ax = plt.subplots(figsize=(5, 5))
#ax.imshow(cm)
#ax.grid(False)
#ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))
#ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))
#ax.set_ylim(1.5, -0.5)
#for i in range(2):
#    for j in range(2):
#        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')
#plt.show()
#print(classification_report(y, model.predict(x)))
12.3.3 Multiple linear Reggresion - Tracing after multiculinear predictors variables using VIF facror
Generallty, multivariable linear regression using the Ordinary Least Squares (OLS) method is not suitable for binary classification problems like the one you're dealing with in your project. OLS regression is designed for continuous outcome variables, and it's not directly applicable to predicting binary outcomes. For binary classification tasks, logistic regression is a more appropriate choice. Logistic regression models the probability of an event occurring, making it well-suited for predicting binary outcomes like the presence or absence of personality disqualification. In logistic regression, the logistic function is used to transform the linear combination of predictors into a probability score between 0 and 1, which can then be thresholded to make class predictions.
However, linear reggression use as a important statistical analysis to swiftly and intuitively identify multicollinear relationships among predictor by assessing tolerance and VIF indices. Tolerance is the reciprocal of the VIF and indicates the proportion of variance in a predictor that is not explained by other predictors in the model. Higher VIF values suggest stronger multicollinearity. Additionally, multivariate linear regression offers a clear and intuitive means to showcase the relative significance of factors within the model in predicting the target variable. This is achieved through the utilization of beta coefficients, which can also be translated into percentages, highlighting the importance of each factor in predicting the criterion.
In accordance with the explanation given above, in the next part we will conduct a linear regression analysis in which we will focus on the VIF index only, and this, for the purpose of obtaining a general indication regarding factors independent of the project characterized by multicollinear relationships.
### first get all data into new dataframe 
df_linear_regg = pd.DataFrame(df2);

### keep in the data new frame only int \ float \ dummy (reprased by object type and coding as 1,0). no categories in \
### this analysis (categor vats can get in only after dummy coding) 

df_linear_regg = df_linear_regg.select_dtypes(include = ['float64', 'int64']);
#df_linear_regg.dtypes

### Drop or replace rows missing values - the analysis cant contaion missing
### In the current case we replaced missing with mean value of the variables, following to many missing values
### in dummy vars - missing values gets the mean proportion of the categor 1 representaion (this not cause biase).
### in a significant number of variables (if we drop rows with missing almost no observations will left in the analysis) 

df_linear_regg=df_logistic.fillna(df_linear_regg.mean());
#df_linear_regg=df_linear_regg.dropna(axis=0)


### check no missing in the dataframe
#df_linear_regg.isnull().sum()


### check all the variables in float or int types - the analysis cant contaion variables with types
#df_linear_regg.dtypes


### set train and test data in diff objects. this anasyis must contaion at list two predictors in x (tarin data) 

x = df_linear_regg[df_linear_regg.columns[~df_linear_regg.columns.isin(['Personality_Disqualification_Dico'])]]
y = df_linear_regg['Personality_Disqualification_Dico']
x = sm.add_constant(x);

regr = linear_model.LinearRegression()
regr.fit(x, y);

### see models intercept and Coefficients

#print('Intercept: \n', regr.intercept_)
#print('Coefficients: \n', regr.coef_)


### Choose the Relevant Prediction Model 

model = sm.OLS(y, x).fit();

### set diff paramters in attempt to improve the quality of the model

predictions = model.predict(x);
### See quality measurs of the model

print_model = model.summary()
print(print_model)
                                    OLS Regression Results                                   
=============================================================================================
Dep. Variable:     Personality_Disqualification_Dico   R-squared:                       0.161
Model:                                           OLS   Adj. R-squared:                  0.139
Method:                                Least Squares   F-statistic:                     7.186
Date:                               Wed, 30 Aug 2023   Prob (F-statistic):           1.31e-51
Time:                                       00:46:01   Log-Likelihood:                -203.25
No. Observations:                               2300   AIC:                             528.5
Df Residuals:                                   2239   BIC:                             878.7
Df Model:                                         60                                         
Covariance Type:                           nonrobust                                         
=================================================================================================
                                    coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------------------------
const                            -0.8268      1.145     -0.722      0.470      -3.071       1.418
id                            -3.984e-05   8.87e-06     -4.492      0.000   -5.72e-05   -2.25e-05
Assessment_Center_Grade           0.0507      0.008      6.426      0.000       0.035       0.066
Cognitive_Ability_Test           -0.0050      0.006     -0.777      0.437      -0.018       0.008
Morals_and_Values_Test            0.0155      0.003      5.110      0.000       0.010       0.021
Job_Interview_Grade              -0.0851      0.011     -7.749      0.000      -0.107      -0.064
Heberew_Test_Grade                0.0029      0.009      0.324      0.746      -0.014       0.020
Filling_Instruction_Test          0.0026      0.003      0.892      0.373      -0.003       0.008
Emotional_Inteligence_Test       -0.0105      0.003     -3.451      0.001      -0.016      -0.005
Recruit_Quality_Measure_Score    -0.0009      0.002     -0.546      0.585      -0.004       0.002
Honesty_Test                     -0.0025      0.003     -0.785      0.432      -0.009       0.004
Years_of_Education               -0.0086      0.004     -2.250      0.025      -0.016      -0.001
Age                               0.0001      0.001      0.107      0.915      -0.002       0.002
M2a                              -0.0010      0.003     -0.369      0.712      -0.006       0.004
ASP                               0.0027      0.002      1.787      0.074      -0.000       0.006
R                                -0.0005      0.001     -0.508      0.611      -0.003       0.002
CYN                              -0.0039      0.001     -2.717      0.007      -0.007      -0.001
BIM                               0.0062      0.002      2.493      0.013       0.001       0.011
PA                                0.0007      0.001      0.701      0.483      -0.001       0.003
M2APS                            -0.0017      0.001     -1.295      0.196      -0.004       0.001
AAS                              -0.0025      0.001     -1.998      0.046      -0.005   -4.68e-05
tcent                             0.0126      0.011      1.144      0.253      -0.009       0.034
OBS                               0.0009      0.001      0.625      0.532      -0.002       0.004
MDS                               0.0007      0.001      0.644      0.520      -0.001       0.003
TPA                              -0.0011      0.001     -0.787      0.431      -0.004       0.002
BIZ                              -0.0018      0.001     -1.700      0.089      -0.004       0.000
ANG                              -0.0007      0.001     -0.490      0.624      -0.004       0.002
WSD                               0.0140      0.004      3.695      0.000       0.007       0.021
DEP                               0.0032      0.002      1.717      0.086      -0.000       0.007
DIS                               0.0035      0.003      1.225      0.221      -0.002       0.009
K                                -0.0016      0.002     -0.650      0.516      -0.006       0.003
SOD                              -0.0007      0.001     -0.508      0.612      -0.004       0.002
D                                 0.0014      0.002      0.674      0.500      -0.003       0.006
D1                               -0.0050      0.002     -2.112      0.035      -0.010      -0.000
D2                               -0.0007      0.001     -0.580      0.562      -0.003       0.002
D3                               -0.0022      0.001     -2.194      0.028      -0.004      -0.000
D4                               -0.0011      0.002     -0.685      0.493      -0.004       0.002
D5                               -0.0059      0.002     -3.029      0.002      -0.010      -0.002
MF                               -0.0009      0.001     -1.340      0.180      -0.002       0.000
DOE                               0.0005      0.001      0.349      0.727      -0.002       0.003
PT                                0.0001      0.002      0.066      0.948      -0.003       0.003
ANX                              -0.0008      0.002     -0.476      0.634      -0.004       0.002
F                                -0.0008      0.001     -0.707      0.480      -0.003       0.001
SC                             9.981e-05      0.001      0.070      0.944      -0.003       0.003
HEA                               0.0009      0.002      0.571      0.568      -0.002       0.004
FP                                0.0010      0.001      1.333      0.183      -0.000       0.002
HS                                0.0029      0.002      1.622      0.105      -0.001       0.007
FAM                               0.0008      0.001      0.601      0.548      -0.002       0.003
PD                                0.0051      0.001      3.992      0.000       0.003       0.008
ES                                0.0030      0.001      2.345      0.019       0.000       0.005
FCENT                             0.0074      0.011      0.668      0.504      -0.014       0.029
WRK                               0.0008      0.002      0.452      0.652      -0.003       0.004
HY                               -0.0010      0.001     -0.920      0.358      -0.003       0.001
L                                 0.0006      0.001      0.681      0.496      -0.001       0.002
MA                               -0.0007      0.001     -0.808      0.419      -0.002       0.001
SE                            -9.892e-05      0.001     -0.084      0.933      -0.002       0.002
RT                                0.0009      0.001      0.694      0.488      -0.002       0.004
TRIN                              0.0006      0.001      0.716      0.474      -0.001       0.002
VRIN                             -0.0010      0.001     -1.169      0.243      -0.003       0.001
CR                               -0.0010      0.001     -0.875      0.382      -0.003       0.001
RS                                0.0004      0.001      0.447      0.655      -0.001       0.002
==============================================================================
Omnibus:                     1016.150   Durbin-Watson:                   1.915
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3966.931
Skew:                           2.224   Prob(JB):                         0.00
Kurtosis:                       7.648   Cond. No.                     2.80e+05
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 2.8e+05. This might indicate that there are
strong multicollinearity or other numerical problems.
### Script to get IVF factor - in predictors with factor 5 and more - there is multiculinear correlation problem 
###with one or more predictors in the model

x = sm.add_constant(x)

vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(x.values.shape[1])]
vif["features"] = x.columns
print(vif.round(1))
    VIF Factor                 features
0      41985.6                    const
1          1.1                       id
2          1.3  Assessment_Center_Grade
3          2.4   Cognitive_Ability_Test
4          1.3   Morals_and_Values_Test
..         ...                      ...
56         2.7                       RT
57         1.4                     TRIN
58         1.9                     VRIN
59         2.5                       CR
60         1.7                       RS

[61 rows x 2 columns]
In the table above, we can identify variables with a VIF factor value of 5 or higher. Such variables exhibit multicollinear relationships with other variables in the model. Notably, numerous personality scales within the MMPI-2 test display these multicollinear associations with other variables. However, there are no signs of multicollinearity issues concerning the research factors connected to the police recruitment process . Throughout the data enrichment and feature selection stages, a range of techniques will be employed to mitigate the challenge of multicollinearity among these pertinent variables.
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Outliers and Missing Values Examinatios
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
13. Outliers Examintion
The plotted charts depict the interquartile range (IQR) of the data. The height of the blue box represents the observations within the interquartile range, which spans from the first quartile (q1) to the third quartile (q3). The vertical lines extending from the box denote the maximum and minimum points of the distribution. For each relevant variable, the blue box showcases the distribution's central tendency. The points outside the vertical lines are considered extreme values in the variable.
The continuous variables in the project are categorized into three main groups: socio-demographic factors, selection test factors, and MMPI-2 personality scales. To facilitate clarity, the box plot charts will be presented separately for each of these three factor groups.
The project continuous variables divided into three main groups - socio-demographic factors; selection test factors; MMPI-2 personality scales. For convenience, the box plot charts will be presented separately for each of the three groups of factors.
13.1 Division to Factors Groups
df_num.dtypes
id                                     int64
Assessment_Center_Grade              float64
Cognitive_Ability_Test               float64
Morals_and_Values_Test                 int64
Job_Interview_Grade                  float64
                                      ...   
TRIN                                 float64
VRIN                                 float64
CR                                   float64
RS                                   float64
Personality_Disqualification_Dico      int64
Length: 61, dtype: object
socio_demographic_plots = df[['Years_of_Education', 'Age']].copy()
selection_test_plots = df[['Assessment_Center_Grade', 'Cognitive_Ability_Test', 'Morals_and_Values_Test', 'Job_Interview_Grade', 'Heberew_Test_Grade', 'Filling_Instruction_Test', 'Emotional_Inteligence_Test', 'Recruit_Quality_Measure_Score', 'Honesty_Test']].copy()
#MMPI_2_scales_plots = df[df_num.columns[~df_num.columns.isin(['cnt', 'id'])]]

MMPI_2_scales_plots = df[df_num.columns[~df_num.columns.isin(['Assessment_Center_Grade', 'Cognitive_Ability_Test', 'Morals_and_Values_Test', 'Job_Interview_Grade', 'Heberew_Test_Grade', 'Filling_Instruction_Test', 'Emotional_Inteligence_Test', 'Recruit_Quality_Measure_Score', 'Honesty_Test', 'Honesty_Test', 'Years_of_Education', 'Age', 'Personality_Disqualification_Dico', 'Q'])]]
13.1.1 Socio-demographic factors
plt.figure(figsize=(20,200))

def outliers_boxplot(socio_demographic_plots):
    for i, col in enumerate(socio_demographic_plots.columns):
        ax = plt.subplot(60, 3, i+1)
        sns.boxplot(data=df, x=col, ax=ax)
        plt.subplots_adjust(hspace = 0.7)
        plt.title('Box Plot: {}'.format(col), fontsize=15)
        plt.xlabel('{}'.format(col), fontsize=14)
        
outliers_boxplot(socio_demographic_plots)
 
As can be seen in the two charts above, extreme values were found in the two continuous socio-demographic factors
13.1.2 Selection test plots
plt.figure(figsize=(20,200))

def outliers_boxplot(selection_test_plots):
    for i, col in enumerate(selection_test_plots.columns):
        ax = plt.subplot(60, 3, i+1)
        sns.boxplot(data=df, x=col, ax=ax)
        plt.subplots_adjust(hspace = 0.7)
        plt.title('Box Plot: {}'.format(col), fontsize=15)
        plt.xlabel('{}'.format(col), fontsize=14)
        
outliers_boxplot(selection_test_plots)
 
As can be seen in the nine charts above, beside the factors Heberew_Test_Grade and Assessment_Center_Grade, extreme values were found in all other seven factors, Mainly in variables : Morals_and_Values_Test, Filling_Instruction_Test, Honesty_Test and Recruit_Quality_Measure.
13.1.3 MMPI-2 scales plots
plt.figure(figsize=(20,200))

def outliers_boxplot(MMPI_2_scales_plots):
    for i, col in enumerate(MMPI_2_scales_plots.columns):
        ax = plt.subplot(60, 3, i+1)
        sns.boxplot(data=df, x=col, ax=ax)
        plt.subplots_adjust(hspace = 0.7)
        plt.title('Box Plot: {}'.format(col), fontsize=15)
        plt.xlabel('{}'.format(col), fontsize=14)
        
outliers_boxplot(MMPI_2_scales_plots)
 
Also with reference to the factors of the personality scales in the MMPI-2 test, a considerable number of extreme values can be seen. The only personality scale in which no extreme values were found is - "DOE".
Important Note :
In a conscientious effort to prevent biases that could potentially lead to issues like overfitting and generalization challenges, it is advisable to consider omitting extreme values from the research variables. However, exercising discretion is crucial in this regard, taking into account the inherent characteristics of the predictor variable, the data type under examination, and the intricate relationship between the independent variables and the dependent variable.
It's worth noting that after extensive consultation with key stakeholders, including the head of the police research unit at the "DBS" (Department of Behavioral Sciences) and senior PhD researchers within the selection and behavioral sciences departments at the Israel Police, a unanimous consensus was reached. Omitting extreme values from the MMPI-2 test scales was deemed incongruent with the purpose and essence of these scales.
In the context of evaluating candidate disqualification based on personality mismatch, an essential criterion is the identification of abnormal scores on personality scales. A significant number of candidates who are disqualified during the police selection process due to personality incompatibility exhibit extreme or abnormal scores on the MMPI-2 personality scales.
Considering this, the exclusion of extreme values from the MMPI-2 personality scales would undermine their utility in pinpointing candidates with elevated scores—a core objective in the selection process. Hence, since the rationale behind the application of personality scales would be compromised by their omission, it was concluded that such omission is unwarranted.
Given the importance of these scales and their integral role in assessing candidate suitability, the decision was made not to omit extreme values from the MMPI-2 personality scales within the research variables. This decision stands irrespective of any potential impact on the distribution of the variable or its relationship with the dependent variable.
14. Missing Values Examintions
14.1 Number  proportion of missing values among project variables
df2.isnull().sum().sort_values(ascending=False)
Recruit_Quality_Measure_Score        1947
Combat_Army_Unit_Dico                1582
Commander_in_Army_Dico               1272
Emotional_Inteligence_Test           1031
Assessment_Center_Grade               988
                                     ... 
Gender_Dico                             0
Age                                     0
Honesty_Test                            0
Morals_and_Values_Test                  0
Personality_Disqualification_Dico       0
Length: 114, dtype: int64
The table presented above reveals a substantial occurrence of missing values across several variables in the dataset:
Recruit_Quality_Measure_Score Combat_Army_Unit_Dico Commander_in_Army_Dico Emotional_Intelligence_Test
Furthermore, a noteworthy proportion of missing values was observed in the following variables:
Assessment_Center_Grade Educational_Institution Education_Level Rovaee_Training_Type
To illustrate the extent of these missing values, the percentage of missing values for each project variable is provided below:
df_is_null = df2.isnull().sum().sort_values(ascending=False)
print(df_is_null/len(df2)*100)
Recruit_Quality_Measure_Score        84.652174
Combat_Army_Unit_Dico                68.782609
Commander_in_Army_Dico               55.304348
Emotional_Inteligence_Test           44.826087
Assessment_Center_Grade              42.956522
                                       ...    
Gender_Dico                           0.000000
Age                                   0.000000
Honesty_Test                          0.000000
Morals_and_Values_Test                0.000000
Personality_Disqualification_Dico     0.000000
Length: 114, dtype: float64
In variables Recruit_Quality_Measure_Score (82.4% missing) and Combat_Army_Unit_Dico (63.8% missing) most of the data are missing values. Of course, data completion is not relevant to these variables. These variables will be omitted at the end of this chapter and will not be used for the development of the models.
Additional variables in which 30% or more of the data are missing are: Commander_in_Army_Dico; Emotional_Inteligence_Test; Assessment_Center_Grade; Educational_Institution; Education_Level; Rovaee_Training_Type.
14.2 Contain only variables with missing values in the data
df_nulls = df2.copy()
for col in df_nulls:
    if df_nulls[col].isna().sum() == 0:
        del df_nulls[col]
df_nulls.head()
  Population_Type_Dico         Designated_Role  Assessment_Center_Grade  \
0                  0.0              prosecutor                      4.9   
1                  NaN     operational officer                      4.3   
2                  0.0  administrative officer                      4.4   
3                  0.0  administrative officer                      5.1   
4                  0.0              prosecutor                      4.9   

   Cognitive_Ability_Test  Job_Interview_Grade  Heberew_Test_Grade  \
0                     7.0                  4.0                 9.0   
1                     6.0                  NaN                 8.0   
2                     9.0                  5.0                 9.0   
3                     6.0                  5.0                 9.0   
4                     7.0                  5.0                 9.0   

   Filling_Instruction_Test  Emotional_Inteligence_Test  \
0                 13.500000                        23.0   
1                       NaN                        15.0   
2                 17.799999                        20.0   
3                 11.000000                        20.0   
4                 12.000000                        24.0   

   Recruit_Quality_Measure_Score Education_Level  ... WRK_dico  HY_dico  \
0                            NaN             NaN  ...      0.0      0.0   
1                            NaN             NaN  ...      NaN      NaN   
2                            NaN             NaN  ...      0.0      0.0   
3                            NaN             NaN  ...      0.0      0.0   
4                            NaN             NaN  ...      0.0      0.0   

  L_dico MA_dico SE_dico  RT_dico  TRIN_dico  VRIN_dico  CR_dico  RS_dico  
0    0.0     0.0     0.0      0.0        0.0        0.0      0.0      0.0  
1    NaN     NaN     NaN      NaN        NaN        NaN      NaN      NaN  
2    1.0     0.0     0.0      0.0        0.0        0.0      0.0      0.0  
3    1.0     0.0     0.0      0.0        1.0        0.0      0.0      0.0  
4    0.0     1.0     0.0      0.0        0.0        0.0      0.0      0.0  

[5 rows x 108 columns]
mis_val = df_nulls.isna().sum()
mis_val_per = df_nulls.isna().sum()/len(df_nulls)*100
mis_val_table = pd.concat([mis_val, mis_val_per], axis=1)
mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})
mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,:] != 0].sort_values(
    '% of Total Values', ascending=False).round(1)
mis_val_table_ren_columns
                               Missing Values  % of Total Values
Recruit_Quality_Measure_Score            1947               84.7
Combat_Army_Unit_Dico                    1582               68.8
Commander_in_Army_Dico                   1272               55.3
Emotional_Inteligence_Test               1031               44.8
Assessment_Center_Grade                   988               43.0
...                                       ...                ...
DOE                                        29                1.3
MF                                         29                1.3
D5                                         29                1.3
RS_dico                                    29                1.3
Years_of_Education                         16                0.7

[108 rows x 2 columns]
14.3 Setting data (only variables with missing values) as int type
df_missing_data = (df2.isna()).astype('int64')
df_missing_data.head()
   id  Population_Type_Dico  Designated_Role  Assessment_Center_Grade  \
0   0                     0                0                        0   
1   0                     1                0                        0   
2   0                     0                0                        0   
3   0                     0                0                        0   
4   0                     0                0                        0   

   Cognitive_Ability_Test  Morals_and_Values_Test  Job_Interview_Grade  \
0                       0                       0                    0   
1                       0                       0                    1   
2                       0                       0                    0   
3                       0                       0                    0   
4                       0                       0                    0   

   Heberew_Test_Grade  Filling_Instruction_Test  Emotional_Inteligence_Test  \
0                   0                         0                           0   
1                   0                         1                           0   
2                   0                         0                           0   
3                   0                         0                           0   
4                   0                         0                           0   

   ...  HY_dico  L_dico  MA_dico  SE_dico  RT_dico  TRIN_dico  VRIN_dico  \
0  ...        0       0        0        0        0          0          0   
1  ...        1       1        1        1        1          1          1   
2  ...        0       0        0        0        0          0          0   
3  ...        0       0        0        0        0          0          0   
4  ...        0       0        0        0        0          0          0   

   CR_dico  RS_dico  Personality_Disqualification_Dico  
0        0        0                                  0  
1        1        1                                  0  
2        0        0                                  0  
3        0        0                                  0  
4        0        0                                  0  

[5 rows x 114 columns]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Part B - EDA Treatments
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
15. Clean Outliers
As demonstrated in previous sections, the variables within the dataset do not exhibit a normal distribution. Hence, the suitable approach for detecting and removing outliers involves utilizing the median as a reference point, with outliers being identified through a range of ±1.5 times the interquartile range (IQR) around the median. This method aligns with the non-normal distribution nature of the variables in question.
Z-scores method (not relevant methode for the current project)
### for only a vector of values
#def zscore(x):
#    return (x - np.mean(x)) / np.std(x)

### for an array or dataframe

#def zscores(values):
#    return [((x - np.mean(x)) / np.std(x)) for x in values]
#    return stats.zscore(values,axis=0,nan_policy='omit')
#df_num_new = df.select_dtypes(include = ['float', 'int64', 'int32'])
#z1 = zscores(df_num_new.loc[:,df_num_new.columns!='revenue'])
#z1
Eliminating outliers z-score > 2.5 or z-score < -2.5
#df_num_new.head(50)
#df_num_no_outliers_z = df_num_new.loc[:,df_num_new.columns!='Personality_Disqualification_Dico'].copy()
#df_num_no_outliers_z
#threshold = 2.5
#df_num_no_outliers_z = df_num_no_outliers_z.where((abs(z1) < threshold),np.nan)
#df_num_no_outliers_z
Median method for droping outliers (The selected method)
15.1 Identification and coding outliers in the numeric variables
As previously discussed, the personality scales derived from the MMPI-2 test serve a pivotal role in identifying candidates with abnormal scores, which significantly influences decisions regarding disqualification for police force roles. Omitting outliers within these personality scales would run counter to the fundamental purpose of the MMPI-2 test. Thus, the process of omitting outliers would not be applied to the variables representing personality scales.
Creating a data frame with factors on an interval / quotient scale, other than the personality scales:
df_1 = df.copy()

for col in df_1:
    if col in df_num.columns:
        df_1[col] = df_1[col].astype(np.float64)
        
#df_num = df.select_dtypes(include = ['float64', 'int64', 'int32', 'long' ])
### invluding all numeric variables beside of personality MMPI scales 

df_no_MMPI = pd.DataFrame(df, columns=['Assessment_Center_Grade', 'Cognitive_Ability_Test', 'Morals_and_Values_Test', 'Job_Interview_Grade', 'Heberew_Test_Grade', 'Filling_Instruction_Test', 'Emotional_Inteligence_Test', 'Recruit_Quality_Measure_Score', 'Honesty_Test', 'Years_of_Education', 'Age'])
def outliers_df(df_1):
    all_outliers = pd.DataFrame(columns={'Outlier count precent','N'})
    for col in df_no_MMPI:
        if col != 'Personality_Disqualification_Dico':
            temp = pd.DataFrame(df_1[col], columns={col})
            Q1 = df_1[col].quantile(0.25)
            Q3 = df_1[col].quantile(0.75)
            upper_limit = Q1 + 1.5 * (Q3-Q1)
            lower_limit = Q3 - 1.5 * (Q3-Q1)
            upper_outliers = temp[temp[col] > upper_limit]
            lower_outliers = temp[temp[col] < lower_limit]
            temp = upper_outliers.append(lower_outliers)
            b = "{}".format(col)
            num = len(temp)
            all_outliers.loc[b] = [num, num/len(df_1)*100]
    return all_outliers[all_outliers['N']>0]
       
outliers_df = outliers_df(df_1).sort_values('N', ascending=False)
outliers_df
                               Outlier count precent          N
Cognitive_Ability_Test                         984.0  42.782609
Job_Interview_Grade                            657.0  28.565217
Years_of_Education                             571.0  24.826087
Heberew_Test_Grade                             522.0  22.695652
Honesty_Test                                   427.0  18.565217
Morals_and_Values_Test                         321.0  13.956522
Age                                            309.0  13.434783
Filling_Instruction_Test                       223.0   9.695652
Assessment_Center_Grade                        160.0   6.956522
Emotional_Inteligence_Test                     143.0   6.217391
Recruit_Quality_Measure_Score                   39.0   1.695652
15.2 Vars distribtion with / without Outliers
The determination of whether to retain or omit extreme values in the research variables is guided by two key assessments:
- Impact on Distribution: Assessing whether the distribution of the variable undergoes changes after the omission of extreme values.
- Effect on Relationship Strength: Evaluating any alterations in the strength of the relationship between predictors and the target factor subsequent to the omission of extreme values.
In cases where the omission of extreme values results in alterations as described above, the decision will be made to retain the extreme values. Conversely, if such changes do not occur, the extreme values will be omitted.
Creating a table with coding "outlier" representing extreme values
outliers_df2 = df_num.fillna(0)
def outliers(df_1):
    new_df_cap = df_1.copy()
    for col in df_no_MMPI:
        if col in outliers_df.index:
            Q1 = new_df_cap[col].quantile(0.25)
            Q3 = new_df_cap[col].quantile(0.75)
            upper_limit = Q1 + 1.5 * (Q3-Q1)
            lower_limit = Q3 - 1.5 * (Q3-Q1)
            new_df_cap[col] = np.where(new_df_cap[col] > upper_limit,'Outlier',np.where(new_df_cap[col] < lower_limit,'Outlier',new_df_cap[col]))
    return new_df_cap
new_df_outliers = outliers(df_1)
new_df_outliers.head()
       id  Population_Type_Dico         Designated_Role  \
0   540.0                   0.0              prosecutor   
1  1128.0                   NaN     operational officer   
2   604.0                   0.0  administrative officer   
3   605.0                   0.0  administrative officer   
4   536.0                   0.0              prosecutor   

  Assessment_Center_Grade Cognitive_Ability_Test Morals_and_Values_Test  \
0             4.900000095                Outlier                    4.0   
1             4.300000191                    6.0                    1.0   
2             4.400000095                Outlier                    1.0   
3             5.099999905                    6.0                    2.0   
4             4.900000095                Outlier                Outlier   

  Job_Interview_Grade Heberew_Test_Grade Filling_Instruction_Test  \
0                 4.0            Outlier                     13.5   
1                 nan                8.0                      nan   
2             Outlier            Outlier                  Outlier   
3             Outlier            Outlier                     11.0   
4             Outlier            Outlier                     12.0   

  Emotional_Inteligence_Test  ... HY_dico L_dico MA_dico SE_dico RT_dico  \
0                    Outlier  ...     0.0    0.0     0.0     0.0     0.0   
1                       15.0  ...     NaN    NaN     NaN     NaN     NaN   
2                       20.0  ...     0.0    1.0     0.0     0.0     0.0   
3                       20.0  ...     0.0    1.0     0.0     0.0     0.0   
4                    Outlier  ...     0.0    0.0     1.0     0.0     0.0   

  TRIN_dico  VRIN_dico  CR_dico  RS_dico Personality_Disqualification_Dico  
0       0.0        0.0      0.0      0.0                               0.0  
1       NaN        NaN      NaN      NaN                               0.0  
2       0.0        0.0      0.0      0.0                               0.0  
3       1.0        0.0      0.0      0.0                               0.0  
4       0.0        0.0      0.0      0.0                               0.0  

[5 rows x 114 columns]
Creating a table with coding "1" representing extreme values and coding "0" representing non-extreme values
df_outliers = new_df_outliers.isin(['Outlier']) 
df_outliers = df_outliers.astype('int')
df_outliers.head()
   id  Population_Type_Dico  Designated_Role  Assessment_Center_Grade  \
0   0                     0                0                        0   
1   0                     0                0                        0   
2   0                     0                0                        0   
3   0                     0                0                        0   
4   0                     0                0                        0   

   Cognitive_Ability_Test  Morals_and_Values_Test  Job_Interview_Grade  \
0                       1                       0                    0   
1                       0                       0                    0   
2                       1                       0                    1   
3                       0                       0                    1   
4                       1                       1                    1   

   Heberew_Test_Grade  Filling_Instruction_Test  Emotional_Inteligence_Test  \
0                   1                         0                           1   
1                   0                         0                           0   
2                   1                         1                           0   
3                   1                         0                           0   
4                   1                         0                           1   

   ...  HY_dico  L_dico  MA_dico  SE_dico  RT_dico  TRIN_dico  VRIN_dico  \
0  ...        0       0        0        0        0          0          0   
1  ...        0       0        0        0        0          0          0   
2  ...        0       0        0        0        0          0          0   
3  ...        0       0        0        0        0          0          0   
4  ...        0       0        0        0        0          0          0   

   CR_dico  RS_dico  Personality_Disqualification_Dico  
0        0        0                                  0  
1        0        0                                  0  
2        0        0                                  0  
3        0        0                                  0  
4        0        0                                  0  

[5 rows x 114 columns]
Do the extreme values in the predictors affect the "behavior" of the target factor?
Running the Spearman test for the purpose of producing a correlation object ("corr_num")
df_1.head()
       id  Population_Type_Dico         Designated_Role  \
0   540.0                   0.0              prosecutor   
1  1128.0                   NaN     operational officer   
2   604.0                   0.0  administrative officer   
3   605.0                   0.0  administrative officer   
4   536.0                   0.0              prosecutor   

   Assessment_Center_Grade  Cognitive_Ability_Test  Morals_and_Values_Test  \
0                      4.9                     7.0                     4.0   
1                      4.3                     6.0                     1.0   
2                      4.4                     9.0                     1.0   
3                      5.1                     6.0                     2.0   
4                      4.9                     7.0                     5.0   

   Job_Interview_Grade  Heberew_Test_Grade  Filling_Instruction_Test  \
0                  4.0                 9.0                 13.500000   
1                  NaN                 8.0                       NaN   
2                  5.0                 9.0                 17.799999   
3                  5.0                 9.0                 11.000000   
4                  5.0                 9.0                 12.000000   

   Emotional_Inteligence_Test  ...  HY_dico  L_dico MA_dico SE_dico  RT_dico  \
0                        23.0  ...      0.0     0.0     0.0     0.0      0.0   
1                        15.0  ...      NaN     NaN     NaN     NaN      NaN   
2                        20.0  ...      0.0     1.0     0.0     0.0      0.0   
3                        20.0  ...      0.0     1.0     0.0     0.0      0.0   
4                        24.0  ...      0.0     0.0     1.0     0.0      0.0   

   TRIN_dico  VRIN_dico  CR_dico  RS_dico Personality_Disqualification_Dico  
0        0.0        0.0      0.0      0.0                               0.0  
1        NaN        NaN      NaN      NaN                               0.0  
2        0.0        0.0      0.0      0.0                               0.0  
3        1.0        0.0      0.0      0.0                               0.0  
4        0.0        0.0      0.0      0.0                               0.0  

[5 rows x 114 columns]
corr_df = pd.DataFrame(columns=('Spearman correlation','p-value'))

def spearman_corr(df_1):
    for i in df_1.columns:
        if i != 'Personality_Disqualification_Dico':
            mask = ~pd.isna(df_1[i]) & ~pd.isna(df_1['Personality_Disqualification_Dico']) 
            a = stats.spearmanr(df_1[i][mask], df_1['Personality_Disqualification_Dico'][mask])
            b = "{}".format(i)
            corr_df.loc[b] = [abs(a[0]),a[1]]
    corr_filtered = corr_df.loc[(corr_df['Spearman correlation'] < 0.999) & (corr_df['p-value'] < 0.999)]
    corr_filtered = corr_filtered.sort_values(by=['Spearman correlation'], ascending=False)
    return corr_filtered 
        

corr_num = spearman_corr(df_1)
#corr_num.head(50)
exsamine correlations changes following outliers drop
#sns.set(font_scale = 0.8)
#temp = df_num.copy()
#for i in temp:
#    if i != 'Personality_Disqualification_Dico' and i in corr_num.head().index:
#        for j in df_outliers:
#            if j in outliers_df.head().index:
#                if i < j:
#                    temp['{}_out'.format(j)] = df_outliers[j]
#                    sns.jointplot(x = 'Personality_Disqualification_Dico', y = i, hue='{}_out'.format(j), data = temp, legend=False)
#                    plt.legend(title=j, loc='upper right', labels=['With outliers', 'Without outliers'])
#plt.show()
pd.DataFrame(new_df_outliers.skew(),columns=['skewness']).sort_values(by='skewness', ascending=False)
             skewness
tcent_dico  33.674897
AAS_dico    16.770510
DEP_dico    11.435405
D4_dico      9.791629
ANG_dico     9.791629
...               ...
K           -0.768937
L_dico      -0.811096
DOE         -0.926189
FCENT       -1.022653
FCENT_dico  -1.382876

[99 rows x 1 columns]
15.3 In which variables should the extreme values be omitted?
Exsamination of extreme values effects variables distribution and correlation with the target factor :
def capping(df_1):
    temp = df_1.copy()
    for col in temp:
        if col in df_no_MMPI.index:
            Q1 = temp[col].quantile(0.25)
            Q3 = temp[col].quantile(0.75)
            upper_limit = Q1 + 1.5 * (Q3-Q1)
            lower_limit = Q3 - 1.5 * (Q3-Q1)
            temp[col] = np.where(temp[col] > upper_limit,np.nan,np.where(temp[col] < lower_limit,np.nan,temp[col]))
    return temp
temp = capping(df_1)
temp.head(100)
        id  Population_Type_Dico         Designated_Role  \
0    540.0                   0.0              prosecutor   
1   1128.0                   NaN     operational officer   
2    604.0                   0.0  administrative officer   
3    605.0                   0.0  administrative officer   
4    536.0                   0.0              prosecutor   
..     ...                   ...                     ...   
95  1745.0                   0.0            Traffic cops   
96  1809.0                   0.0  operational core units   
97    27.0                   0.0              prosecutor   
98   556.0                   0.0              prosecutor   
99   615.0                   0.0  administrative officer   

    Assessment_Center_Grade  Cognitive_Ability_Test  Morals_and_Values_Test  \
0                       4.9                     7.0                     4.0   
1                       4.3                     6.0                     1.0   
2                       4.4                     9.0                     1.0   
3                       5.1                     6.0                     2.0   
4                       4.9                     7.0                     5.0   
..                      ...                     ...                     ...   
95                      5.0                     5.0                     2.0   
96                      2.0                     6.0                     5.0   
97                      4.3                     7.0                     3.0   
98                      4.9                     6.0                     3.0   
99                      5.0                     5.0                     1.0   

    Job_Interview_Grade  Heberew_Test_Grade  Filling_Instruction_Test  \
0                   4.0                 9.0                 13.500000   
1                   NaN                 8.0                       NaN   
2                   5.0                 9.0                 17.799999   
3                   5.0                 9.0                 11.000000   
4                   5.0                 9.0                 12.000000   
..                  ...                 ...                       ...   
95                  4.0                 8.0                  9.500000   
96                  4.0                 6.0                       NaN   
97                  4.0                 8.0                       NaN   
98                  NaN                 9.0                 12.800000   
99                  5.0                 8.0                  9.500000   

    Emotional_Inteligence_Test  ...  HY_dico  L_dico MA_dico SE_dico  RT_dico  \
0                         23.0  ...      0.0     0.0     0.0     0.0      0.0   
1                         15.0  ...      NaN     NaN     NaN     NaN      NaN   
2                         20.0  ...      0.0     1.0     0.0     0.0      0.0   
3                         20.0  ...      0.0     1.0     0.0     0.0      0.0   
4                         24.0  ...      0.0     0.0     1.0     0.0      0.0   
..                         ...  ...      ...     ...     ...     ...      ...   
95                        17.0  ...      0.0     1.0     0.0     0.0      0.0   
96                        16.0  ...      0.0     0.0     0.0     0.0      0.0   
97                         NaN  ...      NaN     NaN     NaN     NaN      NaN   
98                        22.0  ...      0.0     0.0     0.0     0.0      0.0   
99                        20.0  ...      0.0     0.0     0.0     0.0      0.0   

    TRIN_dico  VRIN_dico  CR_dico  RS_dico Personality_Disqualification_Dico  
0         0.0        0.0      0.0      0.0                               0.0  
1         NaN        NaN      NaN      NaN                               0.0  
2         0.0        0.0      0.0      0.0                               0.0  
3         1.0        0.0      0.0      0.0                               0.0  
4         0.0        0.0      0.0      0.0                               0.0  
..        ...        ...      ...      ...                               ...  
95        0.0        0.0      0.0      0.0                               0.0  
96        0.0        0.0      0.0      0.0                               0.0  
97        NaN        NaN      NaN      NaN                               0.0  
98        0.0        0.0      0.0      0.0                               0.0  
99        1.0        0.0      0.0      0.0                               0.0  

[100 rows x 114 columns]
Extreme values in the distribution of the continuous predictors - before and after the omission
def dist_outliers(original_df, new, cols):
    for col in original_df:
        if col in cols:
            plt.figure(figsize=(15,6))
            plt.subplot(2,2,1)
            sns.distplot(original_df[col])
            plt.title('{} Distribution before capping'.format(col), fontsize=15)
            plt.subplot(2,2,2)
            sns.boxplot(original_df[col])
            plt.subplots_adjust(hspace = 0.6)
            plt.subplot(2,2,3)
            sns.distplot(new[col])
            plt.title('{} Distribution after capping'.format(col), fontsize=15)
            plt.subplot(2,2,4)
            sns.boxplot(new[col])
            plt.show()
#dist_outliers(df, temp, outliers_df.index)
def cocor(x1,y1, x2,y2):
    xy1 = x1.corr(y1, method='spearman')
    xy2 = x2.corr(y2, method='spearman')
    n1 = len(x1)
    n2 = len(x2)
    xy_z = 0.5 * np.log((1 + xy1)/(1 - xy1))
    ab_z = 0.5 * np.log((1 + xy2)/(1 - xy2))
    if n2 is None:
        n2 = n1
    se_diff_r = np.sqrt(1/(n1 - 3) + 1/(n2 - 3))
    diff = xy_z - ab_z
    z = abs(diff / se_diff_r)
    p = (1 - norm.cdf(z)) * 2
    return z, p
out_df_final = pd.DataFrame(columns= ['Var', 'outliers_pct', 'distribution_changed', 'correlation_changed'])

np.seterr(divide='ignore', invalid='ignore')
for i in df_no_MMPI:
# column with outliers
    out = df_num[i]
# column without outliers
    non = temp[i].loc[df_outliers[i]==0]
# target value with outliers
    Personality_Disqualification_Dico_out = df_num['Personality_Disqualification_Dico']
# target value without outliers
    Personality_Disqualification_Dico_non = Personality_Disqualification_Dico_out.loc[df_outliers[i]==0]
    p, p_value = cocor(out, Personality_Disqualification_Dico_out,non, Personality_Disqualification_Dico_non)
    cor_change = np.where(p_value<0.5,'+','-')
    mv = df_outliers[i].sum()
# perform Kolmogorov-Smirnov test
    pval = ks_2samp(out, non)[1]
    dis_change = np.where(pval<0.5,'+','-')
    out_df_final = out_df_final.append({'Var': i, 'outliers_pct': mv, 'distribution_changed': dis_change, 'correlation_changed': cor_change},ignore_index=True)
#out_df_final
out_df_final['drop_outliers'] = 'Yes'
out_df_final.loc[(out_df_final.distribution_changed == '+') &
                 (out_df_final.correlation_changed == '+'),'drop_outliers'] = 'No'
out_df_final = out_df_final[out_df_final['outliers_pct']>0].sort_values(by='drop_outliers')
out_df_final
                              Var outliers_pct distribution_changed  \
1          Cognitive_Ability_Test          984                    +   
2          Morals_and_Values_Test          321                    +   
4              Heberew_Test_Grade          522                    +   
5        Filling_Instruction_Test          223                    +   
6      Emotional_Inteligence_Test          143                    +   
9              Years_of_Education          571                    +   
0         Assessment_Center_Grade          160                    +   
3             Job_Interview_Grade          657                    +   
7   Recruit_Quality_Measure_Score           39                    -   
8                    Honesty_Test          427                    +   
10                            Age          309                    +   

   correlation_changed drop_outliers  
1                    +            No  
2                    +            No  
4                    +            No  
5                    +            No  
6                    +            No  
9                    +            No  
0                    -           Yes  
3                    -           Yes  
7                    -           Yes  
8                    -           Yes  
10                   -           Yes  
The test findings revealed that extreme values should be omitted among only 6 numerical variables out of the 11 numerical variables tested. Among 5 numerical variables, the omission of the extreme values leads both to a change in the nature of the distribution and to a change in the relationship between the predictor and the target variable
out_df_final.groupby('drop_outliers').size()
drop_outliers
No     6
Yes    5
dtype: int64
15.4 Summary of the analysis
An assessment of extreme values was carried out among (only) 11 numerical research variables. It's important to note that, due to their primary purpose of identifying candidates with abnormal scores on personality scales, an early decision was made that no extreme values were omitted from the personality scales. Among the 11 relevant numeric variables, the following patterns emerged:
- Change in Distribution and Relationship: Among 6 variables, the omission of extreme values led to changes both in the distribution of the variables and the relationship between the variables and the target factor. Consequently, no extreme values were omitted from these variables.
- Change in Distribution, No Change in Relationship: For 4 variables, omitting extreme values caused shifts in the distribution of the variables while maintaining a consistent relationship between the variables and the target factor. Extreme values were dropped from these variables.
- No Change in Distribution or Relationship: Among 1 variable, no impact was observed in either the distribution of the variable or the relationship between the variable and the target factor. In this case, the extreme values were also omitted.
These assessments provide crucial insights into the potential effects of omitting extreme values within each variable, allowing for a more informed decision-making process in the context of your analysis.
15.5 Drop outliers from data and save under new data frame - "df_num_no_outliers"
delete_outliers = list()
for i,row in out_df_final.iterrows():
    if row['Var'] in outliers_df.index.tolist() and row['drop_outliers'] == 'Yes':
        delete_outliers.append(row['Var'])
delete_outliers
['Assessment_Center_Grade',
 'Job_Interview_Grade',
 'Recruit_Quality_Measure_Score',
 'Honesty_Test',
 'Age']
Extreme values in the distribution of the continuous predictors - before and after the omission
def dist_compare_after_capping(original_df, new, cols):
    for col in original_df:
        if col in cols:
            plt.figure(figsize=(15,6))
            plt.subplot(2,2,1)
            sns.distplot(original_df[col])
            plt.title('{} Distribution before capping'.format(col), fontsize=15)
            plt.subplot(2,2,2)
            sns.boxplot(original_df[col])
            plt.subplots_adjust(hspace = 0.6)
            plt.subplot(2,2,3)
            sns.distplot(new[col])
            plt.title('{} Distribution after capping'.format(col), fontsize=15)
            plt.subplot(2,2,4)
            sns.boxplot(new[col])
            plt.show()
#dist_compare_after_capping(df, temp, delete_outliers)
delete_outliers = ['ssessment_Center_Grade', 'Heberew_Test_Grade', 'Recruit_Quality_Measure_Score', 'Honesty_Test', 'Age']
def capping(df_1):
    for col in df_1:
        if col in delete_outliers:
            Q1 = df_1[col].quantile(0.25)
            Q3 = df_1[col].quantile(0.75)
            upper_limit = Q1 + 1.5 * (Q3-Q1)
            lower_limit = Q3 - 1.5 * (Q3-Q1)
            df_1[col] = np.where(df_1[col] > upper_limit,np.nan,np.where(df_1[col] < lower_limit,np.nan,df_1[col]))
    return df_1
new_df_cap = capping(df_1)
new_df_cap.head()
       id  Population_Type_Dico         Designated_Role  \
0   540.0                   0.0              prosecutor   
1  1128.0                   NaN     operational officer   
2   604.0                   0.0  administrative officer   
3   605.0                   0.0  administrative officer   
4   536.0                   0.0              prosecutor   

   Assessment_Center_Grade  Cognitive_Ability_Test  Morals_and_Values_Test  \
0                      4.9                     7.0                     4.0   
1                      4.3                     6.0                     1.0   
2                      4.4                     9.0                     1.0   
3                      5.1                     6.0                     2.0   
4                      4.9                     7.0                     5.0   

   Job_Interview_Grade  Heberew_Test_Grade  Filling_Instruction_Test  \
0                  4.0                 NaN                 13.500000   
1                  NaN                 8.0                       NaN   
2                  5.0                 NaN                 17.799999   
3                  5.0                 NaN                 11.000000   
4                  5.0                 NaN                 12.000000   

   Emotional_Inteligence_Test  ...  HY_dico  L_dico MA_dico SE_dico  RT_dico  \
0                        23.0  ...      0.0     0.0     0.0     0.0      0.0   
1                        15.0  ...      NaN     NaN     NaN     NaN      NaN   
2                        20.0  ...      0.0     1.0     0.0     0.0      0.0   
3                        20.0  ...      0.0     1.0     0.0     0.0      0.0   
4                        24.0  ...      0.0     0.0     1.0     0.0      0.0   

   TRIN_dico  VRIN_dico  CR_dico  RS_dico Personality_Disqualification_Dico  
0        0.0        0.0      0.0      0.0                               0.0  
1        NaN        NaN      NaN      NaN                               0.0  
2        0.0        0.0      0.0      0.0                               0.0  
3        1.0        0.0      0.0      0.0                               0.0  
4        0.0        0.0      0.0      0.0                               0.0  

[5 rows x 114 columns]
The results of the omission can be seen by comparing the missing values in the original data frame versus the copy frame of the data frame from which the extreme values were omitted in the relevant :
print(new_df_cap.isnull().sum())
id                                     0
Population_Type_Dico                 147
Designated_Role                      148
Assessment_Center_Grade              988
Cognitive_Ability_Test               260
                                    ... 
TRIN_dico                             29
VRIN_dico                             29
CR_dico                               29
RS_dico                               29
Personality_Disqualification_Dico      0
Length: 114, dtype: int64
print(df.isnull().sum())
id                                     0
Population_Type_Dico                 147
Designated_Role                      148
Assessment_Center_Grade              988
Cognitive_Ability_Test               260
                                    ... 
TRIN_dico                             29
VRIN_dico                             29
CR_dico                               29
RS_dico                               29
Personality_Disqualification_Dico      0
Length: 114, dtype: int64
16. Missing Values Treatments
new_df_cap_2 = new_df_cap.copy()
df_nulls = df_1.copy()
for col in df_nulls:
    if df_nulls[col].isna().sum() == 0:
        del df_nulls[col]
df_nulls
      Population_Type_Dico         Designated_Role  Assessment_Center_Grade  \
0                      0.0              prosecutor                      4.9   
1                      NaN     operational officer                      4.3   
2                      0.0  administrative officer                      4.4   
3                      0.0  administrative officer                      5.1   
4                      0.0              prosecutor                      4.9   
...                    ...                     ...                      ...   
2295                   NaN                     NaN                      NaN   
2296                   NaN                     NaN                      NaN   
2297                   NaN                     NaN                      NaN   
2298                   0.0    administrative nagad                      NaN   
2299                   0.0    administrative nagad                      NaN   

      Cognitive_Ability_Test  Job_Interview_Grade  Heberew_Test_Grade  \
0                        7.0                  4.0                 NaN   
1                        6.0                  NaN                 8.0   
2                        9.0                  5.0                 NaN   
3                        6.0                  5.0                 NaN   
4                        7.0                  5.0                 NaN   
...                      ...                  ...                 ...   
2295                     8.0                  5.0                 7.0   
2296                     7.0                  4.0                 7.0   
2297                     4.0                  4.0                 7.0   
2298                     6.0                  5.0                 NaN   
2299                     3.0                  NaN                 8.0   

      Filling_Instruction_Test  Emotional_Inteligence_Test  \
0                    13.500000                        23.0   
1                          NaN                        15.0   
2                    17.799999                        20.0   
3                    11.000000                        20.0   
4                    12.000000                        24.0   
...                        ...                         ...   
2295                 15.000000                        19.0   
2296                 11.000000                        17.0   
2297                       NaN                        18.0   
2298                       NaN                        14.0   
2299                  6.500000                        12.0   

      Recruit_Quality_Measure_Score  Honesty_Test  ... WRK_dico HY_dico  \
0                               NaN           NaN  ...      0.0     0.0   
1                               NaN           NaN  ...      NaN     NaN   
2                               NaN           NaN  ...      0.0     0.0   
3                               NaN           NaN  ...      0.0     0.0   
4                               NaN           NaN  ...      0.0     0.0   
...                             ...           ...  ...      ...     ...   
2295                            NaN           9.0  ...      0.0     0.0   
2296                            NaN           NaN  ...      0.0     0.0   
2297                            NaN           9.0  ...      0.0     0.0   
2298                            NaN           9.0  ...      0.0     1.0   
2299                            NaN           9.0  ...      0.0     0.0   

      L_dico  MA_dico  SE_dico  RT_dico TRIN_dico  VRIN_dico  CR_dico  RS_dico  
0        0.0      0.0      0.0      0.0       0.0        0.0      0.0      0.0  
1        NaN      NaN      NaN      NaN       NaN        NaN      NaN      NaN  
2        1.0      0.0      0.0      0.0       0.0        0.0      0.0      0.0  
3        1.0      0.0      0.0      0.0       1.0        0.0      0.0      0.0  
4        0.0      1.0      0.0      0.0       0.0        0.0      0.0      0.0  
...      ...      ...      ...      ...       ...        ...      ...      ...  
2295     1.0      0.0      0.0      0.0       0.0        0.0      0.0      0.0  
2296     0.0      0.0      0.0      0.0       0.0        0.0      0.0      0.0  
2297     1.0      0.0      0.0      0.0       0.0        0.0      0.0      0.0  
2298     1.0      0.0      0.0      0.0       1.0        0.0      0.0      0.0  
2299     1.0      0.0      0.0      0.0       0.0        0.0      0.0      0.0  

[2300 rows x 110 columns]
16.2 How the missing values distributed along the dataset?
This part focus on:
1.	Identification of variables in which there are many missing values (in these cases we will omit the variable column)
1.	Identification of rows (participants) that have multiple missing values (in these cases we will omit the participant row)
Missing whithin all project variables
msno.matrix(df_nulls)
<AxesSubplot:>
 
msno.bar(df_nulls, sort='descending')
<AxesSubplot:>
 
In the above diagrams you can clearly see from which variables in the data frame the missing data come from
16.3 Rows missing values - Examining lines (datafrme observations) with high missing values
First, it's important to note that the data frame contains approximately 100 variables that correspond to different personality scales in the MMPI-2 test. A noteworthy aspect is that if a candidate did not undertake the MMPI-2 test, they would naturally have missing data across all 100 personality scale variables.
This situation results in an elevated occurrence of missing data, even though the personality scales encapsulate a confined domain of information concerning candidates. It is not the intention to employ all 100 personality scales in constructing the models; rather, the focus will be on selecting a subset of scales that distinctly contribute to the modeling process. This could involve either utilizing scales with evident unique contributions or employing a methodology to combine the various personality scales into a more comprehensive factor.
To avoid an overrepresentation of the MMPI test's personality scales, a step will be taken before examining the missing values among the observations in the data frame. This step involves consolidating the personality scales into a single representative variable within the data frame.
new_df_cap_2.head()
       id  Population_Type_Dico         Designated_Role  \
0   540.0                   0.0              prosecutor   
1  1128.0                   NaN     operational officer   
2   604.0                   0.0  administrative officer   
3   605.0                   0.0  administrative officer   
4   536.0                   0.0              prosecutor   

   Assessment_Center_Grade  Cognitive_Ability_Test  Morals_and_Values_Test  \
0                      4.9                     7.0                     4.0   
1                      4.3                     6.0                     1.0   
2                      4.4                     9.0                     1.0   
3                      5.1                     6.0                     2.0   
4                      4.9                     7.0                     5.0   

   Job_Interview_Grade  Heberew_Test_Grade  Filling_Instruction_Test  \
0                  4.0                 NaN                 13.500000   
1                  NaN                 8.0                       NaN   
2                  5.0                 NaN                 17.799999   
3                  5.0                 NaN                 11.000000   
4                  5.0                 NaN                 12.000000   

   Emotional_Inteligence_Test  ...  HY_dico  L_dico MA_dico SE_dico  RT_dico  \
0                        23.0  ...      0.0     0.0     0.0     0.0      0.0   
1                        15.0  ...      NaN     NaN     NaN     NaN      NaN   
2                        20.0  ...      0.0     1.0     0.0     0.0      0.0   
3                        20.0  ...      0.0     1.0     0.0     0.0      0.0   
4                        24.0  ...      0.0     0.0     1.0     0.0      0.0   

   TRIN_dico  VRIN_dico  CR_dico  RS_dico Personality_Disqualification_Dico  
0        0.0        0.0      0.0      0.0                               0.0  
1        NaN        NaN      NaN      NaN                               0.0  
2        0.0        0.0      0.0      0.0                               0.0  
3        1.0        0.0      0.0      0.0                               0.0  
4        0.0        0.0      0.0      0.0                               0.0  

[5 rows x 114 columns]
internal_miss_raws = pd.DataFrame(new_df_cap_2, columns=["Population_Type_Dico", "Designated_Role", "Assessment_Center_Grade", "Cognitive_Ability_Test", "Morals_and_Values_Test", "Job_Interview_Grade", "Heberew_Test_Grade", "Filling_Instruction_Test", "Emotional_Inteligence_Test", "Recruit_Quality_Measure_Score", "Honesty_Test", "Education_Level", "Educational_Institution", "Years_of_Education", "Age", "Gender_Dico", "Commander_in_Army_Dico", "Combat_Army_Unit_Dico", "Rovaee_Training_Type", "M2a"])
internal_miss_raws.shape
(2300, 20)
internal_miss_raws['number_of_NaNs'] = internal_miss_raws.iloc[:, :1861].isna().sum(1)

internal_miss_raws['precent_of_NaNs'] = internal_miss_raws.number_of_NaNs /114

df_NaNs_precent = internal_miss_raws[['precent_of_NaNs']].copy()

df_NaNs_precent.sort_values(ascending=False,by=['precent_of_NaNs'])


internal_miss_raws['number_of_NaNs'] = internal_miss_raws.iloc[:, :1861].isna().sum(1)

internal_miss_raws['precent_of_NaNs'] = internal_miss_raws.number_of_NaNs / 114
high_cases_missing_rows = pd.DataFrame(internal_miss_raws, columns=["number_of_NaNs","precent_of_NaNs"])
high_cases_missing_rows.sort_values(by='precent_of_NaNs', ascending=False)
      number_of_NaNs  precent_of_NaNs
2006              14         0.122807
555               14         0.122807
1041              13         0.114035
1349              13         0.114035
1580              13         0.114035
...              ...              ...
874                0         0.000000
333                0         0.000000
196                0         0.000000
861                0         0.000000
679                0         0.000000

[2300 rows x 2 columns]
In the above table it can be seen that there are no observations in the data frame with a large number of missing values (Max = 12.2%). Therefore, no rows will be omitted from the data frame.
Columns missing values - Examining Columns (datafrme observations) with high missing values
new_df_cap_3 = new_df_cap_2.copy()
At this stage, there are 104 variables with missing values in the data frame
mis_val = new_df_cap_3.isna().sum()
mis_val_per = new_df_cap_3.isna().sum()/len(new_df_cap_2)*100
mis_val_table = pd.concat([mis_val, mis_val_per], axis=1)
mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})
mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,:] != 0].sort_values(
    '% of Total Values', ascending=False).round(1)
mis_val_table_ren_columns
                                   Missing Values  % of Total Values
Recruit_Quality_Measure_Score              1986.0               86.3
Combat_Army_Unit_Dico                      1582.0               68.8
Commander_in_Army_Dico                     1272.0               55.3
Emotional_Inteligence_Test                 1031.0               44.8
Assessment_Center_Grade                     988.0               43.0
...                                           ...                ...
Years_of_Education                           16.0                0.7
id                                            NaN                NaN
Morals_and_Values_Test                        NaN                NaN
Gender_Dico                                   NaN                NaN
Personality_Disqualification_Dico             NaN                NaN

[114 rows x 2 columns]
The variables "Recruit_Quality_Measure_Score" and "Combat_Army_Unit_Dico" contain an excessive number of outliers. Given the substantial extent of missing data, it is deemed inappropriate to incorporate these variables for model development purposes.  Consequently, these variables will be excluded from the current version of the data frame.
new_df_cap_3 = new_df_cap_3.drop(columns=['Recruit_Quality_Measure_Score', 'Combat_Army_Unit_Dico'])
16.4 What is the missing values mechanism?
The missing values mechanism refers to the pattern or relationship between the missing values and other variables in the dataset. It helps in understanding whether the missing data occurs randomly or if there is any systematic pattern to it. There are three main categories for the missing values mechanism:
- Missing Completely at Random (MCAR): In this scenario, the missing values occur randomly and independently of other variables in the dataset. There is no significant correlation between the presence of missing values and the values of any other variables.
- Missing at Random (MAR): This situation arises when the missing values are dependent on observed variables, but not on the values of the missing data itself. In other words, there may be a partial correlation between the missing values and the observed variables, but this correlation is explainable by other variables in the dataset.
- Missing Not at Random (MNAR): Here, the missing values are systematically related to the values of the missing data itself. There is a significant correlation between the presence of missing values and the values of other variables in the dataset, including the missing values themselves. This situation can introduce bias into the analysis.
It's important to note that if the missing data mechanism is MCAR or MAR, it's possible to impute or handle the missing values to some extent. However, if the mechanism is MNAR, imputing missing data becomes more challenging and can introduce bias into the analysis. Careful consideration of the missing data mechanism is essential for making informed decisions about how to handle missing values effectively. This occurs when the missing values are related to the missing values themselves, and this relationship is not explained by the observed variables. There is a systematic pattern or correlation between the missing values and other variables that is not accounted for by the available data.
It's important to note that dealing with missing data depends on understanding the missing values mechanism. In the case of MCAR or MAR, various imputation techniques can be used to estimate missing values. However, in the case of MNAR, imputing missing values can lead to biased results, as the missingness is related to unobserved factors.
mis_val = new_df_cap_3.isna().sum()
mis_val_per = new_df_cap_3.isna().sum()/len(new_df_cap_2)*100
mis_val_table = pd.concat([mis_val, mis_val_per], axis=1)
mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : 'Total Values'})
mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,:] != 0].sort_values(
    'Total Values', ascending=False).round(1)
mis_val_table_ren_columns
                                   Missing Values  Total Values
Commander_in_Army_Dico                     1272.0          55.3
Emotional_Inteligence_Test                 1031.0          44.8
Assessment_Center_Grade                     988.0          43.0
Educational_Institution                     964.0          41.9
Education_Level                             957.0          41.6
...                                           ...           ...
Years_of_Education                           16.0           0.7
id                                            NaN           NaN
Morals_and_Values_Test                        NaN           NaN
Gender_Dico                                   NaN           NaN
Personality_Disqualification_Dico             NaN           NaN

[112 rows x 2 columns]
Creating a dataframe with missing values as 1 and existing values as 0
df_missing_data = new_df_cap_3.isin([np.nan]) 
df_missing_data = df_missing_data.astype('int')
df_missing_data.head(150)
     id  Population_Type_Dico  Designated_Role  Assessment_Center_Grade  \
0     0                     0                0                        0   
1     0                     1                0                        0   
2     0                     0                0                        0   
3     0                     0                0                        0   
4     0                     0                0                        0   
..   ..                   ...              ...                      ...   
145   0                     0                0                        0   
146   0                     0                0                        0   
147   0                     0                0                        0   
148   0                     0                0                        0   
149   0                     0                0                        0   

     Cognitive_Ability_Test  Morals_and_Values_Test  Job_Interview_Grade  \
0                         0                       0                    0   
1                         0                       0                    1   
2                         0                       0                    0   
3                         0                       0                    0   
4                         0                       0                    0   
..                      ...                     ...                  ...   
145                       0                       0                    0   
146                       0                       0                    0   
147                       0                       0                    0   
148                       0                       0                    0   
149                       0                       0                    0   

     Heberew_Test_Grade  Filling_Instruction_Test  Emotional_Inteligence_Test  \
0                     1                         0                           0   
1                     0                         1                           0   
2                     1                         0                           0   
3                     1                         0                           0   
4                     1                         0                           0   
..                  ...                       ...                         ...   
145                   1                         0                           0   
146                   0                         0                           0   
147                   0                         0                           0   
148                   1                         0                           0   
149                   0                         0                           0   

     ...  HY_dico  L_dico  MA_dico  SE_dico  RT_dico  TRIN_dico  VRIN_dico  \
0    ...        0       0        0        0        0          0          0   
1    ...        1       1        1        1        1          1          1   
2    ...        0       0        0        0        0          0          0   
3    ...        0       0        0        0        0          0          0   
4    ...        0       0        0        0        0          0          0   
..   ...      ...     ...      ...      ...      ...        ...        ...   
145  ...        0       0        0        0        0          0          0   
146  ...        0       0        0        0        0          0          0   
147  ...        0       0        0        0        0          0          0   
148  ...        0       0        0        0        0          0          0   
149  ...        0       0        0        0        0          0          0   

     CR_dico  RS_dico  Personality_Disqualification_Dico  
0          0        0                                  0  
1          1        1                                  0  
2          0        0                                  0  
3          0        0                                  0  
4          0        0                                  0  
..       ...      ...                                ...  
145        0        0                                  0  
146        0        0                                  0  
147        0        0                                  0  
148        0        0                                  0  
149        0        0                                  0  

[150 rows x 112 columns]
plt.figure(figsize=(100,100))
sns.heatmap(df_missing_data.corr(), annot=True,cmap='coolwarm');
 
new_df_cap_4 = new_df_cap_3.drop(columns=['Designated_Role', 'Education_Level', 'Educational_Institution', 'Rovaee_Training_Type'])
Clearly, There are alot of independent variables that correlate with other independent variables in terms of missing values. we can see it in a significant way in the chart show above.
sns.set(font_scale = 1)
temp = new_df_cap_4.copy()
for i in temp:
    if i != 'Personality_Disqualification_Dico' and i in corr_num.index.values:
        for j in df_missing_data:
            if j in outliers_df.head().index:
                if i < j:
                    temp['{}_mv'.format(j)] = df_missing_data[j]
                    sns.displot(temp, x=i, hue='{}_mv'.format(j), kind="kde", fill=True, legend=False)
                    plt.legend(title='{}_mv'.format(j), loc='upper right', labels=['With mv', 'Without MV'])
plt.show()
 
 
Project_df_final = pd.DataFrame(columns= ['Var', 'MV_pct', 'distribution_changed'])

np.seterr(all='ignore')
for i in new_df_cap_4:
    if i in mis_val_table_ren_columns.index.tolist():
    # column with NULLS
        null = new_df_cap_4[i]
    # column without NULLS
        non = new_df_cap_4[i].loc[df_missing_data[i]==0]
    # target value with NULLS
        Personality_Disqualification_Dico_null = new_df_cap_4['Personality_Disqualification_Dico']
    # target value without NULLS
        Personality_Disqualification_Dico_non = Personality_Disqualification_Dico_null.loc[df_missing_data[i]==0]
        mv = df_missing_data[i].sum()
    # perform Kolmogorov-Smirnov test
        pval = ks_2samp(null, non)[1]
        dis_change = np.where(pval<=0.5,'+','-')
        Project_df_final = Project_df_final.append({'Var': i, 'MV_pct': mv, 'distribution_changed': dis_change},ignore_index=True)
        
Project_df_final
                                   Var MV_pct distribution_changed
0                                   id      0                    -
1                 Population_Type_Dico    147                    +
2              Assessment_Center_Grade    988                    +
3               Cognitive_Ability_Test    260                    +
4               Morals_and_Values_Test      0                    -
..                                 ...    ...                  ...
103                          TRIN_dico     29                    -
104                          VRIN_dico     29                    -
105                            CR_dico     29                    -
106                            RS_dico     29                    -
107  Personality_Disqualification_Dico      0                    -

[108 rows x 3 columns]
Project_df_final['replace']= 'Yes'
Project_df_final['MV_type']= 'MCAR/MAR'
Project_df_final.loc[(Project_df_final.distribution_changed== '+'),'replace']='No'
Project_df_final.loc[(Project_df_final.distribution_changed== '+'),'MV_type']='MNAR'
Project_df_final = Project_df_final[Project_df_final['MV_pct']>0].sort_values(by='MV_pct', ascending=False)
Project_df_final
                            Var MV_pct distribution_changed replace   MV_type
13       Commander_in_Army_Dico   1272                    +      No      MNAR
8    Emotional_Inteligence_Test   1031                    +      No      MNAR
2       Assessment_Center_Grade    988                    +      No      MNAR
6            Heberew_Test_Grade    753                    +      No      MNAR
7      Filling_Instruction_Test    704                    +      No      MNAR
..                          ...    ...                  ...     ...       ...
40                          DOE     29                    -     Yes  MCAR/MAR
39                           MF     29                    -     Yes  MCAR/MAR
38                           D5     29                    -     Yes  MCAR/MAR
106                     RS_dico     29                    -     Yes  MCAR/MAR
10           Years_of_Education     16                    -     Yes  MCAR/MAR

[104 rows x 5 columns]
Project_df_final.groupby('replace').size()
replace
No     10
Yes    94
dtype: int64
Of the 104 numerical variables that include missing values, it was found that among 10 variables the values are missing in a non-random way. In light of the above, out of the 104 independent research variables, it is possible to complete missing values among 94 variables.
Data Imputaion Method :
- Numeric Variables - MCAR/MAR missing data - Completing missing values using KNN algoritem
- Categoric  Dummies Variables - MCAR/MAR missing data - Completing missing values using the common value
- All Variables MNAR missing data- Dividing the variables into categories and entering missing values as a separate category
It is important to note that as of the current stage, the dichotomous and categorical project variables include a negligible number of missing values (the absolute majority of which is about 1%). In light of the low amount of missing values in these variables, we will be satisfied with their completion by the common value, without using complex missing value completion methods.
16.6 Data Imputation
Mode imputaion within Categoric  Dummies Variables - MCAR/MAR
Categoric_Dummie_MCAR_MAR = [ 'RS_dico', 'TPA_dico', 'D2_dico', 'D3_dico', 'D1_dico', 'D_dico', 'SOD_dico', 'K_dico', 'DEP_dico', 'ANG_dico', 'BIZ_dico', 'PA_dico', 'MDS_dico', 'OBS_dico', 'tcent_dico', 'AAS_dico', 'D5_dico', 'BIM_dico', 'CYN_dico', 'R_dico', 'ASP_dico', 'M2a_dico', 'D4_dico', 'ANX_dico', 'MF_dico', 'FCENT_dico', 'CR_dico', 'VRIN_dico', 'TRIN_dico', 'RT_dico', 'SE_dico', 'MA_dico', 'L_dico', 'HY_dico', 'WRK_dico', 'ES_dico', 'DOE_dico', 'PD_dico', 'FAM_dico', 'HS_dico', 'FP_dico', 'HEA_dico', 'SC_dico', 'F_dico', 'CR', 'PT_dico']
for col in Categoric_Dummie_MCAR_MAR:
    most_common_value = new_df_cap_4[col].mode()[0]
    new_df_cap_4[col].fillna(most_common_value, inplace=True)
MNAR missing data- Dividing relevant variables to categories (missing values in separate category)
var_to_cat = ['Commander_in_Army_Dico','Emotional_Inteligence_Test','Assessment_Center_Grade','Filling_Instruction_Test', 'Honesty_Test', 'Age','Population_Type_Dico', 'Heberew_Test_Grade']
var_to_imp = ['RS', 'MA', 'VRIN', 'MDS', 'D', 'SOD', 'K', 'DIS', 'DEP', 'WSD', 'ANG', 'BIZ', 'TPA', 'OBS', 'TRIN', 'tcent', 'AAS', 'M2APS', 'PA', 'BIM', 'CYN', 'R', 'ASP', 'M2a', 'D1', 'D2', 'D3', 'D4', 'RT', 'SE', 'L', 'HY', 'WRK', 'FCENT', 'ES', 'PD', 'FAM', 'HS', 'FP', 'HEA', 'SC', 'F', 'ANX', 'PT', 'DOE', 'MF', 'D5', 'Years_of_Education', 'Cognitive_Ability_Test', 'Job_Interview_Grade']
var_to_cat
['Commander_in_Army_Dico',
 'Emotional_Inteligence_Test',
 'Assessment_Center_Grade',
 'Filling_Instruction_Test',
 'Honesty_Test',
 'Age',
 'Population_Type_Dico',
 'Heberew_Test_Grade']
var_to_imp
['RS',
 'MA',
 'VRIN',
 'MDS',
 'D',
 'SOD',
 'K',
 'DIS',
 'DEP',
 'WSD',
 'ANG',
 'BIZ',
 'TPA',
 'OBS',
 'TRIN',
 'tcent',
 'AAS',
 'M2APS',
 'PA',
 'BIM',
 'CYN',
 'R',
 'ASP',
 'M2a',
 'D1',
 'D2',
 'D3',
 'D4',
 'RT',
 'SE',
 'L',
 'HY',
 'WRK',
 'FCENT',
 'ES',
 'PD',
 'FAM',
 'HS',
 'FP',
 'HEA',
 'SC',
 'F',
 'ANX',
 'PT',
 'DOE',
 'MF',
 'D5',
 'Years_of_Education',
 'Cognitive_Ability_Test',
 'Job_Interview_Grade']
Dividing Numric variables to categorical with the below classes :
for col in new_df_cap_4:
    if col in df_dummy.columns:
        new_df_cap_3[col] = new_df_cap_3[col].astype(np.float64)
labels = ['very high', 'high', 'medium', 'low', 'very low']
group_intervals = pd.DataFrame()
temp = new_df_cap_4.copy()
for col in temp:
    if col in var_to_cat:
        if col in df_dummy:
            temp[col] = np.where(temp[col].isnull(),'missing',temp[col])
        else:
            temp[col] = pd.cut(temp[col], bins=5, labels=labels)
            temp[col] = np.where(temp[col].isnull(),'missing',temp[col])
c = 1  # initialize plot counter

fig = plt.figure(figsize=(15,50))
for col in temp:
    if col in var_to_cat:
        plt.subplot(9, 2, c)
        plt.title('{}'.format(col))
        plt.xlabel(col)
        sns.countplot(temp[col])
        c = c + 1
    plt.subplots_adjust(hspace = 0.5, wspace = 0.3)
plt.show()
 
Plotting the new variables with Personality_Disqualification_Dico:
c = 1  # initialize plot counter

fig = plt.figure(figsize=(15,50))
for col in temp:
    if col in var_to_cat:
        plot_order = temp.groupby(col)['Personality_Disqualification_Dico'].mean().sort_values(ascending=False).index.values
        plt.subplot(9, 2, c)
        plt.title('{}'.format(col))
        plt.xlabel(col)
        sns.barplot(x=col, y='Personality_Disqualification_Dico', data=temp, order=plot_order)
        c = c + 1
    plt.subplots_adjust(hspace = 0.5, wspace = 0.3)
plt.show()
 
Performing imputaion on variables to impute, which are only the continuous variables left with MV:
temp.isnull().sum()
id                                     0
Population_Type_Dico                   0
Assessment_Center_Grade                0
Cognitive_Ability_Test               260
Morals_and_Values_Test                 0
                                    ... 
TRIN_dico                              0
VRIN_dico                              0
CR_dico                                0
RS_dico                                0
Personality_Disqualification_Dico      0
Length: 108, dtype: int64
df_nulls_new = temp.copy()
for col in df_nulls_new:
    if df_nulls_new[col].isna().sum() == 0:
        del df_nulls_new[col]
df_nulls_new.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2300 entries, 0 to 2299
Data columns (total 50 columns):
 #   Column                  Non-Null Count  Dtype  
---  ------                  --------------  -----  
 0   Cognitive_Ability_Test  2040 non-null   float64
 1   Job_Interview_Grade     2060 non-null   float64
 2   Years_of_Education      2284 non-null   float64
 3   M2a                     2271 non-null   float64
 4   ASP                     2271 non-null   float64
 5   R                       2271 non-null   float64
 6   CYN                     2271 non-null   float64
 7   BIM                     2271 non-null   float64
 8   PA                      2271 non-null   float64
 9   M2APS                   2271 non-null   float64
 10  AAS                     2271 non-null   float64
 11  tcent                   2271 non-null   float64
 12  OBS                     2271 non-null   float64
 13  MDS                     2271 non-null   float64
 14  TPA                     2271 non-null   float64
 15  BIZ                     2271 non-null   float64
 16  ANG                     2271 non-null   float64
 17  WSD                     2271 non-null   float64
 18  DEP                     2271 non-null   float64
 19  DIS                     2271 non-null   float64
 20  K                       2271 non-null   float64
 21  SOD                     2271 non-null   float64
 22  D                       2271 non-null   float64
 23  D1                      2271 non-null   float64
 24  D2                      2271 non-null   float64
 25  D3                      2271 non-null   float64
 26  D4                      2271 non-null   float64
 27  D5                      2271 non-null   float64
 28  MF                      2271 non-null   float64
 29  DOE                     2271 non-null   float64
 30  PT                      2271 non-null   float64
 31  ANX                     2271 non-null   float64
 32  F                       2271 non-null   float64
 33  SC                      2271 non-null   float64
 34  HEA                     2271 non-null   float64
 35  FP                      2271 non-null   float64
 36  HS                      2271 non-null   float64
 37  FAM                     2271 non-null   float64
 38  PD                      2271 non-null   float64
 39  ES                      2271 non-null   float64
 40  FCENT                   2271 non-null   float64
 41  WRK                     2271 non-null   float64
 42  HY                      2271 non-null   float64
 43  L                       2271 non-null   float64
 44  MA                      2271 non-null   float64
 45  SE                      2271 non-null   float64
 46  RT                      2271 non-null   float64
 47  TRIN                    2271 non-null   float64
 48  VRIN                    2271 non-null   float64
 49  RS                      2271 non-null   float64
dtypes: float64(50)
memory usage: 898.6 KB
KNN imputaion within Numeric Variables - MCAR/MAR
Imputation on features that are in the imp_cols list below, with one neighbor sample to use for imputation (in order to fit with the binary variables):
from sklearn.impute import KNNImputer
final_df = temp.copy()
final_columns = final_df.select_dtypes(include = ['float64']).columns
knn_imputer = KNNImputer(n_neighbors=1)

imputed_data = pd.DataFrame(knn_imputer.fit_transform(final_df[final_columns]),columns = final_columns)
imputed_data.head()
       id  Cognitive_Ability_Test  Morals_and_Values_Test  \
0   540.0                     7.0                     4.0   
1  1128.0                     6.0                     1.0   
2   604.0                     9.0                     1.0   
3   605.0                     6.0                     2.0   
4   536.0                     7.0                     5.0   

   Job_Interview_Grade  Years_of_Education   M2a   ASP     R   CYN   BIM  ...  \
0                  4.0                15.0  40.0  42.0  65.0  38.0  42.0  ...   
1                  4.0                12.0  56.0  52.0  62.0  56.0  44.0  ...   
2                  5.0                13.0  37.0  34.0  63.0  35.0  39.0  ...   
3                  5.0                12.0  38.0  45.0  67.0  46.0  41.0  ...   
4                  5.0                17.0  44.0  56.0  49.0  53.0  44.0  ...   

   HY_dico  L_dico  MA_dico  SE_dico  RT_dico  TRIN_dico  VRIN_dico  CR_dico  \
0      0.0     0.0      0.0      0.0      0.0        0.0        0.0      0.0   
1      0.0     1.0      0.0      0.0      0.0        0.0        0.0      0.0   
2      0.0     1.0      0.0      0.0      0.0        0.0        0.0      0.0   
3      0.0     1.0      0.0      0.0      0.0        1.0        0.0      0.0   
4      0.0     0.0      1.0      0.0      0.0        0.0        0.0      0.0   

   RS_dico  Personality_Disqualification_Dico  
0      0.0                                0.0  
1      0.0                                0.0  
2      0.0                                0.0  
3      0.0                                0.0  
4      0.0                                0.0  

[5 rows x 99 columns]
final_df[final_columns] = imputed_data
Join categorical variables - missing which coded as separate category
df_finllna = df.fillna('missing')
df_join_categor = pd.DataFrame(df_finllna, columns=["id",'Designated_Role', 'Education_Level', 'Educational_Institution', 'Rovaee_Training_Type'])
final_df_joined = pd.merge(final_df,df_join_categor,how='left')
final_df_joined.head()
       id Population_Type_Dico Assessment_Center_Grade  \
0   540.0                  0.0                     low   
1  1128.0              missing                     low   
2   604.0                  0.0                     low   
3   605.0                  0.0                     low   
4   536.0                  0.0                     low   

   Cognitive_Ability_Test  Morals_and_Values_Test  Job_Interview_Grade  \
0                     7.0                     4.0                  4.0   
1                     6.0                     1.0                  4.0   
2                     9.0                     1.0                  5.0   
3                     6.0                     2.0                  5.0   
4                     7.0                     5.0                  5.0   

  Heberew_Test_Grade Filling_Instruction_Test Emotional_Inteligence_Test  \
0            missing                      low                   very low   
1           very low                  missing                       high   
2            missing                 very low                        low   
3            missing                   medium                        low   
4            missing                   medium                   very low   

  Honesty_Test  ...  RT_dico TRIN_dico  VRIN_dico CR_dico  RS_dico  \
0      missing  ...      0.0       0.0        0.0     0.0      0.0   
1      missing  ...      0.0       0.0        0.0     0.0      0.0   
2      missing  ...      0.0       0.0        0.0     0.0      0.0   
3      missing  ...      0.0       1.0        0.0     0.0      0.0   
4      missing  ...      0.0       0.0        0.0     0.0      0.0   

   Personality_Disqualification_Dico         Designated_Role  Education_Level  \
0                                0.0              prosecutor          missing   
1                                0.0     operational officer          missing   
2                                0.0  administrative officer          missing   
3                                0.0  administrative officer          missing   
4                                0.0              prosecutor          missing   

   Educational_Institution  Rovaee_Training_Type  
0                  missing               missing  
1                  missing               missing  
2                  missing               missing  
3                  missing               missing  
4                  missing              rovaee 3  

[5 rows x 112 columns]
final_df_joined.isnull().sum().sum()
0
final_df_joined.to_csv('EDA_Final_data.csv')
------------------------------------------------------------------------------------------EDA STAGE END-----------------------------------------------------------------------------------------------
